
# === PYTHON FILES ===

# __init__.py



# abstract_classes\__init__.py



# abstract_classes\agent_base.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from abc import ABC, abstractmethod
# 5 
# 6 
# 7 class AgentBase(ABC):
# 8     """Base class for executing agent logic."""
# 9 
# 10     def __init__(self) -> None:
# 11         self.logger = logging.getLogger(self.__class__.__name__)
# 12 
# 13     def run(self, *args, **kwargs) -> None:
# 14         self.logger.debug("Agent run start")
# 15         self._run_agent_logic(*args, **kwargs)
# 16         self.logger.debug("Agent run end")
# 17 
# 18     @abstractmethod
# 19     def _run_agent_logic(self, *args, **kwargs) -> None:
# 20         """Execute agent specific logic."""
# 21         raise NotImplementedError


# abstract_classes\context_provider_base.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from abc import ABC, abstractmethod
# 5 
# 6 
# 7 class ContextProviderBase(ABC):
# 8     """Base class for providing context from symbol graphs."""
# 9 
# 10     def __init__(self) -> None:
# 11         self.logger = logging.getLogger(self.__class__.__name__)
# 12 
# 13     def get_context(self, *args, **kwargs):
# 14         self.logger.debug("Context retrieval start")
# 15         context = self._get_context(*args, **kwargs)
# 16         self.logger.debug("Context retrieval end")
# 17         return context
# 18 
# 19     @abstractmethod
# 20     def _get_context(self, *args, **kwargs):
# 21         """Return context information."""
# 22         raise NotImplementedError


# abstract_classes\experiment.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from typing import Any, Dict, List
# 5 
# 6 from ..factories.scoring_provider import ScoringProviderFactory
# 7 from ..factories.system_manager import SystemManagerFactory
# 8 from ..factories.experiment_config_provider import ExperimentConfigProvider
# 9 from ..utilities.db import init_db, insert_logs
# 10 from ..utilities.metadata.logging import ScoringLog
# 11 from ..utilities.metrics import EVALUATION_METRICS
# 12 
# 13 
# 14 class Experiment:
# 15     """Base experiment handling execution and scoring."""
# 16 
# 17     def __init__(self, config_id: int) -> None:
# 18         self.logger = logging.getLogger(self.__class__.__name__)
# 19         self.config_id = config_id
# 20         self.config: Dict[str, Any] | None = None
# 21         self.scoring_model_id = "dummy"
# 22 
# 23         self.evaluation_logs: List[Any] = []
# 24         self.code_quality_logs: List[Any] = []
# 25         self.conversation_logs: List[Any] = []
# 26         self.prompt_logs: List[Any] = []
# 27         self.state_logs: List[Any] = []
# 28         self.metrics: Dict[str, float] | None = None
# 29 
# 30     def run(self, *args, **kwargs) -> Dict[str, float]:
# 31         self.logger.debug("Experiment run start")
# 32 
# 33         #  load configuration
# 34         self.config = ExperimentConfigProvider.load(self.config_id)
# 35         self.scoring_model_id = self.config.get("scoring_model_id", "dummy")
# 36         system_manager_id = self.config.get("system_manager_id", "dummy")
# 37 
# 38         #  run system manager
# 39         manager = SystemManagerFactory.create(system_manager_id)
# 40         manager.run()
# 41 
# 42         #  collect logs from manager
# 43         self.evaluation_logs = getattr(manager, "evaluation_logs", [])
# 44         self.code_quality_logs = getattr(manager, "code_quality_logs", [])
# 45         self.conversation_logs = getattr(manager, "conversation_logs", [])
# 46         self.prompt_logs = getattr(manager, "prompt_logs", [])
# 47         self.state_logs = getattr(manager, "state_logs", [])
# 48 
# 49         logs = {
# 50             "evaluation": self.evaluation_logs,
# 51             "code_quality": self.code_quality_logs,
# 52             "conversation": self.conversation_logs,
# 53             "prompt": self.prompt_logs,
# 54             "state": self.state_logs,
# 55         }
# 56 
# 57         scoring_provider = ScoringProviderFactory.create(self.scoring_model_id)
# 58         metrics = scoring_provider.score(logs)
# 59         for key in EVALUATION_METRICS:
# 60             metrics.setdefault(key, 0.0)
# 61         self.logger.info("Experiment metrics: %s", metrics)
# 62 
# 63         #  persist scoring logs
# 64         conn = init_db()
# 65         scoring_logs = [
# 66             ScoringLog(
# 67                 experiment_id="exp",
# 68                 round=0,
# 69                 metric=k,
# 70                 value=v,
# 71             )
# 72             for k, v in metrics.items()
# 73         ]
# 74         insert_logs(conn, "scoring_log", scoring_logs)
# 75         conn.close()
# 76 
# 77         self.metrics = metrics
# 78         self.logger.debug("Experiment run end")
# 79         return metrics
# 80 
# 81     def _run_experiment_logic(self, *args, **kwargs) -> None:
# 82         """Override to implement experiment steps."""
# 83         raise NotImplementedError


# abstract_classes\prompt_generator_base.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from abc import ABC, abstractmethod
# 5 
# 6 
# 7 class PromptGeneratorBase(ABC):
# 8     """Base class for generating prompts."""
# 9 
# 10     def __init__(self) -> None:
# 11         self.logger = logging.getLogger(self.__class__.__name__)
# 12 
# 13     def generate_prompt(self, *args, **kwargs) -> str:
# 14         self.logger.debug("Prompt generation start")
# 15         prompt = self._generate_prompt(*args, **kwargs)
# 16         self.logger.debug("Prompt generation end")
# 17         return prompt
# 18 
# 19     @abstractmethod
# 20     def _generate_prompt(self, *args, **kwargs) -> str:
# 21         """Return a generated prompt."""
# 22         raise NotImplementedError


# abstract_classes\scoring_provider_base.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from abc import ABC, abstractmethod
# 5 
# 6 
# 7 class ScoringProviderBase(ABC):
# 8     """Base class for computing evaluation metrics."""
# 9 
# 10     def __init__(self) -> None:
# 11         self.logger = logging.getLogger(self.__class__.__name__)
# 12 
# 13     def score(self, *args, **kwargs):
# 14         self.logger.debug("Scoring start")
# 15         result = self._score(*args, **kwargs)
# 16         self.logger.debug("Scoring end")
# 17         return result
# 18 
# 19     @abstractmethod
# 20     def _score(self, *args, **kwargs):
# 21         """Compute evaluation metrics."""
# 22         raise NotImplementedError


# abstract_classes\state_manager_base.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from abc import ABC, abstractmethod
# 5 
# 6 
# 7 class StateManagerBase(ABC):
# 8     """Base class for managing state-level execution."""
# 9 
# 10     def __init__(self) -> None:
# 11         self.logger = logging.getLogger(self.__class__.__name__)
# 12 
# 13     def run(self, *args, **kwargs) -> None:
# 14         self.logger.debug("StateManager run start")
# 15         self._run_state_logic(*args, **kwargs)
# 16         self.logger.debug("StateManager run end")
# 17 
# 18     @abstractmethod
# 19     def _run_state_logic(self, *args, **kwargs) -> None:
# 20         """Execute state specific logic."""
# 21         raise NotImplementedError


# abstract_classes\system_manager_base.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from abc import ABC, abstractmethod
# 5 
# 6 
# 7 class SystemManagerBase(ABC):
# 8     """Base class for coordinating high level system logic."""
# 9 
# 10     def __init__(self) -> None:
# 11         self.logger = logging.getLogger(self.__class__.__name__)
# 12 
# 13     def run(self, *args, **kwargs) -> None:
# 14         self.logger.debug("SystemManager run start")
# 15         self._run_system_logic(*args, **kwargs)
# 16         self.logger.debug("SystemManager run end")
# 17 
# 18     @abstractmethod
# 19     def _run_system_logic(self, *args, **kwargs) -> None:
# 20         """Execute system-specific logic and state transitions."""
# 21         raise NotImplementedError


# abstract_classes\tool_provider_base.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from abc import ABC, abstractmethod
# 5 
# 6 
# 7 class ToolProviderBase(ABC):
# 8     """Base class for running external tools."""
# 9 
# 10     def __init__(self) -> None:
# 11         self.logger = logging.getLogger(self.__class__.__name__)
# 12 
# 13     def run(self, *args, **kwargs):
# 14         self.logger.debug("Tool run start")
# 15         result = self._run(*args, **kwargs)
# 16         self.logger.debug("Tool run end")
# 17         return result
# 18 
# 19     @abstractmethod
# 20     def _run(self, *args, **kwargs):
# 21         """Run tool-specific logic."""
# 22         raise NotImplementedError


# bootstrap.py

# 1 from __future__ import annotations
# 2 
# 3 from importlib import import_module
# 4 from pathlib import Path
# 5 from typing import Type
# 6 
# 7 from .factories.agent import AgentFactory
# 8 from .factories.prompt_manager import PromptGeneratorFactory
# 9 from .registries.context_providers import CONTEXT_PROVIDER_REGISTRY
# 10 from .factories.tool_provider import ToolProviderFactory
# 11 from .factories.scoring_provider import ScoringProviderFactory
# 12 from .factories.state_manager import StateManagerFactory
# 13 from .factories.experiment_config_provider import ExperimentConfigProvider
# 14 from .abstract_classes.agent_base import AgentBase
# 15 from .abstract_classes.prompt_generator_base import PromptGeneratorBase
# 16 from .abstract_classes.tool_provider_base import ToolProviderBase
# 17 from .abstract_classes.scoring_provider_base import ScoringProviderBase
# 18 from .abstract_classes.state_manager_base import StateManagerBase
# 19 
# 20 from .utilities.schema import initialize_database
# 21 
# 22 
# 23 def _import_class(path: str | Path, base_cls: Type) -> Type:
# 24     mod_path = str(path).replace("\\", "/")
# 25     if mod_path.endswith(".py"):
# 26         mod_path = mod_path[:-3]
# 27     mod_path = mod_path.replace("/", ".")
# 28     module = import_module(f"app.extensions.{mod_path}")
# 29     for attr in module.__dict__.values():
# 30         if (
# 31             isinstance(attr, type)
# 32             and issubclass(attr, base_cls)
# 33             and attr is not base_cls
# 34         ):
# 35             return attr
# 36     raise ImportError(f"No subclass of {base_cls.__name__} found in {module.__name__}")
# 37 
# 38 
# 39 def seed_registries(reset_db: bool = False) -> None:
# 40     conn = initialize_database(reset=reset_db)
# 41     cur = conn.cursor()
# 42 
# 43     #  Tool providers
# 44     for row in cur.execute("SELECT name, artifact_path FROM tooling_provider"):
# 45         name, artifact = row
# 46         path = Path(artifact)
# 47         cls = _import_class(path.as_posix(), ToolProviderBase)
# 48         try:
# 49             ToolProviderFactory.register(name, cls)
# 50         except KeyError:
# 51             pass
# 52 
# 53     #  Context providers
# 54     for row in cur.execute("SELECT name FROM context_provider"):
# 55         (name,) = row
# 56         #  Assume providers are already registered via extensions
# 57         if CONTEXT_PROVIDER_REGISTRY.get(name) is not None:
# 58             continue
# 59 
# 60     #  Prompt generators
# 61     for row in cur.execute("SELECT name, artifact_path FROM prompt_generator"):
# 62         name, artifact = row
# 63         path = Path(artifact)
# 64         cls = _import_class(path.as_posix(), PromptGeneratorBase)
# 65         try:
# 66             PromptGeneratorFactory.register(name, cls)
# 67         except KeyError:
# 68             pass
# 69 
# 70     #  Agents
# 71     for row in cur.execute("SELECT name, artifact_path FROM agent_config"):
# 72         name, artifact = row
# 73         path = Path(artifact)
# 74         cls = _import_class(path.as_posix(), AgentBase)
# 75         try:
# 76             AgentFactory.register(name, cls)
# 77         except KeyError:
# 78             pass
# 79 
# 80     #  Scoring providers
# 81     for row in cur.execute("SELECT name, artifact_path FROM scoring_provider"):
# 82         name, artifact = row
# 83         path = Path(artifact)
# 84         cls = _import_class(path.as_posix(), ScoringProviderBase)
# 85         try:
# 86             ScoringProviderFactory.register(name, cls)
# 87         except KeyError:
# 88             pass
# 89 
# 90     #  State managers
# 91     for row in cur.execute("SELECT name, artifact_path FROM state_manager"):
# 92         name, artifact = row
# 93         path = Path(artifact)
# 94         cls = _import_class(path.as_posix(), StateManagerBase)
# 95         try:
# 96             StateManagerFactory.register(name, cls)
# 97         except KeyError:
# 98             pass
# 99 
# 100     #  System managers are loaded via extensions; only register configs
# 101     for row in cur.execute(
# 102         "SELECT id, system_manager_id, scoring_model_id FROM experiment_config"
# 103     ):
# 104         config_id, system_manager_id, scoring_model_id = row
# 105         config = {
# 106             "system_manager_id": system_manager_id,
# 107             "scoring_model_id": scoring_model_id,
# 108         }
# 109         ExperimentConfigProvider.register(config_id, config)
# 110 
# 111     conn.close()
# 112 
# 113 
# 114 if __name__ == "__main__":
# 115     seed_registries(reset_db=True)


# enums\__init__.py



# enums\agent_enums.py

# 1 from __future__ import annotations
# 2 
# 3 from enum import Enum
# 4 
# 5 
# 6 class AgentRole(str, Enum):
# 7     GENERATOR = "generator"
# 8     DISCRIMINATOR = "discriminator"
# 9     MEDIATOR = "mediator"
# 10     PATCHER = "patcher"
# 11     EVALUATOR = "evaluator"
# 12 
# 13 
# 14 class AgentState(Enum):
# 15     INIT = "init"
# 16     RUNNING = "running"
# 17     COMPLETE = "complete"


# enums\logging_enums.py

# 1 from enum import Enum
# 2 
# 3 
# 4 class LogType(str, Enum):
# 5     EXPERIMENT = "experiment"
# 6     STATE = "state"
# 7     STATE_TRANSITION = "state_transition"
# 8     PROMPT = "prompt"
# 9     CONVERSATION = "conversation"
# 10     SCORING = "scoring"
# 11     CODE_QUALITY = "code_quality"
# 12     ERROR = "error"


# enums\scoring_enums.py

# 1 from enum import Enum
# 2 
# 3 
# 4 class ScoringMetric(str, Enum):
# 5     FUNCTIONAL_CORRECTNESS = "functional_correctness"
# 6     MAINTAINABILITY_INDEX = "maintainability_index"
# 7     BUG_FIX_SUCCESS = "bug_fix_success_rate"
# 8     LINTING_COMPLIANCE = "linting_compliance"
# 9     AGENT_SUCCESS_RATE = "agent_role_success_rate"
# 10     RETRY_SUCCESS_RATE = "retry_success_rate"


# enums\system_enums.py

# 1 from __future__ import annotations
# 2 
# 3 from enum import Enum
# 4 
# 5 
# 6 class SystemType(str, Enum):
# 7     #  Core Code Transformation
# 8     LINTING = "linting"
# 9     FORMATTING = "formatting"
# 10     DOCSTRING = "docstring"
# 11     TYPE_ANNOTATION = "type_annotation"
# 12     REFACTORING = "refactoring"
# 13     DEAD_CODE_REMOVAL = "dead_code_removal"
# 14     COMPLEXITY_REDUCTION = "complexity_reduction"
# 15 
# 16     #  Testing and Validation
# 17     UNIT_TESTING = "unit_testing"
# 18     EDGE_TESTING = "edge_testing"
# 19     INTEGRATION_TESTING = "integration_testing"
# 20     TEST_FIX_GENERATION = "test_fix_generation"
# 21     MOCK_GENERATION = "mock_generation"
# 22     STUB_EXTRACTION = "stub_extraction"
# 23 
# 24     #  Observability and Instrumentation
# 25     LOGGING_INJECTION = "logging_injection"
# 26     TRACE_ANNOTATION = "trace_annotation"
# 27     ERROR_HANDLING = "error_handling"
# 28 
# 29     #  Documentation and Explanation
# 30     INLINE_COMMENTING = "inline_commenting"
# 31     FUNCTION_SUMMARIZATION = "function_summarization"
# 32     FILE_SUMMARIZATION = "file_summarization"
# 33     CHANGE_SUMMARIZATION = "change_summarization"
# 34 
# 35     #  Review and Oversight
# 36     CODE_REVIEW = "code_review"
# 37     STATIC_ANALYSIS = "static_analysis"
# 38     REGRESSION_ANALYSIS = "regression_analysis"
# 39     MEDIATOR_RECONCILIATION = "mediator_reconciliation"
# 40     SCORING_EVALUATION = "scoring_evaluation"
# 41 
# 42     #  Planning and Organization
# 43     SYMBOL_GRAPH_GENERATION = "symbol_graph_generation"
# 44     DEPENDENCY_MAPPING = "dependency_mapping"
# 45     FILE_RESTRUCTURING = "file_restructuring"
# 46     FUNCTION_SPLITTING = "function_splitting"
# 47 
# 48     #  MLOps-Specific
# 49     DATA_VALIDATION = "data_validation"
# 50     METADATA_ANNOTATION = "metadata_annotation"
# 51     MODEL_DOC_GENERATION = "model_doc_generation"
# 52     PIPELINE_TESTING = "pipeline_testing"
# 53     TRAINING_CONFIG_AUDIT = "training_config_audit"
# 54     FEATURE_TRACEABILITY = "feature_traceability"
# 55 
# 56     #  Production Safety
# 57     SECURITY_CHECK = "security_check"
# 58     POLICY_COMPLIANCE = "policy_compliance"
# 59     LICENSE_CHECK = "license_check"
# 60 
# 61     #  Misc / Cross-cutting
# 62     PATCHING = "patching"
# 63     PROMPT_REFINEMENT = "prompt_refinement"
# 64     AGENT_RECOMMENDATION = "agent_recommendation"
# 65     SEMANTIC_DIFF = "semantic_diff"
# 66     CONTEXT_FILTERING = "context_filtering"
# 67 
# 68 
# 69 class SystemState(Enum):
# 70     """Finite state machine states for the CodeCritic system."""
# 71 
# 72     START = "start"
# 73     GENERATE = "generate"
# 74     DISCRIMINATE = "discriminate"
# 75     MEDIATE = "mediate"
# 76     PATCH = "patch"
# 77     EVALUATE = "evaluate"
# 78     END = "end"
# 79 
# 80 
# 81 class StateTransitionReason(str, Enum):
# 82     FIRST_ROUND = "first_round"
# 83     MAX_ITERATIONS_REACHED = "max_iterations_reached"
# 84     SCORE_THRESHOLD_MET = "score_threshold_met"
# 85     SCORE_STAGNATION = "score_stagnation"
# 86     AGENT_FAILURE = "agent_failure"
# 87     MEDIATOR_OVERRIDE = "mediator_override"
# 88     PATCH_RETRY = "patch_retry"
# 89     CUSTOM_RULE = "custom_rule"
# 90     END_REACHED = "end_reached"


# extensions\__init__.py



# extensions\agent_prompts\__init__.py

# 1 


# extensions\agents\__init__.py

# 1 from .dummy_agent import DummyAgent
# 2 from .generator_agent import GeneratorAgent
# 3 from .evaluator_agent import EvaluatorAgent
# 4 from ...registries.agents import AGENT_REGISTRY
# 5 
# 6 AGENT_REGISTRY.register("dummy", DummyAgent)
# 7 AGENT_REGISTRY.register("generator", GeneratorAgent)
# 8 AGENT_REGISTRY.register("evaluator", EvaluatorAgent)
# 9 
# 10 __all__ = ["DummyAgent", "GeneratorAgent", "EvaluatorAgent"]


# extensions\agents\dummy_agent.py

# 1 from ...abstract_classes.agent_base import AgentBase
# 2 
# 3 
# 4 class DummyAgent(AgentBase):
# 5     def _run_agent_logic(self, *args, **kwargs) -> None:
# 6         self.logger.info("Dummy agent logic executed")


# extensions\agents\evaluator_agent.py

# 1 from __future__ import annotations
# 2 from typing import List
# 3 from ...abstract_classes.agent_base import AgentBase
# 4 from ...factories.tool_provider import ToolProviderFactory
# 5 from ...utilities.metadata.logging import CodeQualityLog, ErrorLog
# 6 
# 7 
# 8 class EvaluatorAgent(AgentBase):
# 9     def __init__(self, target: str) -> None:
# 10         super().__init__()
# 11         self.target = target
# 12         self.mypy = ToolProviderFactory.create("mypy")
# 13         self.ruff = ToolProviderFactory.create("ruff")
# 14         self.radon = ToolProviderFactory.create("radon")
# 15         self.quality_logs: List[CodeQualityLog] = []
# 16         self.error_logs: List[ErrorLog] = []
# 17 
# 18     def _run_agent_logic(self, *args, **kwargs) -> None:
# 19         complexity = 0.0
# 20         try:
# 21             self.mypy.run(self.target)
# 22             ruff_proc = self.ruff.run(self.target)
# 23             radon_proc = self.radon.run(self.target)
# 24 
# 25             #  Parse "Complexity: X.Y" from radon output
# 26             for line in radon_proc.stdout.splitlines():
# 27                 if "Complexity:" in line:
# 28                     try:
# 29                         complexity = float(line.split("Complexity:")[1].strip())
# 30                     except ValueError:
# 31                         pass
# 32                     break
# 33 
# 34         except Exception as exc:
# 35             self.logger.warning("Radon unavailable: %s", exc)
# 36 
# 37         lines = sum(1 for _ in open(self.target, "r", encoding="utf-8"))
# 38         log = CodeQualityLog(
# 39             experiment_id="exp",
# 40             round=0,
# 41             symbol=self.target,
# 42             lines_of_code=lines,
# 43             cyclomatic_complexity=complexity,
# 44             maintainability_index=0.0,
# 45             lint_errors=0 if ruff_proc.returncode == 0 else 1,
# 46         )
# 47         self.quality_logs.append(log)


# extensions\agents\generator_agent.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import List
# 4 from datetime import datetime, timezone
# 5 
# 6 from ...abstract_classes.agent_base import AgentBase
# 7 from ...enums.agent_enums import AgentRole
# 8 from ...factories.tool_provider import ToolProviderFactory
# 9 from ...utilities.metadata.logging import ErrorLog, PromptLog
# 10 
# 11 
# 12 class GeneratorAgent(AgentBase):
# 13     """Agent responsible for formatting code using black."""
# 14 
# 15     def __init__(self, target: str) -> None:
# 16         super().__init__()
# 17         self.target = target
# 18         self.formatter = ToolProviderFactory.create("black")
# 19         self.docformatter = ToolProviderFactory.create("docformatter")
# 20         self.prompt_logs: List[PromptLog] = []
# 21         self.error_logs: List[ErrorLog] = []
# 22 
# 23     def _run_agent_logic(self, *args, **kwargs) -> None:
# 24         log = PromptLog(
# 25             experiment_id="exp",
# 26             round=0,
# 27             system="system",
# 28             agent_id="generator",
# 29             agent_role=AgentRole.GENERATOR,
# 30             symbol=self.target,
# 31             prompt="format code",
# 32             response=None,
# 33             attempt_number=0,
# 34             agent_action_outcome="started",
# 35         )
# 36         self.prompt_logs.append(log)
# 37         try:
# 38             self.formatter.run(self.target)
# 39             self.docformatter.run(self.target)
# 40             log.agent_action_outcome = "success"
# 41             self.logger.info("Formatted %s", self.target)
# 42         except Exception as exc:
# 43             log.agent_action_outcome = "error"
# 44             err = ErrorLog(
# 45                 experiment_id="exp",
# 46                 round=0,
# 47                 error_type=type(exc).__name__,
# 48                 message=str(exc),
# 49                 file_path=self.target,
# 50             )
# 51             self.error_logs.append(err)
# 52             self.logger.exception("Formatting failed for %s", self.target)
# 53         finally:
# 54             log.stop = datetime.now(timezone.utc)


# extensions\context_providers\__init__.py

# 1 from .dummy_context_provider import DummyContextProvider
# 2 from .symbol_graph_provider import SymbolGraphProvider
# 3 from ...registries.context_providers import CONTEXT_PROVIDER_REGISTRY
# 4 
# 5 CONTEXT_PROVIDER_REGISTRY.register("dummy", DummyContextProvider)
# 6 CONTEXT_PROVIDER_REGISTRY.register("symbol_graph", SymbolGraphProvider)
# 7 
# 8 __all__ = ["DummyContextProvider", "SymbolGraphProvider"]


# extensions\context_providers\dummy_context_provider.py

# 1 from ...abstract_classes.context_provider_base import ContextProviderBase
# 2 
# 3 
# 4 class DummyContextProvider(ContextProviderBase):
# 5     def _get_context(self, *args, **kwargs):
# 6         self.logger.info("Dummy context provided")
# 7         return {}


# extensions\context_providers\symbol_graph_provider.py

# 1 from __future__ import annotations
# 2 
# 3 import ast
# 4 from pathlib import Path
# 5 from typing import Any, Dict, List, Set
# 6 
# 7 from ...abstract_classes.context_provider_base import ContextProviderBase
# 8 
# 9 
# 10 class SymbolGraphProvider(ContextProviderBase):
# 11     """Generate a simple symbol graph for a Python module."""
# 12 
# 13     def __init__(self, module_path: str) -> None:
# 14         super().__init__()
# 15         self.module_path = Path(module_path)
# 16 
# 17     def _get_context(self) -> Dict[str, Any]:
# 18         """Parse the module and return context information."""
# 19         if not self.module_path.exists():
# 20             self.logger.error("Module not found: %s", self.module_path)
# 21             return {}
# 22 
# 23         source = self.module_path.read_text(encoding="utf-8")
# 24         tree = ast.parse(source)
# 25 
# 26         functions: List[str] = []
# 27         classes: Dict[str, List[str]] = {}
# 28         call_map: Dict[str, Set[str]] = {}
# 29 
# 30         for node in tree.body:
# 31             if isinstance(node, ast.FunctionDef):
# 32                 sig = self._format_signature(node)
# 33                 functions.append(sig)
# 34                 call_map[node.name] = self._collect_calls(node)
# 35             elif isinstance(node, ast.ClassDef):
# 36                 method_sigs: List[str] = []
# 37                 for item in node.body:
# 38                     if isinstance(item, ast.FunctionDef):
# 39                         sig = self._format_signature(item)
# 40                         method_sigs.append(sig)
# 41                         key = f"{node.name}.{item.name}"
# 42                         call_map[key] = self._collect_calls(item)
# 43                 classes[node.name] = method_sigs
# 44 
# 45         context = {
# 46             "functions": functions,
# 47             "classes": classes,
# 48             "call_map": {k: sorted(v) for k, v in call_map.items()},
# 49         }
# 50         self.logger.info("Context generated for %s", self.module_path)
# 51         return context
# 52 
# 53     def _format_signature(self, func: ast.FunctionDef) -> str:
# 54         args = [arg.arg for arg in func.args.args]
# 55         if func.args.vararg:
# 56             args.append("*" + func.args.vararg.arg)
# 57         for kw in func.args.kwonlyargs:
# 58             args.append(kw.arg)
# 59         if func.args.kwarg:
# 60             args.append("**" + func.args.kwarg.arg)
# 61         return f"{func.name}({', '.join(args)})"
# 62 
# 63     def _collect_calls(self, func: ast.FunctionDef) -> Set[str]:
# 64         calls: Set[str] = set()
# 65         for node in ast.walk(func):
# 66             if isinstance(node, ast.Call):
# 67                 if isinstance(node.func, ast.Name):
# 68                     calls.add(node.func.id)
# 69                 elif isinstance(node.func, ast.Attribute):
# 70                     calls.add(node.func.attr)
# 71         return calls


# extensions\prompt_generators\__init__.py

# 1 from .dummy_prompt_generator import DummyPromptGenerator
# 2 from .basic_prompt_generator import BasicPromptGenerator
# 3 from ...registries.prompt_generators import PROMPT_GENERATOR_REGISTRY
# 4 
# 5 PROMPT_GENERATOR_REGISTRY.register("dummy", DummyPromptGenerator)
# 6 PROMPT_GENERATOR_REGISTRY.register("basic", BasicPromptGenerator)
# 7 
# 8 __all__ = ["DummyPromptGenerator", "BasicPromptGenerator"]


# extensions\prompt_generators\basic_prompt_generator.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Any, Dict
# 4 
# 5 from ...abstract_classes.prompt_generator_base import PromptGeneratorBase
# 6 from ...abstract_classes.context_provider_base import ContextProviderBase
# 7 
# 8 
# 9 class BasicPromptGenerator(PromptGeneratorBase):
# 10     """Combine system and agent templates with code context."""
# 11 
# 12     def __init__(self, context_provider: ContextProviderBase) -> None:
# 13         super().__init__()
# 14         self.context_provider = context_provider
# 15 
# 16     def _generate_prompt(
# 17         self, agent_config: Dict[str, Any], system_config: Dict[str, Any]
# 18     ) -> str:
# 19         context = self.context_provider.get_context()
# 20         self.logger.debug("Raw context: %s", context)
# 21 
# 22         system_template = system_config.get("template", "")
# 23         agent_template = agent_config.get("template", "")
# 24 
# 25         context_snippet = ""
# 26         if context:
# 27             funcs = ", ".join(context.get("functions", []))
# 28             classes = ", ".join(context.get("classes", {}).keys())
# 29             context_snippet = f"Functions: {funcs}\nClasses: {classes}"
# 30 
# 31         prompt = f"{system_template}\n\n{agent_template}\n\n{context_snippet}"
# 32         self.logger.info("Prompt generated")
# 33         self.logger.debug("Prompt content: %s", prompt)
# 34         return prompt


# extensions\prompt_generators\dummy_prompt_generator.py

# 1 from ...abstract_classes.prompt_generator_base import PromptGeneratorBase
# 2 
# 3 
# 4 class DummyPromptGenerator(PromptGeneratorBase):
# 5     def _generate_prompt(self, *args, **kwargs) -> str:
# 6         self.logger.info("Dummy prompt generated")
# 7         return "dummy prompt"


# extensions\scoring_models\__init__.py

# 1 from .dummy_scoring_provider import DummyScoringProvider
# 2 from .basic_scoring_provider import BasicScoringProvider
# 3 from ...registries.scoring_models import SCORING_MODEL_REGISTRY
# 4 
# 5 SCORING_MODEL_REGISTRY.register("dummy", DummyScoringProvider)
# 6 SCORING_MODEL_REGISTRY.register("basic", BasicScoringProvider)
# 7 
# 8 __all__ = ["DummyScoringProvider", "BasicScoringProvider"]


# extensions\scoring_models\basic_scoring_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Any, Dict, List
# 4 
# 5 from ...abstract_classes.scoring_provider_base import ScoringProviderBase
# 6 from ...utilities.metrics import compute_metrics
# 7 
# 8 
# 9 class BasicScoringProvider(ScoringProviderBase):
# 10     """Compute experiment metrics using basic heuristics."""
# 11 
# 12     def _score(self, logs: Dict[str, List[Any]]) -> Dict[str, float]:
# 13         self.logger.debug("Scoring with logs: %s", logs)
# 14         evaluation_logs = logs.get("evaluation", [])
# 15         code_quality_logs = logs.get("code_quality", [])
# 16         conversation_logs = logs.get("conversation", [])
# 17         prompt_logs = logs.get("prompt", [])
# 18         state_logs = logs.get("state", [])
# 19 
# 20         metrics = compute_metrics(
# 21             evaluation_logs,
# 22             code_quality_logs,
# 23             conversation_logs,
# 24             prompt_logs,
# 25             state_logs,
# 26         )
# 27         self.logger.info("Computed metrics: %s", metrics)
# 28         return metrics


# extensions\scoring_models\dummy_scoring_provider.py

# 1 from ...abstract_classes.scoring_provider_base import ScoringProviderBase
# 2 
# 3 
# 4 class DummyScoringProvider(ScoringProviderBase):
# 5     def _score(self, *args, **kwargs):
# 6         self.logger.info("Dummy scoring")
# 7         return 0


# extensions\state_managers\__init__.py

# 1 from .dummy_state_manager import DummyStateManager
# 2 from .state_manager import StateManager
# 3 from ...registries.state_managers import STATE_MANAGER_REGISTRY
# 4 
# 5 STATE_MANAGER_REGISTRY.register("dummy", DummyStateManager)
# 6 STATE_MANAGER_REGISTRY.register("state", StateManager)
# 7 
# 8 __all__ = ["DummyStateManager", "StateManager"]


# extensions\state_managers\dummy_state_manager.py

# 1 from ...abstract_classes.state_manager_base import StateManagerBase
# 2 
# 3 
# 4 class DummyStateManager(StateManagerBase):
# 5     def _run_state_logic(self, *args, **kwargs) -> None:
# 6         self.logger.info("Dummy state logic executed")


# extensions\state_managers\state_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ...abstract_classes.state_manager_base import StateManagerBase
# 4 
# 5 
# 6 class StateManager(StateManagerBase):
# 7     """Concrete manager executing actions within each FSM state."""
# 8 
# 9     def _run_state_logic(self, *args, **kwargs) -> None:
# 10         state = kwargs.get("state")
# 11         self.logger.info("Running state logic for %s", state)


# extensions\system_managers\__init__.py

# 1 from .dummy_system_manager import DummySystemManager
# 2 from .system_manager import SystemManager
# 3 from ...registries.system_managers import SYSTEM_MANAGER_REGISTRY
# 4 
# 5 SYSTEM_MANAGER_REGISTRY.register("dummy", DummySystemManager)
# 6 SYSTEM_MANAGER_REGISTRY.register("system", SystemManager)
# 7 
# 8 __all__ = ["DummySystemManager", "SystemManager"]


# extensions\system_managers\dummy_system_manager.py

# 1 from ...abstract_classes.system_manager_base import SystemManagerBase
# 2 
# 3 
# 4 class DummySystemManager(SystemManagerBase):
# 5     def _run_system_logic(self, *args, **kwargs) -> None:
# 6         self.logger.info("Dummy system logic executed")


# extensions\system_managers\system_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import List
# 4 
# 5 from ...abstract_classes.system_manager_base import SystemManagerBase
# 6 from ...enums.system_enums import SystemState, StateTransitionReason
# 7 from ...factories.agent import AgentFactory
# 8 from ...factories.context_provider import ContextProviderFactory
# 9 from ...factories.prompt_manager import PromptGeneratorFactory
# 10 from ...utilities.db import init_db, insert_logs
# 11 from ...utilities.metadata.logging import (
# 12     CodeQualityLog,
# 13     PromptLog,
# 14     ScoringLog,
# 15     StateLog,
# 16     StateTransitionLog,
# 17 )
# 18 from ..state_managers.state_manager import StateManager
# 19 
# 20 
# 21 class SystemManager(SystemManagerBase):
# 22     def __init__(self) -> None:
# 23         super().__init__()
# 24         self.state_manager = StateManager()
# 25         self.current_state = SystemState.START
# 26         self.transition_logs: List[StateTransitionLog] = []
# 27         self.state_logs: List[StateLog] = []
# 28         self.prompt_logs: List[PromptLog] = []
# 29         self.code_quality_logs: List[CodeQualityLog] = []
# 30         self.scoring_logs: List[ScoringLog] = []
# 31 
# 32         self.context_provider = ContextProviderFactory.create(
# 33             "symbol_graph", module_path="sample_module.py"
# 34         )
# 35         self.prompt_generator = PromptGeneratorFactory.create(
# 36             "basic", context_provider=self.context_provider
# 37         )
# 38         self.generator = AgentFactory.create("generator", target="sample_module.py")
# 39         self.evaluator = AgentFactory.create("evaluator", target="sample_module.py")
# 40 
# 41     def _transition_to(
# 42         self, next_state: SystemState, reason: StateTransitionReason
# 43     ) -> None:
# 44         log = StateTransitionLog(
# 45             experiment_id="exp",
# 46             round=0,
# 47             from_state=self.current_state,
# 48             to_state=next_state,
# 49             reason=reason,
# 50         )
# 51         self.transition_logs.append(log)
# 52         self.logger.info("%s -> %s", self.current_state.value, next_state.value)
# 53         self.current_state = next_state
# 54 
# 55     def _run_system_logic(self, *args, **kwargs) -> None:
# 56         sequence = [
# 57             SystemState.GENERATE,
# 58             SystemState.DISCRIMINATE,
# 59             SystemState.MEDIATE,
# 60             SystemState.PATCH,
# 61             SystemState.EVALUATE,
# 62             SystemState.END,
# 63         ]
# 64         conn = init_db()
# 65         for state in sequence:
# 66             self._transition_to(state, reason=StateTransitionReason.FIRST_ROUND)
# 67             if state is not SystemState.END:
# 68                 self.state_manager.run(state=state.value)
# 69                 self.state_logs.append(
# 70                     StateLog(
# 71                         experiment_id="exp",
# 72                         system="system",
# 73                         round=0,
# 74                         state=state,
# 75                         action="run",
# 76                     )
# 77                 )
# 78                 if state == SystemState.GENERATE:
# 79                     self.generator.run()
# 80                     self.prompt_logs.extend(self.generator.prompt_logs)
# 81                 elif state == SystemState.DISCRIMINATE:
# 82                     self.evaluator.run()
# 83                     self.code_quality_logs.extend(self.evaluator.quality_logs)
# 84 
# 85         insert_logs(conn, "state_transition_log", self.transition_logs)
# 86         insert_logs(conn, "state_log", self.state_logs)
# 87         insert_logs(conn, "prompt_log", self.prompt_logs)
# 88         insert_logs(conn, "code_quality_log", self.code_quality_logs)
# 89         if self.scoring_logs:
# 90             insert_logs(conn, "scoring_log", self.scoring_logs)
# 91         conn.close()


# extensions\system_prompts\__init__.py

# 1 


# extensions\tool_providers\__init__.py

# 1 from .dummy_tool_provider import DummyToolProvider
# 2 from .black_runner import BlackToolProvider
# 3 from .mypy_runner import MypyToolProvider
# 4 from .radon_runner import RadonToolProvider
# 5 from .ruff_runner import RuffToolProvider
# 6 from .docformatter_runner import DocFormatterToolProvider
# 7 from ...registries.tool_providers import TOOL_PROVIDER_REGISTRY
# 8 
# 9 TOOL_PROVIDER_REGISTRY.register("dummy", DummyToolProvider)
# 10 TOOL_PROVIDER_REGISTRY.register("black", BlackToolProvider)
# 11 TOOL_PROVIDER_REGISTRY.register("mypy", MypyToolProvider)
# 12 TOOL_PROVIDER_REGISTRY.register("radon", RadonToolProvider)
# 13 TOOL_PROVIDER_REGISTRY.register("ruff", RuffToolProvider)
# 14 TOOL_PROVIDER_REGISTRY.register("docformatter", DocFormatterToolProvider)
# 15 
# 16 __all__ = [
# 17     "DummyToolProvider",
# 18     "BlackToolProvider",
# 19     "MypyToolProvider",
# 20     "RadonToolProvider",
# 21     "RuffToolProvider",
# 22     "DocFormatterToolProvider",
# 23 ]


# extensions\tool_providers\black_runner.py

# 1 from __future__ import annotations
# 2 import subprocess
# 3 import sys
# 4 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 5 
# 6 
# 7 class BlackToolProvider(ToolProviderBase):
# 8     def _run(self, target: str, check: bool = False):
# 9         #  Use quiet mode to suppress emojis and extra output on Windows
# 10         cmd = [sys.executable, "-m", "black", "--quiet", target]
# 11         if check:
# 12             cmd.append("--check")
# 13         proc = subprocess.run(
# 14             cmd, capture_output=True, text=True, encoding="utf-8", errors="ignore"
# 15         )
# 16         if proc.stdout:
# 17             self.logger.debug(proc.stdout)
# 18         if proc.stderr:
# 19             self.logger.error(proc.stderr)
# 20         if proc.returncode != 0:
# 21             raise RuntimeError(f"black failed: {proc.stderr}")
# 22         return proc


# extensions\tool_providers\docformatter_runner.py

# 1 import subprocess
# 2 import sys
# 3 from app.abstract_classes.tool_provider_base import ToolProviderBase
# 4 
# 5 
# 6 class DocFormatterToolProvider(ToolProviderBase):
# 7     def _run(self, target: str, check: bool = False):
# 8         cmd = [sys.executable, "-m", "docformatter", target, "--in-place"]
# 9         if check:
# 10             cmd.append("--check")
# 11         proc = subprocess.run(cmd, capture_output=True, text=True)
# 12         if proc.stdout:
# 13             self.logger.debug(proc.stdout)
# 14         if proc.stderr:
# 15             self.logger.error(proc.stderr)
# 16         if proc.returncode != 0:
# 17             raise RuntimeError(f"docformatter failed: {proc.stderr}")
# 18         return proc


# extensions\tool_providers\dummy_tool_provider.py

# 1 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 2 
# 3 
# 4 class DummyToolProvider(ToolProviderBase):
# 5     def _run(self, *args, **kwargs):
# 6         self.logger.info("Dummy tool run")
# 7         return True


# extensions\tool_providers\mypy_runner.py

# 1 from __future__ import annotations
# 2 
# 3 import subprocess
# 4 import sys
# 5 
# 6 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 7 
# 8 
# 9 class MypyToolProvider(ToolProviderBase):
# 10     def _run(self, target: str):
# 11         cmd = [sys.executable, "-m", "mypy", target]
# 12         proc = subprocess.run(cmd, capture_output=True, text=True)
# 13         if proc.stdout:
# 14             self.logger.debug(proc.stdout)
# 15         if proc.stderr:
# 16             self.logger.error(proc.stderr)
# 17         if proc.returncode not in (0, 1):  #  mypy returns 1 if it finds issues
# 18             raise RuntimeError(f"mypy execution error: {proc.stderr}")
# 19         return proc


# extensions\tool_providers\radon_runner.py

# 1 from __future__ import annotations
# 2 import subprocess
# 3 import sys
# 4 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 5 
# 6 
# 7 class RadonToolProvider(ToolProviderBase):
# 8     def _run(self, target: str):
# 9         #  Call radon via -m so it returns a CompletedProcess
# 10         cmd = [sys.executable, "-m", "radon", "cc", target]
# 11         proc = subprocess.run(cmd, capture_output=True, text=True)
# 12 
# 13         if proc.stdout:
# 14             self.logger.debug(proc.stdout)
# 15         if proc.stderr:
# 16             self.logger.error(proc.stderr)
# 17 
# 18         if proc.returncode != 0:
# 19             raise RuntimeError(f"radon failed: {proc.stderr.strip()}")
# 20 
# 21         return proc


# extensions\tool_providers\ruff_runner.py

# 1 from __future__ import annotations
# 2 
# 3 import subprocess
# 4 import sys
# 5 
# 6 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 7 
# 8 
# 9 class RuffToolProvider(ToolProviderBase):
# 10     def _run(self, target: str):
# 11         cmd = [sys.executable, "-m", "ruff", "check", target]
# 12         proc = subprocess.run(cmd, capture_output=True, text=True)
# 13         if proc.stdout:
# 14             self.logger.debug(proc.stdout)
# 15         if proc.stderr:
# 16             self.logger.error(proc.stderr)
# 17         if proc.returncode != 0:
# 18             raise RuntimeError(f"ruff failed: {proc.stderr}")
# 19         return proc


# extensions\tool_providers\sonarcloud_runner.py

# 1 from __future__ import annotations
# 2 
# 3 from app.abstract_classes.tool_provider_base import ToolProviderBase
# 4 
# 5 
# 6 class SonarCloudToolProvider(ToolProviderBase):
# 7     def _run(self, project_key: str, organization: str, token: str):
# 8         self.logger.info(
# 9             "SonarCloud stub executed for project '%s' in organization '%s'.",
# 10             project_key,
# 11             organization,
# 12         )
# 13         return {
# 14             "status": "success",
# 15             "project_key": project_key,
# 16             "organization": organization,
# 17             "analysis_url": "https://sonarcloud.io/dashboard?id=" + project_key,
# 18         }


# extensions\tools\__init__.py



# factories\__init__.py



# factories\agent.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.agents import AGENT_REGISTRY
# 4 
# 5 
# 6 class AgentFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             AGENT_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = AGENT_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"Agent {name} not registered")
# 19         return cls(**kwargs)


# factories\context_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.context_providers import CONTEXT_PROVIDER_REGISTRY
# 4 
# 5 
# 6 class ContextProviderFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             CONTEXT_PROVIDER_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = CONTEXT_PROVIDER_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"Context provider {name} not registered")
# 19         return cls(**kwargs)


# factories\experiment_config_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Dict, Any
# 4 
# 5 _CONFIGS: Dict[str, Dict[str, Any]] = {}
# 6 
# 7 
# 8 class ExperimentConfigProvider:
# 9     """Simple provider storing experiment configurations in memory."""
# 10 
# 11     @staticmethod
# 12     def register(config_id: int | str, config: Dict[str, Any]) -> None:
# 13         _CONFIGS[str(config_id)] = config
# 14 
# 15     @staticmethod
# 16     def load(config_id: int | str) -> Dict[str, Any]:
# 17         config = _CONFIGS.get(str(config_id))
# 18         if config is None:
# 19             raise KeyError(f"experiment config {config_id} not found")
# 20         return config


# factories\logging_provider.py

# 1 from __future__ import annotations
# 2 
# 3 import sqlite3
# 4 from pathlib import Path
# 5 from typing import Any, Iterable
# 6 from dataclasses import asdict, is_dataclass
# 7 from app.enums.logging_enums import LogType
# 8 from app.utilities.db import get_connection
# 9 from app.utilities.metadata.logging.log_schemas import (
# 10     StateLog,
# 11     StateTransitionLog,
# 12     PromptLog,
# 13     CodeQualityLog,
# 14     ErrorLog,
# 15     ScoringLog,
# 16     ConversationLog,
# 17     ExperimentLog,
# 18 )
# 19 
# 20 LOG_MODEL_MAP = {
# 21     LogType.STATE: StateLog,
# 22     LogType.STATE_TRANSITION: StateTransitionLog,
# 23     LogType.PROMPT: PromptLog,
# 24     LogType.CODE_QUALITY: CodeQualityLog,
# 25     LogType.ERROR: ErrorLog,
# 26     LogType.SCORING: ScoringLog,
# 27     LogType.CONVERSATION: ConversationLog,
# 28     LogType.EXPERIMENT: ExperimentLog,
# 29 }
# 30 
# 31 
# 32 class LoggingProvider:
# 33     def __init__(self, db_path: str | Path = "experiments/codecritic.sqlite3") -> None:
# 34         self.db_path = Path(db_path)
# 35         self.db_path.parent.mkdir(exist_ok=True, parents=True)
# 36         self.conn: sqlite3.Connection = get_connection()
# 37 
# 38     def _serialize(self, obj: Any) -> dict:
# 39         if not is_dataclass(obj) or isinstance(obj, type):
# 40             raise TypeError(f"Expected dataclass instance, got {type(obj)}")
# 41         return asdict(obj)
# 42 
# 43     def _insert_many(self, table: str, items: Iterable[dict]) -> None:
# 44         items = list(items)
# 45         if not items:
# 46             return
# 47         keys = list(items[0].keys())
# 48         cols = ",".join(keys)
# 49         placeholders = ",".join(["?"] * len(keys))
# 50         values = [tuple(i[k] for k in keys) for i in items]
# 51         cur = self.conn.cursor()
# 52         cur.executemany(f"INSERT INTO {table} ({cols}) VALUES ({placeholders})", values)
# 53         self.conn.commit()
# 54 
# 55     def write(self, log_type: LogType, entries: list[Any] | Any) -> None:
# 56         if not isinstance(entries, list):
# 57             entries = [entries]
# 58         model_cls = LOG_MODEL_MAP.get(log_type)
# 59         if model_cls is None:
# 60             raise ValueError(f"Unsupported log type: {log_type}")
# 61         for entry in entries:
# 62             if not isinstance(entry, model_cls):
# 63                 raise TypeError(f"Expected {model_cls.__name__}, got {type(entry)}")
# 64         serialized = [self._serialize(e) for e in entries]
# 65         self._insert_many(log_type.value + "_log", serialized)
# 66 
# 67     def close(self):
# 68         self.conn.close()
# 69 
# 70 
# 71 #  Example:
# 72 #  logger = LoggingProvider()
# 73 #  logger.write(LogType.STATE, StateLog(...))


# factories\prompt_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.prompt_generators import PROMPT_GENERATOR_REGISTRY
# 4 
# 5 
# 6 class PromptGeneratorFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             PROMPT_GENERATOR_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = PROMPT_GENERATOR_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"Prompt generator {name} not registered")
# 19         return cls(**kwargs)


# factories\scoring_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.scoring_models import SCORING_MODEL_REGISTRY
# 4 
# 5 
# 6 class ScoringProviderFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             SCORING_MODEL_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = SCORING_MODEL_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"Scoring provider {name} not registered")
# 19         return cls(**kwargs)


# factories\state_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.state_managers import STATE_MANAGER_REGISTRY
# 4 
# 5 
# 6 class StateManagerFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             STATE_MANAGER_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = STATE_MANAGER_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"State manager {name} not registered")
# 19         return cls(**kwargs)


# factories\system_config_provider.py

# 1 from __future__ import annotations
# 2 
# 3 
# 4 class SystemConfigProvider:
# 5     @staticmethod
# 6     def create(config: dict):
# 7         return config


# factories\system_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.system_managers import SYSTEM_MANAGER_REGISTRY
# 4 
# 5 
# 6 class SystemManagerFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             SYSTEM_MANAGER_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = SYSTEM_MANAGER_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"System manager {name} not registered")
# 19         return cls(**kwargs)


# factories\tool_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.tool_providers import TOOL_PROVIDER_REGISTRY
# 4 
# 5 
# 6 class ToolProviderFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             TOOL_PROVIDER_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = TOOL_PROVIDER_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"Tool provider {name} not registered")
# 19         return cls(**kwargs)


# registries\__init__.py

# 1 class Registry:
# 2     def __init__(self):
# 3         self._registry = {}
# 4 
# 5     def register(self, name: str, item):
# 6         existing = self._registry.get(name)
# 7         if existing is not None:
# 8             if existing is item:
# 9                 return
# 10             raise KeyError(f"{name} already registered")
# 11         self._registry[name] = item
# 12 
# 13     def get(self, name: str):
# 14         return self._registry.get(name)
# 15 
# 16     def all(self):
# 17         return dict(self._registry)


# registries\agent_engines\__init__.py

# 1 from .. import Registry
# 2 
# 3 AGENT_ENGINE_REGISTRY = Registry()


# registries\agent_prompts\__init__.py

# 1 from .. import Registry
# 2 
# 3 AGENT_PROMPT_REGISTRY = Registry()


# registries\agents\__init__.py

# 1 from .. import Registry
# 2 
# 3 AGENT_REGISTRY = Registry()


# registries\context_providers\__init__.py

# 1 from .. import Registry
# 2 
# 3 CONTEXT_PROVIDER_REGISTRY = Registry()


# registries\prompt_generators\__init__.py

# 1 from .. import Registry
# 2 
# 3 PROMPT_GENERATOR_REGISTRY = Registry()


# registries\scoring_models\__init__.py

# 1 from .. import Registry
# 2 
# 3 SCORING_MODEL_REGISTRY = Registry()


# registries\state_managers\__init__.py

# 1 from .. import Registry
# 2 
# 3 STATE_MANAGER_REGISTRY = Registry()


# registries\system_managers\__init__.py

# 1 from .. import Registry
# 2 
# 3 SYSTEM_MANAGER_REGISTRY = Registry()


# registries\system_prompts\__init__.py

# 1 from .. import Registry
# 2 
# 3 SYSTEM_PROMPT_REGISTRY = Registry()


# registries\tool_providers\__init__.py

# 1 from .. import Registry
# 2 
# 3 TOOL_PROVIDER_REGISTRY = Registry()


# registries\tools\__init__.py

# 1 from .. import Registry
# 2 
# 3 TOOL_REGISTRY = Registry()


# schemas\__init__.py

# 1 from .agent_engine_schema import AgentEngine
# 2 from .agent_prompt_schema import AgentPrompt
# 3 from .system_prompt_schema import SystemPrompt
# 4 from .context_provider_schema import ContextProvider
# 5 from .tooling_provider_schema import ToolingProvider
# 6 from .file_path_schema import FilePath
# 7 from .agent_config_schema import AgentConfig
# 8 from .prompt_generator_schema import PromptGenerator
# 9 from .scoring_provider_schema import ScoringProvider
# 10 from .state_manager_schema import StateManager
# 11 from .system_config_schema import SystemConfig
# 12 from .experiment_config_schema import ExperimentConfig
# 13 from .series_schema import Series
# 14 
# 15 __all__ = [
# 16     "AgentEngine",
# 17     "AgentPrompt",
# 18     "SystemPrompt",
# 19     "ContextProvider",
# 20     "ToolingProvider",
# 21     "FilePath",
# 22     "AgentConfig",
# 23     "PromptGenerator",
# 24     "ScoringProvider",
# 25     "StateManager",
# 26     "SystemConfig",
# 27     "ExperimentConfig",
# 28     "Series",
# 29 ]


# schemas\agent_config_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator, model_validator
# 8 
# 9 from .agent_engine_schema import AgentEngine
# 10 from .prompt_generator_schema import PromptGenerator
# 11 
# 12 
# 13 class AgentConfig(BaseModel):
# 14     """Schema for the agent_config table."""
# 15 
# 16     id: Optional[int] = None
# 17     name: str
# 18     description: Optional[str] = None
# 19     agent_role: Optional[str] = None
# 20     system_type: Optional[str] = None
# 21     agent_engine_id: Optional[int] = None
# 22     prompt_generator_id: Optional[int] = None
# 23     agent_engine: Optional[AgentEngine] = None
# 24     prompt_generator: Optional[PromptGenerator] = None
# 25     artifact_path: Optional[Path] = None
# 26 
# 27     table_name: str = "agent_config"
# 28 
# 29     @model_validator(mode="after")
# 30     def _sync_ids(self) -> "AgentConfig":
# 31         if self.agent_engine is not None and self.agent_engine_id is None:
# 32             self.agent_engine_id = getattr(self.agent_engine, "id", None)
# 33         if self.prompt_generator is not None and self.prompt_generator_id is None:
# 34             self.prompt_generator_id = getattr(self.prompt_generator, "id", None)
# 35         return self
# 36 
# 37     @field_validator("artifact_path")
# 38     @classmethod
# 39     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 40         if v is None:
# 41             return v
# 42         p = Path(v)
# 43         if not p.is_absolute() and ".." in p.parts:
# 44             raise ValueError("artifact_path must be absolute or project relative")
# 45         return p
# 46 
# 47     def model_dump(self, **kwargs) -> dict:
# 48         return BaseModel.dict(self, **kwargs)


# schemas\agent_engine_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class AgentEngine(BaseModel):
# 11     """Schema for the agent_engine table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     model: Optional[str] = None
# 17     engine_config: Optional[str] = None
# 18     artifact_path: Optional[Path] = None
# 19 
# 20     table_name: str = "agent_engine"
# 21 
# 22     @field_validator("artifact_path")
# 23     @classmethod
# 24     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 25         if v is None:
# 26             return v
# 27         p = Path(v)
# 28         if not p.is_absolute() and ".." in p.parts:
# 29             raise ValueError("artifact_path must be absolute or project relative")
# 30         return p
# 31 
# 32     def model_dump(self, **kwargs) -> dict:
# 33         return BaseModel.dict(self, **kwargs)


# schemas\agent_prompt_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class AgentPrompt(BaseModel):
# 11     """Schema for the agent_prompt table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     agent_role: Optional[str] = None
# 17     system_type: Optional[str] = None
# 18     artifact_path: Optional[Path] = None
# 19 
# 20     table_name: str = "agent_prompt"
# 21 
# 22     @field_validator("artifact_path")
# 23     @classmethod
# 24     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 25         if v is None:
# 26             return v
# 27         p = Path(v)
# 28         if not p.is_absolute() and ".." in p.parts:
# 29             raise ValueError("artifact_path must be absolute or project relative")
# 30         return p
# 31 
# 32     def model_dump(self, **kwargs) -> dict:
# 33         return BaseModel.dict(self, **kwargs)


# schemas\context_provider_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .tooling_provider_schema import ToolingProvider
# 9 
# 10 
# 11 class ContextProvider(BaseModel):
# 12     """Schema for the context_provider table."""
# 13 
# 14     id: Optional[int] = None
# 15     name: str
# 16     description: Optional[str] = None
# 17     system_type: Optional[str] = None
# 18     tooling_provider_id: Optional[int] = None
# 19     tooling_provider: Optional[ToolingProvider] = None
# 20 
# 21     table_name: str = "context_provider"
# 22 
# 23     @model_validator(mode="after")
# 24     def _sync_ids(self) -> "ContextProvider":
# 25         if self.tooling_provider is not None and self.tooling_provider_id is None:
# 26             self.tooling_provider_id = getattr(self.tooling_provider, "id", None)
# 27         return self
# 28 
# 29     def model_dump(self, **kwargs) -> dict:
# 30         return BaseModel.dict(self, **kwargs)


# schemas\experiment_config_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .system_config_schema import SystemConfig
# 9 from .scoring_provider_schema import ScoringProvider
# 10 
# 11 
# 12 class ExperimentConfig(BaseModel):
# 13     """Schema for the experiment_config table."""
# 14 
# 15     id: Optional[int] = None
# 16     name: str
# 17     description: Optional[str] = None
# 18     system_manager_id: Optional[int | str] = None
# 19     scoring_model_id: Optional[int | str] = None
# 20 
# 21     system_manager: Optional[SystemConfig] = None
# 22     scoring_model: Optional[ScoringProvider] = None
# 23 
# 24     table_name: str = "experiment_config"
# 25 
# 26     @model_validator(mode="after")
# 27     def _sync_ids(self) -> "ExperimentConfig":
# 28         if self.system_manager is not None and self.system_manager_id is None:
# 29             self.system_manager_id = getattr(self.system_manager, "id", None)
# 30         if self.scoring_model is not None and self.scoring_model_id is None:
# 31             self.scoring_model_id = getattr(self.scoring_model, "id", None)
# 32         return self
# 33 
# 34     def model_dump(self, **kwargs) -> dict:
# 35         return BaseModel.dict(self, **kwargs)


# schemas\file_path_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import field_validator
# 7 
# 8 
# 9 class FilePath(BaseModel):
# 10     """Schema for file paths tracked in the database."""
# 11 
# 12     artifact_path: Path
# 13 
# 14     table_name: str = "file_path"
# 15 
# 16     @field_validator("artifact_path")
# 17     @classmethod
# 18     def _check_path(cls, v: Path) -> Path:
# 19         p = Path(v)
# 20         if not p.is_absolute() and ".." in p.parts:
# 21             raise ValueError("artifact_path must be absolute or project relative")
# 22         return p
# 23 
# 24     def model_dump(self, **kwargs) -> dict:
# 25         return BaseModel.dict(self, **kwargs)


# schemas\prompt_generator_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator, model_validator
# 8 
# 9 from .agent_prompt_schema import AgentPrompt
# 10 from .system_prompt_schema import SystemPrompt
# 11 from .context_provider_schema import ContextProvider
# 12 
# 13 
# 14 class PromptGenerator(BaseModel):
# 15     """Schema for the prompt_generator table."""
# 16 
# 17     id: Optional[int] = None
# 18     name: str
# 19     description: Optional[str] = None
# 20     agent_prompt_id: Optional[int] = None
# 21     system_prompt_id: Optional[int] = None
# 22     content_provider_id: Optional[int] = None
# 23 
# 24     agent_prompt: Optional[AgentPrompt] = None
# 25     system_prompt: Optional[SystemPrompt] = None
# 26     content_provider: Optional[ContextProvider] = None
# 27     artifact_path: Optional[Path] = None
# 28 
# 29     table_name: str = "prompt_generator"
# 30 
# 31     @model_validator(mode="after")
# 32     def _sync_ids(self) -> "PromptGenerator":
# 33         if self.agent_prompt is not None and self.agent_prompt_id is None:
# 34             self.agent_prompt_id = getattr(self.agent_prompt, "id", None)
# 35         if self.system_prompt is not None and self.system_prompt_id is None:
# 36             self.system_prompt_id = getattr(self.system_prompt, "id", None)
# 37         if self.content_provider is not None and self.content_provider_id is None:
# 38             self.content_provider_id = getattr(self.content_provider, "id", None)
# 39         return self
# 40 
# 41     @field_validator("artifact_path")
# 42     @classmethod
# 43     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 44         if v is None:
# 45             return v
# 46         p = Path(v)
# 47         if not p.is_absolute() and ".." in p.parts:
# 48             raise ValueError("artifact_path must be absolute or project relative")
# 49         return p
# 50 
# 51     def model_dump(self, **kwargs) -> dict:
# 52         return BaseModel.dict(self, **kwargs)


# schemas\scoring_provider_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class ScoringProvider(BaseModel):
# 11     """Schema for the scoring_provider table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     artifact_path: Optional[Path] = None
# 17 
# 18     table_name: str = "scoring_provider"
# 19 
# 20     @field_validator("artifact_path")
# 21     @classmethod
# 22     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 23         if v is None:
# 24             return v
# 25         p = Path(v)
# 26         if not p.is_absolute() and ".." in p.parts:
# 27             raise ValueError("artifact_path must be absolute or project relative")
# 28         return p
# 29 
# 30     def model_dump(self, **kwargs) -> dict:
# 31         return BaseModel.dict(self, **kwargs)


# schemas\series_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .experiment_config_schema import ExperimentConfig
# 9 
# 10 
# 11 class Series(BaseModel):
# 12     """Schema for the series table."""
# 13 
# 14     id: Optional[int] = None
# 15     experiment_config_id: Optional[int] = None
# 16     experiment_config: Optional[ExperimentConfig] = None
# 17 
# 18     table_name: str = "series"
# 19 
# 20     @model_validator(mode="after")
# 21     def _sync_ids(self) -> "Series":
# 22         if self.experiment_config is not None and self.experiment_config_id is None:
# 23             self.experiment_config_id = getattr(self.experiment_config, "id", None)
# 24         return self
# 25 
# 26     def model_dump(self, **kwargs) -> dict:
# 27         return BaseModel.dict(self, **kwargs)


# schemas\state_manager_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class StateManager(BaseModel):
# 11     """Schema for the state_manager table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     system_state: Optional[str] = None
# 17     system_type: Optional[str] = None
# 18     agent_id: Optional[int] = None
# 19     artifact_path: Optional[Path] = None
# 20 
# 21     table_name: str = "state_manager"
# 22 
# 23     @field_validator("artifact_path")
# 24     @classmethod
# 25     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 26         if v is None:
# 27             return v
# 28         p = Path(v)
# 29         if not p.is_absolute() and ".." in p.parts:
# 30             raise ValueError("artifact_path must be absolute or project relative")
# 31         return p
# 32 
# 33     def model_dump(self, **kwargs) -> dict:
# 34         return BaseModel.dict(self, **kwargs)


# schemas\system_config_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .state_manager_schema import StateManager
# 9 from .scoring_provider_schema import ScoringProvider
# 10 
# 11 
# 12 class SystemConfig(BaseModel):
# 13     """Schema for the system_config table."""
# 14 
# 15     id: Optional[int] = None
# 16     name: str
# 17     description: Optional[str] = None
# 18     system_type: Optional[str] = None
# 19     state_manager_id: Optional[int] = None
# 20     scoring_model_id: Optional[int] = None
# 21 
# 22     state_manager: Optional[StateManager] = None
# 23     scoring_model: Optional[ScoringProvider] = None
# 24 
# 25     table_name: str = "system_config"
# 26 
# 27     @model_validator(mode="after")
# 28     def _sync_ids(self) -> "SystemConfig":
# 29         if self.state_manager is not None and self.state_manager_id is None:
# 30             self.state_manager_id = getattr(self.state_manager, "id", None)
# 31         if self.scoring_model is not None and self.scoring_model_id is None:
# 32             self.scoring_model_id = getattr(self.scoring_model, "id", None)
# 33         return self
# 34 
# 35     def model_dump(self, **kwargs) -> dict:
# 36         return BaseModel.dict(self, **kwargs)


# schemas\system_prompt_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class SystemPrompt(BaseModel):
# 11     """Schema for the system_prompt table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     system_type: Optional[str] = None
# 17     artifact_path: Optional[Path] = None
# 18 
# 19     table_name: str = "system_prompt"
# 20 
# 21     @field_validator("artifact_path")
# 22     @classmethod
# 23     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 24         if v is None:
# 25             return v
# 26         p = Path(v)
# 27         if not p.is_absolute() and ".." in p.parts:
# 28             raise ValueError("artifact_path must be absolute or project relative")
# 29         return p
# 30 
# 31     def model_dump(self, **kwargs) -> dict:
# 32         return BaseModel.dict(self, **kwargs)


# schemas\tooling_provider_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class ToolingProvider(BaseModel):
# 11     """Schema for the tooling_provider table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     artifact_path: Optional[Path] = None
# 17 
# 18     table_name: str = "tooling_provider"
# 19 
# 20     @field_validator("artifact_path")
# 21     @classmethod
# 22     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 23         if v is None:
# 24             return v
# 25         p = Path(v)
# 26         if not p.is_absolute() and ".." in p.parts:
# 27             raise ValueError("artifact_path must be absolute or project relative")
# 28         return p
# 29 
# 30     def model_dump(self, **kwargs) -> dict:
# 31         return BaseModel.dict(self, **kwargs)


# utilities\__init__.py



# utilities\db.py

# 1 import sqlite3
# 2 from pathlib import Path
# 3 from typing import Iterable, Any
# 4 
# 5 DB_PATH = Path("experiments") / "codecritic.sqlite3"
# 6 
# 7 
# 8 def _serialize(obj: Any) -> dict:
# 9     from dataclasses import asdict, is_dataclass
# 10     from enum import Enum
# 11     from pathlib import Path
# 12 
# 13     if not is_dataclass(obj) or isinstance(obj, type):
# 14         raise TypeError(f"Expected dataclass instance, got {type(obj)}")
# 15 
# 16     result = asdict(obj)
# 17     return {
# 18         k: (str(v) if isinstance(v, (Path, Enum)) else v) for k, v in result.items()
# 19     }
# 20 
# 21 
# 22 def get_connection() -> sqlite3.Connection:
# 23     DB_PATH.parent.mkdir(exist_ok=True)
# 24     conn = sqlite3.connect(DB_PATH)
# 25     return conn
# 26 
# 27 
# 28 def init_db() -> sqlite3.Connection:
# 29     conn = get_connection()
# 30     cur = conn.cursor()
# 31     cur.execute(
# 32         """CREATE TABLE IF NOT EXISTS state_log (
# 33             experiment_id TEXT,
# 34             system TEXT,
# 35             round INTEGER,
# 36             state TEXT,
# 37             action TEXT,
# 38             score REAL,
# 39             details TEXT,
# 40             timestamp TEXT
# 41         )"""
# 42     )
# 43     cur.execute(
# 44         """CREATE TABLE IF NOT EXISTS state_transition_log (
# 45             experiment_id TEXT,
# 46             round INTEGER,
# 47             from_state TEXT,
# 48             to_state TEXT,
# 49             reason TEXT,
# 50             timestamp TEXT
# 51         )"""
# 52     )
# 53     cur.execute(
# 54         """CREATE TABLE IF NOT EXISTS prompt_log (
# 55             experiment_id TEXT,
# 56             round INTEGER,
# 57             system TEXT,
# 58             agent_id TEXT,
# 59             agent_role TEXT,
# 60             agent_engine TEXT,
# 61             symbol TEXT,
# 62             prompt TEXT,
# 63             response TEXT,
# 64             attempt_number INTEGER,
# 65             agent_action_outcome TEXT,
# 66             start TEXT,
# 67             stop TEXT
# 68         )"""
# 69     )
# 70     cur.execute(
# 71         """CREATE TABLE IF NOT EXISTS code_quality_log (
# 72             experiment_id TEXT,
# 73             round INTEGER,
# 74             symbol TEXT,
# 75             lines_of_code INTEGER,
# 76             cyclomatic_complexity REAL,
# 77             maintainability_index REAL,
# 78             lint_errors INTEGER,
# 79             timestamp TEXT
# 80         )"""
# 81     )
# 82     cur.execute(
# 83         """CREATE TABLE IF NOT EXISTS scoring_log (
# 84             experiment_id TEXT,
# 85             round INTEGER,
# 86             metric TEXT,
# 87             value REAL,
# 88             timestamp TEXT
# 89         )"""
# 90     )
# 91     conn.commit()
# 92     return conn
# 93 
# 94 
# 95 def insert_logs(conn: sqlite3.Connection, table: str, logs: Iterable[Any]) -> None:
# 96     logs = list(logs)
# 97     if not logs:
# 98         return
# 99     row = _serialize(logs[0])
# 100     cols = ",".join(row.keys())
# 101     placeholders = ",".join(["?"] * len(row))
# 102     values = [tuple(_serialize(log).values()) for log in logs]
# 103     conn.executemany(
# 104         f"INSERT INTO {table} ({cols}) VALUES ({placeholders})",
# 105         values,
# 106     )
# 107     conn.commit()


# utilities\file_management\__init__.py



# utilities\metadata\__init__.py



# utilities\metadata\footer\__init__.py



# utilities\metadata\logging\__init__.py

# 1 """Logging schemas for structured experiment data."""
# 2 
# 3 from .log_schemas import (
# 4     StateLog,
# 5     StateTransitionLog,
# 6     PromptLog,
# 7     CodeQualityLog,
# 8     ErrorLog,
# 9     ScoringLog,
# 10 )
# 11 
# 12 __all__ = [
# 13     "StateLog",
# 14     "StateTransitionLog",
# 15     "PromptLog",
# 16     "CodeQualityLog",
# 17     "ErrorLog",
# 18     "ScoringLog",
# 19 ]


# utilities\metadata\logging\log_schemas.py

# 1 from __future__ import annotations
# 2 
# 3 from dataclasses import dataclass, field
# 4 from datetime import datetime, timezone
# 5 from app.enums.agent_enums import AgentRole
# 6 from app.enums.scoring_enums import ScoringMetric
# 7 from app.enums.system_enums import SystemState, StateTransitionReason
# 8 
# 9 
# 10 @dataclass
# 11 class StateLog:
# 12     experiment_id: str
# 13     system: str
# 14     round: int
# 15     state: SystemState
# 16     action: str
# 17     score: float | None = None
# 18     details: str | None = None
# 19     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 20 
# 21 
# 22 @dataclass
# 23 class StateTransitionLog:
# 24     experiment_id: str
# 25     round: int
# 26     from_state: SystemState
# 27     to_state: SystemState
# 28     reason: StateTransitionReason
# 29     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 30 
# 31 
# 32 @dataclass
# 33 class PromptLog:
# 34     experiment_id: str
# 35     round: int
# 36     system: str
# 37     agent_id: str
# 38     agent_role: AgentRole
# 39     agent_engine: str | None = None
# 40     symbol: str | None = None
# 41     prompt: str | None = None
# 42     response: str | None = None
# 43     attempt_number: int = 0
# 44     agent_action_outcome: str | None = None
# 45     start: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 46     stop: datetime | None = None
# 47 
# 48 
# 49 @dataclass
# 50 class CodeQualityLog:
# 51     experiment_id: str
# 52     round: int
# 53     symbol: str
# 54     lines_of_code: int
# 55     cyclomatic_complexity: float
# 56     maintainability_index: float
# 57     lint_errors: int
# 58     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 59 
# 60 
# 61 @dataclass
# 62 class ErrorLog:
# 63     experiment_id: str
# 64     round: int
# 65     error_type: str
# 66     message: str
# 67     file_path: str | None = None
# 68     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 69 
# 70 
# 71 @dataclass
# 72 class ScoringLog:
# 73     experiment_id: str
# 74     round: int
# 75     metric: ScoringMetric
# 76     value: float
# 77     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 78 
# 79 
# 80 @dataclass
# 81 class ConversationLog:
# 82     experiment_id: str
# 83     round: int
# 84     agent_role: AgentRole
# 85     target: str
# 86     content: str
# 87     originating_agent: str
# 88     intervention: bool
# 89     intervention_type: str | None = None
# 90     intervention_reason: str | None = None
# 91     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 92 
# 93 
# 94 @dataclass
# 95 class ExperimentLog:
# 96     experiment_id: str
# 97     description: str
# 98     mode: str
# 99     variant: str
# 100     max_iterations: int
# 101     stop_threshold: float
# 102     model_engine: str
# 103     evaluator_name: str
# 104     evaluator_version: str
# 105     final_score: float
# 106     passed: bool
# 107     reason_for_stop: str
# 108     start: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 109     stop: datetime | None = None


# utilities\metadata\snapshots\__init__.py



# utilities\metrics.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from typing import Any, Dict, List
# 5 
# 6 logger = logging.getLogger(__name__)
# 7 
# 8 #  Unified list of metric names expected in experiment results
# 9 EVALUATION_METRICS = [
# 10     "bug_fix_success_rate",
# 11     "functional_correctness",
# 12     "avg_test_pass_rate",
# 13     "maintainability_index",
# 14     "cyclomatic_complexity",
# 15     "linting_compliance_rate",
# 16     "iterations_to_convergence",
# 17     "intervention_frequency",
# 18     "agent_role_success_rate",
# 19     "retry_success_rate",
# 20     "mediation_success_rate",
# 21 ]
# 22 
# 23 
# 24 def _get(obj: Any, key: str, default: Any = None) -> Any:
# 25     if isinstance(obj, dict):
# 26         return obj.get(key, default)
# 27     return getattr(obj, key, default)
# 28 
# 29 
# 30 def _avg(values: List[float]) -> float:
# 31     return sum(values) / len(values) if values else 0.0
# 32 
# 33 
# 34 def compute_metrics(
# 35     evaluation_logs: List[Any],
# 36     code_quality_logs: List[Any],
# 37     conversation_logs: List[Any],
# 38     prompt_logs: List[Any],
# 39     state_logs: List[Any],
# 40 ) -> Dict[str, float]:
# 41     """Compute experiment metrics from structured logs."""
# 42 
# 43     logger.debug(
# 44         "Computing metrics: evaluations=%d quality=%d conversation=%d prompts=%d states=%d",
# 45         len(evaluation_logs),
# 46         len(code_quality_logs),
# 47         len(conversation_logs),
# 48         len(prompt_logs),
# 49         len(state_logs),
# 50     )
# 51 
# 52     metrics: Dict[str, float] = {}
# 53 
# 54     bug_fixes = [_get(log, "bug_fixed", False) for log in evaluation_logs]
# 55     metrics["bug_fix_success_rate"] = _avg([1.0 for bug in bug_fixes if bug])
# 56 
# 57     func_correct = [_get(log, "all_tests_passed", False) for log in evaluation_logs]
# 58     metrics["functional_correctness"] = _avg([1.0 for passes in func_correct if passes])
# 59 
# 60     pass_rates = []
# 61     for log in evaluation_logs:
# 62         total = _get(log, "tests_total", 0)
# 63         passed = _get(log, "tests_passed", 0)
# 64         if total:
# 65             pass_rates.append(passed / total)
# 66     metrics["avg_test_pass_rate"] = _avg(pass_rates)
# 67 
# 68     mi_values = [_get(log, "maintainability_index", 0.0) for log in code_quality_logs]
# 69     metrics["maintainability_index"] = _avg(mi_values)
# 70 
# 71     cc_values = [_get(log, "cyclomatic_complexity", 0.0) for log in code_quality_logs]
# 72     metrics["cyclomatic_complexity"] = _avg(cc_values)
# 73 
# 74     lint_compliance = [
# 75         1.0 if _get(log, "lint_errors", 1) == 0 else 0.0 for log in code_quality_logs
# 76     ]
# 77     metrics["linting_compliance_rate"] = _avg(lint_compliance)
# 78 
# 79     rounds = [_get(log, "round", 0) for log in state_logs]
# 80     metrics["iterations_to_convergence"] = max(rounds) + 1 if rounds else 0.0
# 81 
# 82     interventions = [
# 83         1.0 for log in conversation_logs if _get(log, "intervention", False)
# 84     ]
# 85     metrics["intervention_frequency"] = _avg(interventions)
# 86 
# 87     outcomes = [_get(log, "agent_action_outcome", None) for log in prompt_logs]
# 88     successes = [1.0 for o in outcomes if o == "success"]
# 89     metrics["agent_role_success_rate"] = _avg(successes)
# 90 
# 91     retry_logs = [log for log in prompt_logs if _get(log, "attempt_number", 0) > 0]
# 92     retry_successes = [
# 93         1.0
# 94         for log in retry_logs
# 95         if _get(log, "agent_action_outcome", None) == "success"
# 96     ]
# 97     metrics["retry_success_rate"] = _avg(retry_successes)
# 98 
# 99     mediation_logs = [
# 100         log
# 101         for log in conversation_logs
# 102         if _get(log, "intervention_type", None) == "mediation"
# 103     ]
# 104     mediation_success = [
# 105         1.0 for log in mediation_logs if _get(log, "intervention", False)
# 106     ]
# 107     metrics["mediation_success_rate"] = _avg(mediation_success)
# 108 
# 109     logger.info("Metrics computed: %s", metrics)
# 110     return metrics


# utilities\pydantic_compat.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import TYPE_CHECKING, Callable
# 4 
# 5 try:  #  pragma: no cover - pydantic v1 compatibility
# 6     from pydantic import BaseModel
# 7 
# 8     if not hasattr(BaseModel, "model_dump"):
# 9         from typing import Callable
# 10 
# 11         def _model_dump(self: BaseModel, **kwargs) -> dict:
# 12             return BaseModel.dict(self, **kwargs)
# 13 
# 14         setattr(BaseModel, "model_dump", _model_dump)  #  type: ignore[attr-defined]
# 15 except Exception:  #  pragma: no cover - ignore if pydantic v2
# 16     pass
# 17 
# 18 if TYPE_CHECKING:
# 19     from pydantic import field_validator, model_validator  #  type: ignore
# 20 else:  #  pragma: no cover - runtime fallback for pydantic v1
# 21     try:
# 22         from pydantic import field_validator, model_validator  #  type: ignore
# 23     except ImportError:  #  pragma: no cover - pydantic v1
# 24         from pydantic import validator, root_validator
# 25 
# 26         def field_validator(*fields: str, **kwargs) -> Callable:
# 27             kwargs.pop("mode", None)
# 28             return validator(*fields, **kwargs)
# 29 
# 30         def model_validator(*fields, **kwargs) -> Callable:
# 31             mode = kwargs.pop("mode", "after")
# 32 
# 33             def decorator(func: Callable) -> Callable:
# 34                 if mode == "after":
# 35 
# 36                     def wrapper(cls, values):
# 37                         obj = cls.construct(**values)
# 38                         func(obj)
# 39                         return obj.dict()
# 40 
# 41                     return root_validator(pre=False, allow_reuse=True)(wrapper)
# 42 
# 43                 def wrapper(cls, values):
# 44                     return func(values)
# 45 
# 46                 return root_validator(pre=True, allow_reuse=True)(wrapper)
# 47 
# 48             return decorator


# utilities\schema\__init__.py

# 1 from .create_schema import initialize_database, create_tables, load_seed_data
# 2 
# 3 __all__ = ["initialize_database", "create_tables", "load_seed_data"]


# utilities\schema\create_schema.py

# 1 from __future__ import annotations
# 2 
# 3 import json
# 4 import sqlite3
# 5 from pathlib import Path
# 6 from typing import Type, Any, Union, get_origin, get_args
# 7 from enum import Enum
# 8 
# 9 from app.schemas import (
# 10     AgentEngine,
# 11     AgentPrompt,
# 12     SystemPrompt,
# 13     ContextProvider,
# 14     ToolingProvider,
# 15     FilePath,
# 16     AgentConfig,
# 17     PromptGenerator,
# 18     ScoringProvider,
# 19     StateManager,
# 20     SystemConfig,
# 21     ExperimentConfig,
# 22     Series,
# 23 )
# 24 from app.utilities import db
# 25 
# 26 SCHEMAS = {
# 27     "agent_engine": AgentEngine,
# 28     "agent_prompt": AgentPrompt,
# 29     "system_prompt": SystemPrompt,
# 30     "context_provider": ContextProvider,
# 31     "tooling_provider": ToolingProvider,
# 32     "file_path": FilePath,
# 33     "agent_config": AgentConfig,
# 34     "prompt_generator": PromptGenerator,
# 35     "scoring_provider": ScoringProvider,
# 36     "state_manager": StateManager,
# 37     "system_config": SystemConfig,
# 38     "experiment_config": ExperimentConfig,
# 39     "series": Series,
# 40 }
# 41 
# 42 _TYPE_MAP = {
# 43     int: "INTEGER",
# 44     str: "TEXT",
# 45     float: "REAL",
# 46     Path: "TEXT",
# 47 }
# 48 
# 49 
# 50 def _sqlite_type(py_type: Type) -> str:
# 51     origin = get_origin(py_type)
# 52     if origin is Union:
# 53         py_type = get_args(py_type)[0]
# 54     return _TYPE_MAP.get(py_type, "TEXT")
# 55 
# 56 
# 57 def _is_optional(annotation: Any) -> bool:
# 58     return get_origin(annotation) is Union and type(None) in get_args(annotation)
# 59 
# 60 
# 61 def create_tables(conn: sqlite3.Connection) -> None:
# 62     cur = conn.cursor()
# 63     for table_name, model_cls in SCHEMAS.items():
# 64         fields = model_cls.__annotations__
# 65         columns = []
# 66         for name, annotation in fields.items():
# 67             col_type = _sqlite_type(annotation)
# 68             if name == "id" and _is_optional(annotation):
# 69                 columns.append(f"{name} INTEGER PRIMARY KEY")
# 70             else:
# 71                 columns.append(f"{name} {col_type}")
# 72         col_sql = ", ".join(columns)
# 73         cur.execute(f"CREATE TABLE IF NOT EXISTS {table_name} ({col_sql})")
# 74     conn.commit()
# 75 
# 76 
# 77 def load_seed_data(
# 78     conn: sqlite3.Connection, seed_dir: Path | str = "experiments/config/seed"
# 79 ) -> None:
# 80     seed_path = Path(seed_dir)
# 81     if not seed_path.exists():
# 82         return
# 83     cur = conn.cursor()
# 84     for file in seed_path.glob("*.json"):
# 85         table_name = file.stem
# 86         model = SCHEMAS.get(table_name)
# 87         if model is None:
# 88             continue
# 89         entries = json.loads(file.read_text())
# 90         if isinstance(entries, dict):
# 91             entries = [entries]
# 92         for entry in entries:
# 93             obj = model(**entry)
# 94             data = obj.model_dump()
# 95             #  Convert enums and paths to strings
# 96             data = {
# 97                 k: str(v) if isinstance(v, (Path, Enum)) else v for k, v in data.items()
# 98             }
# 99             cols = ",".join(data.keys())
# 100             placeholders = ",".join(["?"] * len(data))
# 101             cur.execute(
# 102                 f"INSERT INTO {table_name} ({cols}) VALUES ({placeholders})",
# 103                 list(data.values()),
# 104             )
# 105     conn.commit()
# 106 
# 107 
# 108 def initialize_database(reset: bool = False) -> sqlite3.Connection:
# 109     if reset:
# 110         db_path = Path("experiments/codecritic.sqlite3")
# 111         if db_path.exists():
# 112             db_path.unlink()
# 113     conn = db.get_connection()
# 114     create_tables(conn)
# 115     load_seed_data(conn)
# 116     return conn
# 117 
# 118 
# 119 if __name__ == "__main__":
# 120     initialize_database()


# utilities\tools\__init__.py



# utilities\tools\black_runner.py

# 1 #  Placeholder for black runner tool


# utilities\tools\doc_formatter.py

# 1 #  Placeholder for documentation formatter


# utilities\tools\mypy_runner.py

# 1 #  Placeholder for mypy runner


# utilities\tools\radon_runner.py

# 1 import sys
# 2 import json
# 3 from radon.complexity import cc_visit
# 4 from pathlib import Path
# 5 
# 6 
# 7 def analyze_file(filepath):
# 8     with open(filepath, "r", encoding="utf-8") as file:
# 9         code = file.read()
# 10     complexity = cc_visit(code)
# 11     result = [
# 12         {"name": c.name, "complexity": c.complexity, "lineno": c.lineno}
# 13         for c in complexity
# 14     ]
# 15     return result
# 16 
# 17 
# 18 if __name__ == "__main__":
# 19     try:
# 20         if len(sys.argv) != 2:
# 21             raise ValueError("Provide exactly one file path.")
# 22         file_path = Path(sys.argv[1])
# 23         if not file_path.exists():
# 24             raise FileNotFoundError("File does not exist.")
# 25         analysis_result = analyze_file(str(file_path))
# 26         print(json.dumps({"result": analysis_result}, ensure_ascii=False))
# 27     except Exception as e:
# 28         print(json.dumps({"error": str(e)}, ensure_ascii=False))
# 29         sys.exit(1)


# utilities\tools\ruff_runner.py

# 1 #  Placeholder for ruff runner


# utilities\tools\sonarcloud_runner.py

# 1 #  Placeholder for sonarcloud runner



# === NON-PYTHON FILES ===

