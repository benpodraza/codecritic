
# === PYTHON FILES ===

# __init__.py



# abstract_classes\__init__.py



# abstract_classes\agent_base.py

# 1 from __future__ import annotations
# 2 
# 3 from abc import ABC, abstractmethod
# 4 
# 5 from ..factories.logging_provider import LoggingMixin, LoggingProvider
# 6 
# 7 
# 8 class AgentBase(LoggingMixin, ABC):
# 9     """Base class for executing agent logic."""
# 10 
# 11     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 12         super().__init__(logger)
# 13 
# 14     def run(self, *args, **kwargs) -> None:
# 15         self._log.debug("Agent run start")
# 16         self._run_agent_logic(*args, **kwargs)
# 17         self._log.debug("Agent run end")
# 18 
# 19     @abstractmethod
# 20     def _run_agent_logic(self, *args, **kwargs) -> None:
# 21         """Execute agent specific logic."""
# 22         raise NotImplementedError


# abstract_classes\context_provider_base.py

# 1 from __future__ import annotations
# 2 
# 3 from abc import ABC, abstractmethod
# 4 
# 5 from ..factories.logging_provider import LoggingMixin, LoggingProvider
# 6 
# 7 
# 8 class ContextProviderBase(LoggingMixin, ABC):
# 9     """Base class for providing context from symbol graphs."""
# 10 
# 11     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 12         super().__init__(logger)
# 13 
# 14     def get_context(self, *args, **kwargs):
# 15         self._log.debug("Context retrieval start")
# 16         context = self._get_context(*args, **kwargs)
# 17         self._log.debug("Context retrieval end")
# 18         return context
# 19 
# 20     @abstractmethod
# 21     def _get_context(self, *args, **kwargs):
# 22         """Return context information."""
# 23         raise NotImplementedError


# abstract_classes\experiment.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Any, Dict, List
# 4 
# 5 from ..factories.scoring_provider import ScoringProviderFactory
# 6 from ..factories.system_manager import SystemManagerFactory
# 7 from ..factories.experiment_config_provider import ExperimentConfigProvider
# 8 from ..factories.logging_provider import LoggingMixin, LoggingProvider, ScoringLog
# 9 from ..utilities.metrics import EVALUATION_METRICS
# 10 
# 11 
# 12 class Experiment(LoggingMixin):
# 13     """Base experiment handling execution and scoring."""
# 14 
# 15     def __init__(self, config_id: int, logger: LoggingProvider | None = None) -> None:
# 16         super().__init__(logger)
# 17         self.config_id = config_id
# 18         self.config: Dict[str, Any] | None = None
# 19         self.scoring_model_id = "dummy"
# 20 
# 21         self.evaluation_logs: List[Any] = []
# 22         self.code_quality_logs: List[Any] = []
# 23         self.conversation_logs: List[Any] = []
# 24         self.prompt_logs: List[Any] = []
# 25         self.state_logs: List[Any] = []
# 26         self.metrics: Dict[str, float] | None = None
# 27 
# 28     def run(self, *args, **kwargs) -> Dict[str, float]:
# 29         self._log.debug("Experiment run start")
# 30 
# 31         #   load configuration
# 32         self.config = ExperimentConfigProvider.load(self.config_id)
# 33         self.scoring_model_id = self.config.get("scoring_model_id", "dummy")
# 34         system_manager_id = self.config.get("system_manager_id", "dummy")
# 35 
# 36         #   run system manager *with* our injected logger
# 37         manager = SystemManagerFactory.create(
# 38             system_manager_id,
# 39             logger=self.logger,
# 40         )
# 41         manager.run()
# 42 
# 43         #   collect logs from manager
# 44         self.evaluation_logs = getattr(manager, "evaluation_logs", [])
# 45         self.code_quality_logs = getattr(manager, "code_quality_logs", [])
# 46         self.conversation_logs = getattr(manager, "conversation_logs", [])
# 47         self.prompt_logs = getattr(manager, "prompt_logs", [])
# 48         self.state_logs = getattr(manager, "state_logs", [])
# 49 
# 50         logs = {
# 51             "evaluation": self.evaluation_logs,
# 52             "code_quality": self.code_quality_logs,
# 53             "conversation": self.conversation_logs,
# 54             "prompt": self.prompt_logs,
# 55             "state": self.state_logs,
# 56         }
# 57 
# 58         scoring_provider = ScoringProviderFactory.create(
# 59             self.scoring_model_id,
# 60             logger=self.logger,
# 61         )
# 62         metrics = scoring_provider.score(logs)
# 63         for key in EVALUATION_METRICS:
# 64             metrics.setdefault(key, 0.0)
# 65         self._log.info("Experiment metrics: %s", metrics)
# 66 
# 67         for k, v in metrics.items():
# 68             self.log_scoring(
# 69                 ScoringLog(
# 70                     experiment_id=str(self.config_id),
# 71                     round=0,
# 72                     metric=k,
# 73                     value=v,
# 74                 )
# 75             )
# 76 
# 77         self.metrics = metrics
# 78         self._log.debug("Experiment run end")
# 79         return metrics
# 80 
# 81     def _run_experiment_logic(self, *args, **kwargs) -> None:
# 82         """Override to implement experiment steps."""
# 83         raise NotImplementedError


# abstract_classes\prompt_generator_base.py

# 1 from __future__ import annotations
# 2 
# 3 from abc import ABC, abstractmethod
# 4 
# 5 from ..factories.logging_provider import LoggingMixin, LoggingProvider
# 6 
# 7 
# 8 class PromptGeneratorBase(LoggingMixin, ABC):
# 9     """Base class for generating prompts."""
# 10 
# 11     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 12         super().__init__(logger)
# 13 
# 14     def generate_prompt(self, *args, **kwargs) -> str:
# 15         self._log.debug("Prompt generation start")
# 16         prompt = self._generate_prompt(*args, **kwargs)
# 17         self._log.debug("Prompt generation end")
# 18         return prompt
# 19 
# 20     @abstractmethod
# 21     def _generate_prompt(self, *args, **kwargs) -> str:
# 22         """Return a generated prompt."""
# 23         raise NotImplementedError


# abstract_classes\scoring_provider_base.py

# 1 from __future__ import annotations
# 2 
# 3 from abc import ABC, abstractmethod
# 4 
# 5 from ..factories.logging_provider import LoggingMixin, LoggingProvider
# 6 
# 7 
# 8 class ScoringProviderBase(LoggingMixin, ABC):
# 9     """Base class for computing evaluation metrics."""
# 10 
# 11     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 12         super().__init__(logger)
# 13 
# 14     def score(self, *args, **kwargs):
# 15         self._log.debug("Scoring start")
# 16         result = self._score(*args, **kwargs)
# 17         self._log.debug("Scoring end")
# 18         return result
# 19 
# 20     @abstractmethod
# 21     def _score(self, *args, **kwargs):
# 22         """Compute evaluation metrics."""
# 23         raise NotImplementedError


# abstract_classes\state_manager_base.py

# 1 from __future__ import annotations
# 2 
# 3 from abc import ABC, abstractmethod
# 4 
# 5 from ..factories.logging_provider import LoggingMixin, LoggingProvider
# 6 
# 7 
# 8 class StateManagerBase(LoggingMixin, ABC):
# 9     """Base class for managing state-level execution."""
# 10 
# 11     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 12         super().__init__(logger)
# 13 
# 14     def run(self, *args, **kwargs) -> None:
# 15         self._log.debug("StateManager run start")
# 16         self._run_state_logic(*args, **kwargs)
# 17         self._log.debug("StateManager run end")
# 18 
# 19     @abstractmethod
# 20     def _run_state_logic(self, *args, **kwargs) -> None:
# 21         """Execute state specific logic."""
# 22         raise NotImplementedError


# abstract_classes\system_manager_base.py

# 1 from __future__ import annotations
# 2 
# 3 from abc import ABC, abstractmethod
# 4 
# 5 from app.factories.logging_provider import LoggingMixin, LoggingProvider
# 6 
# 7 
# 8 class SystemManagerBase(LoggingMixin, ABC):
# 9     """Base class for coordinating high level system logic."""
# 10 
# 11     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 12         super().__init__(logger)
# 13 
# 14     def run(self, *args, **kwargs) -> None:
# 15         self._log.debug("SystemManager run start")
# 16         self._run_system_logic(*args, **kwargs)
# 17         self._log.debug("SystemManager run end")
# 18 
# 19     @abstractmethod
# 20     def _run_system_logic(self, *args, **kwargs) -> None:
# 21         """Execute system-specific logic and state transitions."""
# 22         raise NotImplementedError


# abstract_classes\tool_provider_base.py

# 1 from __future__ import annotations
# 2 
# 3 from abc import ABC, abstractmethod
# 4 
# 5 from ..factories.logging_provider import LoggingMixin, LoggingProvider
# 6 
# 7 
# 8 class ToolProviderBase(LoggingMixin, ABC):
# 9     """Base class for running external tools."""
# 10 
# 11     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 12         super().__init__(logger)
# 13 
# 14     def run(self, *args, **kwargs):
# 15         self._log.debug("Tool run start")
# 16         result = self._run(*args, **kwargs)
# 17         self._log.debug("Tool run end")
# 18         return result
# 19 
# 20     @abstractmethod
# 21     def _run(self, *args, **kwargs):
# 22         """Run tool-specific logic."""
# 23         raise NotImplementedError


# bootstrap.py

# 1 from __future__ import annotations
# 2 
# 3 from importlib import import_module
# 4 from pathlib import Path
# 5 from typing import Type
# 6 
# 7 from .factories.agent import AgentFactory
# 8 from .factories.prompt_manager import PromptGeneratorFactory
# 9 from .registries.context_providers import CONTEXT_PROVIDER_REGISTRY
# 10 from .factories.tool_provider import ToolProviderFactory
# 11 from .factories.scoring_provider import ScoringProviderFactory
# 12 from .factories.state_manager import StateManagerFactory
# 13 from .factories.experiment_config_provider import ExperimentConfigProvider
# 14 from .abstract_classes.agent_base import AgentBase
# 15 from .abstract_classes.prompt_generator_base import PromptGeneratorBase
# 16 from .abstract_classes.tool_provider_base import ToolProviderBase
# 17 from .abstract_classes.scoring_provider_base import ScoringProviderBase
# 18 from .abstract_classes.state_manager_base import StateManagerBase
# 19 
# 20 from .utilities.schema import initialize_database
# 21 
# 22 
# 23 def _import_class(path: str | Path, base_cls: Type) -> Type:
# 24     mod_path = str(path).replace("\\", "/")
# 25     if mod_path.endswith(".py"):
# 26         mod_path = mod_path[:-3]
# 27     mod_path = mod_path.replace("/", ".")
# 28     module = import_module(f"app.extensions.{mod_path}")
# 29     for attr in module.__dict__.values():
# 30         if (
# 31             isinstance(attr, type)
# 32             and issubclass(attr, base_cls)
# 33             and attr is not base_cls
# 34         ):
# 35             return attr
# 36     raise ImportError(f"No subclass of {base_cls.__name__} found in {module.__name__}")
# 37 
# 38 
# 39 def seed_registries(reset_db: bool = False) -> None:
# 40     conn = initialize_database(reset=reset_db)
# 41     cur = conn.cursor()
# 42 
# 43     #  Tool providers
# 44     for row in cur.execute("SELECT name, artifact_path FROM tooling_provider"):
# 45         name, artifact = row
# 46         path = Path(artifact)
# 47         cls = _import_class(path.as_posix(), ToolProviderBase)
# 48         try:
# 49             ToolProviderFactory.register(name, cls)
# 50         except KeyError:
# 51             pass
# 52 
# 53     #  Context providers
# 54     for row in cur.execute("SELECT name FROM context_provider"):
# 55         (name,) = row
# 56         #  Assume providers are already registered via extensions
# 57         if CONTEXT_PROVIDER_REGISTRY.get(name) is not None:
# 58             continue
# 59 
# 60     #  Prompt generators
# 61     for row in cur.execute("SELECT name, artifact_path FROM prompt_generator"):
# 62         name, artifact = row
# 63         path = Path(artifact)
# 64         cls = _import_class(path.as_posix(), PromptGeneratorBase)
# 65         try:
# 66             PromptGeneratorFactory.register(name, cls)
# 67         except KeyError:
# 68             pass
# 69 
# 70     #  Agents
# 71     for row in cur.execute("SELECT name, artifact_path FROM agent_config"):
# 72         name, artifact = row
# 73         path = Path(artifact)
# 74         cls = _import_class(path.as_posix(), AgentBase)
# 75         try:
# 76             AgentFactory.register(name, cls)
# 77         except KeyError:
# 78             pass
# 79 
# 80     #  Scoring providers
# 81     for row in cur.execute("SELECT name, artifact_path FROM scoring_provider"):
# 82         name, artifact = row
# 83         path = Path(artifact)
# 84         cls = _import_class(path.as_posix(), ScoringProviderBase)
# 85         try:
# 86             ScoringProviderFactory.register(name, cls)
# 87         except KeyError:
# 88             pass
# 89 
# 90     #  State managers
# 91     for row in cur.execute("SELECT name, artifact_path FROM state_manager"):
# 92         name, artifact = row
# 93         path = Path(artifact)
# 94         cls = _import_class(path.as_posix(), StateManagerBase)
# 95         try:
# 96             StateManagerFactory.register(name, cls)
# 97         except KeyError:
# 98             pass
# 99 
# 100     #  System managers are loaded via extensions; only register configs
# 101     for row in cur.execute(
# 102         "SELECT id, system_manager_id, scoring_model_id FROM experiment_config"
# 103     ):
# 104         config_id, system_manager_id, scoring_model_id = row
# 105         config = {
# 106             "system_manager_id": system_manager_id,
# 107             "scoring_model_id": scoring_model_id,
# 108         }
# 109         ExperimentConfigProvider.register(config_id, config)
# 110 
# 111     conn.close()
# 112 
# 113 
# 114 if __name__ == "__main__":
# 115     seed_registries(reset_db=True)


# enums\__init__.py



# enums\agent_enums.py

# 1 from __future__ import annotations
# 2 
# 3 from enum import Enum
# 4 
# 5 
# 6 class AgentRole(str, Enum):
# 7     GENERATOR = "generator"
# 8     DISCRIMINATOR = "discriminator"
# 9     MEDIATOR = "mediator"
# 10     PATCHER = "patcher"
# 11     EVALUATOR = "evaluator"
# 12 
# 13 
# 14 class AgentState(Enum):
# 15     INIT = "init"
# 16     RUNNING = "running"
# 17     COMPLETE = "complete"


# enums\logging_enums.py

# 1 from enum import Enum
# 2 
# 3 
# 4 class LogType(str, Enum):
# 5     EXPERIMENT = "experiment"
# 6     STATE = "state"
# 7     STATE_TRANSITION = "state_transition"
# 8     PROMPT = "prompt"
# 9     CONVERSATION = "conversation"
# 10     SCORING = "scoring"
# 11     CODE_QUALITY = "code_quality"
# 12     ERROR = "error"


# enums\scoring_enums.py

# 1 from enum import Enum
# 2 
# 3 
# 4 class ScoringMetric(str, Enum):
# 5     FUNCTIONAL_CORRECTNESS = "functional_correctness"
# 6     MAINTAINABILITY_INDEX = "maintainability_index"
# 7     BUG_FIX_SUCCESS = "bug_fix_success_rate"
# 8     LINTING_COMPLIANCE = "linting_compliance"
# 9     AGENT_SUCCESS_RATE = "agent_role_success_rate"
# 10     RETRY_SUCCESS_RATE = "retry_success_rate"


# enums\system_enums.py

# 1 from __future__ import annotations
# 2 
# 3 from enum import Enum
# 4 
# 5 
# 6 class SystemType(str, Enum):
# 7     #  Core Code Transformation
# 8     LINTING = "linting"
# 9     FORMATTING = "formatting"
# 10     DOCSTRING = "docstring"
# 11     TYPE_ANNOTATION = "type_annotation"
# 12     REFACTORING = "refactoring"
# 13     DEAD_CODE_REMOVAL = "dead_code_removal"
# 14     COMPLEXITY_REDUCTION = "complexity_reduction"
# 15 
# 16     #  Testing and Validation
# 17     UNIT_TESTING = "unit_testing"
# 18     EDGE_TESTING = "edge_testing"
# 19     INTEGRATION_TESTING = "integration_testing"
# 20     TEST_FIX_GENERATION = "test_fix_generation"
# 21     MOCK_GENERATION = "mock_generation"
# 22     STUB_EXTRACTION = "stub_extraction"
# 23 
# 24     #  Observability and Instrumentation
# 25     LOGGING_INJECTION = "logging_injection"
# 26     TRACE_ANNOTATION = "trace_annotation"
# 27     ERROR_HANDLING = "error_handling"
# 28 
# 29     #  Documentation and Explanation
# 30     INLINE_COMMENTING = "inline_commenting"
# 31     FUNCTION_SUMMARIZATION = "function_summarization"
# 32     FILE_SUMMARIZATION = "file_summarization"
# 33     CHANGE_SUMMARIZATION = "change_summarization"
# 34 
# 35     #  Review and Oversight
# 36     CODE_REVIEW = "code_review"
# 37     STATIC_ANALYSIS = "static_analysis"
# 38     REGRESSION_ANALYSIS = "regression_analysis"
# 39     MEDIATOR_RECONCILIATION = "mediator_reconciliation"
# 40     SCORING_EVALUATION = "scoring_evaluation"
# 41 
# 42     #  Planning and Organization
# 43     SYMBOL_GRAPH_GENERATION = "symbol_graph_generation"
# 44     DEPENDENCY_MAPPING = "dependency_mapping"
# 45     FILE_RESTRUCTURING = "file_restructuring"
# 46     FUNCTION_SPLITTING = "function_splitting"
# 47 
# 48     #  MLOps-Specific
# 49     DATA_VALIDATION = "data_validation"
# 50     METADATA_ANNOTATION = "metadata_annotation"
# 51     MODEL_DOC_GENERATION = "model_doc_generation"
# 52     PIPELINE_TESTING = "pipeline_testing"
# 53     TRAINING_CONFIG_AUDIT = "training_config_audit"
# 54     FEATURE_TRACEABILITY = "feature_traceability"
# 55 
# 56     #  Production Safety
# 57     SECURITY_CHECK = "security_check"
# 58     POLICY_COMPLIANCE = "policy_compliance"
# 59     LICENSE_CHECK = "license_check"
# 60 
# 61     #  Misc / Cross-cutting
# 62     PATCHING = "patching"
# 63     PROMPT_REFINEMENT = "prompt_refinement"
# 64     AGENT_RECOMMENDATION = "agent_recommendation"
# 65     SEMANTIC_DIFF = "semantic_diff"
# 66     CONTEXT_FILTERING = "context_filtering"
# 67 
# 68 
# 69 class SystemState(Enum):
# 70     """Finite state machine states for the CodeCritic system."""
# 71 
# 72     START = "start"
# 73     GENERATE = "generate"
# 74     DISCRIMINATE = "discriminate"
# 75     MEDIATE = "mediate"
# 76     PATCH = "patch"
# 77     EVALUATE = "evaluate"
# 78     END = "end"
# 79 
# 80 
# 81 class StateTransitionReason(str, Enum):
# 82     FIRST_ROUND = "first_round"
# 83     MAX_ITERATIONS_REACHED = "max_iterations_reached"
# 84     SCORE_THRESHOLD_MET = "score_threshold_met"
# 85     SCORE_STAGNATION = "score_stagnation"
# 86     AGENT_FAILURE = "agent_failure"
# 87     MEDIATOR_OVERRIDE = "mediator_override"
# 88     PATCH_RETRY = "patch_retry"
# 89     CUSTOM_RULE = "custom_rule"
# 90     END_REACHED = "end_reached"


# extensions\__init__.py



# extensions\agent_prompts\__init__.py

# 1 


# extensions\agents\__init__.py

# 1 from .dummy_agent import DummyAgent
# 2 from .generator_agent import GeneratorAgent
# 3 from .evaluator_agent import EvaluatorAgent
# 4 from .mediator_agent import MediatorAgent
# 5 from .patch_agent import PatchAgent
# 6 from ...registries.agents import AGENT_REGISTRY
# 7 
# 8 AGENT_REGISTRY.register("dummy", DummyAgent)
# 9 AGENT_REGISTRY.register("generator", GeneratorAgent)
# 10 AGENT_REGISTRY.register("evaluator", EvaluatorAgent)
# 11 AGENT_REGISTRY.register("mediator", MediatorAgent)
# 12 AGENT_REGISTRY.register("patch", PatchAgent)
# 13 
# 14 __all__ = [
# 15     "DummyAgent",
# 16     "GeneratorAgent",
# 17     "EvaluatorAgent",
# 18     "MediatorAgent",
# 19     "PatchAgent",
# 20 ]


# extensions\agents\dummy_agent.py

# 1 from ...abstract_classes.agent_base import AgentBase
# 2 
# 3 
# 4 class DummyAgent(AgentBase):
# 5     def _run_agent_logic(self, *args, **kwargs) -> None:
# 6         self._log.info("Dummy agent logic executed")


# extensions\agents\evaluator_agent.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import List
# 5 from datetime import datetime, timezone
# 6 
# 7 from ...abstract_classes.agent_base import AgentBase
# 8 from ...enums.agent_enums import AgentRole
# 9 from ...factories.tool_provider import ToolProviderFactory
# 10 from ...factories.logging_provider import (
# 11     CodeQualityLog,
# 12     ErrorLog,
# 13     LoggingProvider,
# 14 )
# 15 from ...utilities.snapshots.snapshot_writer import SnapshotWriter
# 16 
# 17 
# 18 def _analyze_radon(code: str) -> tuple[float, float]:
# 19     """Return cyclomatic complexity and maintainability index for the code."""
# 20     try:
# 21         from radon.complexity import cc_visit
# 22         from radon.metrics import mi_visit
# 23 
# 24         blocks = cc_visit(code)
# 25         cc = sum(b.complexity for b in blocks) / max(len(blocks), 1)
# 26         mi = mi_visit(code, True)
# 27         return float(cc), float(mi)
# 28     except Exception:
# 29         return 0.0, 0.0
# 30 
# 31 
# 32 class EvaluatorAgent(AgentBase):
# 33     """Run static analysis tools and log code quality metrics."""
# 34 
# 35     def __init__(
# 36         self,
# 37         target: str,
# 38         logger: LoggingProvider | None = None,
# 39         snapshot_writer: SnapshotWriter | None = None,
# 40     ) -> None:
# 41         super().__init__(logger)
# 42         self.target = target
# 43         self.snapshot_writer = snapshot_writer or SnapshotWriter()
# 44         self.mypy = ToolProviderFactory.create("mypy")
# 45         self.ruff = ToolProviderFactory.create("ruff")
# 46         self.black = ToolProviderFactory.create("black")
# 47         self.radon = ToolProviderFactory.create("radon")
# 48         self.quality_logs: List[CodeQualityLog] = []
# 49         self.error_logs: List[ErrorLog] = []
# 50 
# 51     def _run_agent_logic(self, *args, **kwargs) -> None:
# 52         before = Path(self.target).read_text(encoding="utf-8")
# 53         lines = len(before.splitlines())
# 54         complexity = 0.0
# 55         maintainability = 0.0
# 56         lint_return = 0
# 57 
# 58         try:
# 59             self.mypy.run(self.target)
# 60         except Exception as exc:  #  pragma: no cover - tool execution failure
# 61             self.error_logs.append(
# 62                 ErrorLog(
# 63                     experiment_id="exp",
# 64                     round=0,
# 65                     error_type=type(exc).__name__,
# 66                     message=str(exc),
# 67                     file_path=self.target,
# 68                 )
# 69             )
# 70 
# 71         try:
# 72             ruff_proc = self.ruff.run(self.target)
# 73             lint_return = ruff_proc.returncode
# 74         except Exception as exc:  #  pragma: no cover - tool execution failure
# 75             self.error_logs.append(
# 76                 ErrorLog(
# 77                     experiment_id="exp",
# 78                     round=0,
# 79                     error_type=type(exc).__name__,
# 80                     message=str(exc),
# 81                     file_path=self.target,
# 82                 )
# 83             )
# 84             lint_return = 1
# 85 
# 86         try:
# 87             self.black.run(self.target, check=True)
# 88         except Exception as exc:  #  pragma: no cover - tool execution failure
# 89             self.error_logs.append(
# 90                 ErrorLog(
# 91                     experiment_id="exp",
# 92                     round=0,
# 93                     error_type=type(exc).__name__,
# 94                     message=str(exc),
# 95                     file_path=self.target,
# 96                 )
# 97             )
# 98 
# 99         try:
# 100             #  Run radon CLI primarily for logging/debug purposes
# 101             self.radon.run(self.target)
# 102             complexity, maintainability = _analyze_radon(before)
# 103         except Exception as exc:
# 104             self._log.warning("Radon unavailable: %s", exc)
# 105             self.error_logs.append(
# 106                 ErrorLog(
# 107                     experiment_id="exp",
# 108                     round=0,
# 109                     error_type=type(exc).__name__,
# 110                     message=f"Radon analysis failed: {exc}",
# 111                     file_path=self.target,
# 112                     timestamp=datetime.now(timezone.utc),
# 113                 )
# 114             )
# 115 
# 116         log = CodeQualityLog(
# 117             experiment_id="exp",
# 118             round=0,
# 119             symbol=self.target,
# 120             lines_of_code=lines,
# 121             cyclomatic_complexity=complexity,
# 122             maintainability_index=maintainability,
# 123             lint_errors=0 if lint_return == 0 else 1,
# 124         )
# 125         self.quality_logs.append(log)
# 126         self.log_code_quality(log)
# 127         for err in self.error_logs:
# 128             self.log_error(err)
# 129 
# 130         after = Path(self.target).read_text(encoding="utf-8")
# 131         self.snapshot_writer.write_snapshot(
# 132             experiment_id="exp",
# 133             round=0,
# 134             file_path=self.target,
# 135             before=before,
# 136             after=after,
# 137             symbol=self.target,
# 138             agent_role=AgentRole.EVALUATOR,
# 139         )


# extensions\agents\generator_agent.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import List
# 4 from datetime import datetime, timezone
# 5 from pathlib import Path
# 6 
# 7 from ...abstract_classes.agent_base import AgentBase
# 8 from ...enums.agent_enums import AgentRole
# 9 from ...factories.tool_provider import ToolProviderFactory
# 10 from ...factories.logging_provider import (
# 11     ErrorLog,
# 12     PromptLog,
# 13     LoggingProvider,
# 14 )
# 15 from ...utilities.snapshots.snapshot_writer import SnapshotWriter
# 16 
# 17 
# 18 class GeneratorAgent(AgentBase):
# 19     """Agent responsible for formatting code using black."""
# 20 
# 21     def __init__(
# 22         self,
# 23         target: str,
# 24         logger: LoggingProvider | None = None,
# 25         snapshot_writer: SnapshotWriter | None = None,
# 26     ) -> None:
# 27         super().__init__(logger)
# 28         self.target = target
# 29         self.formatter = ToolProviderFactory.create("black")
# 30         self.docformatter = ToolProviderFactory.create("docformatter")
# 31         self.prompt_logs: List[PromptLog] = []
# 32         self.error_logs: List[ErrorLog] = []
# 33         self.snapshot_writer = snapshot_writer or SnapshotWriter()
# 34 
# 35     def _run_agent_logic(self, *args, **kwargs) -> None:
# 36         before = Path(self.target).read_text(encoding="utf-8")
# 37         log = PromptLog(
# 38             experiment_id="exp",
# 39             round=0,
# 40             system="system",
# 41             agent_id="generator",
# 42             agent_role=AgentRole.GENERATOR,
# 43             symbol=self.target,
# 44             prompt="format code",
# 45             response=None,
# 46             attempt_number=0,
# 47             agent_action_outcome="started",
# 48         )
# 49         self.prompt_logs.append(log)
# 50         try:
# 51             self.formatter.run(self.target)
# 52             self.docformatter.run(self.target)
# 53             log.agent_action_outcome = "success"
# 54             self._log.info("Formatted %s", self.target)
# 55         except Exception as exc:
# 56             log.agent_action_outcome = "error"
# 57             err = ErrorLog(
# 58                 experiment_id="exp",
# 59                 round=0,
# 60                 error_type=type(exc).__name__,
# 61                 message=str(exc),
# 62                 file_path=self.target,
# 63             )
# 64             self.error_logs.append(err)
# 65             self._log.exception("Formatting failed for %s", self.target)
# 66         finally:
# 67             log.stop = datetime.now(timezone.utc)
# 68             after = Path(self.target).read_text(encoding="utf-8")
# 69             self.snapshot_writer.write_snapshot(
# 70                 experiment_id="exp",
# 71                 round=0,
# 72                 file_path=self.target,
# 73                 before=before,
# 74                 after=after,
# 75                 symbol=self.target,
# 76                 agent_role=AgentRole.GENERATOR,
# 77             )
# 78             self.log_prompt(log)
# 79             for err in self.error_logs:
# 80                 self.log_error(err)


# extensions\agents\mediator_agent.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import List, Any
# 4 
# 5 from ...abstract_classes.agent_base import AgentBase
# 6 from ...enums.agent_enums import AgentRole
# 7 from ...factories.logging_provider import (
# 8     ConversationLog,
# 9     ErrorLog,
# 10     LoggingProvider,
# 11 )
# 12 from ...utilities.snapshots.snapshot_writer import SnapshotWriter
# 13 
# 14 
# 15 class MediatorAgent(AgentBase):
# 16     """Agent that reconciles discrepancies between generator and evaluator outputs."""
# 17 
# 18     def __init__(
# 19         self,
# 20         target: str = "generated.py",
# 21         logger: LoggingProvider | None = None,
# 22         snapshot_writer: SnapshotWriter | None = None,
# 23     ) -> None:
# 24         super().__init__(logger)
# 25         self.target = target
# 26         self.snapshot_writer = snapshot_writer or SnapshotWriter()
# 27         self.conversation_logs: List[ConversationLog] = []
# 28         self.error_logs: List[ErrorLog] = []
# 29 
# 30     def _run_agent_logic(self, *args, **kwargs) -> None:
# 31         code: str | None = kwargs.get("generator_output")
# 32         metrics: dict[str, Any] | None = kwargs.get("evaluation_metrics")
# 33 
# 34         if code is None or metrics is None:
# 35             err = ErrorLog(
# 36                 experiment_id="exp",
# 37                 round=0,
# 38                 error_type="ValueError",
# 39                 message="Missing generator output or evaluation metrics",
# 40                 file_path=self.target,
# 41             )
# 42             self.error_logs.append(err)
# 43             self.log_error(err)
# 44             return
# 45 
# 46         lint_errors = int(metrics.get("lint_errors", 0))
# 47         maintainability = float(metrics.get("maintainability_index", 100.0))
# 48         complexity = float(metrics.get("cyclomatic_complexity", 0.0))
# 49 
# 50         recommendations: List[str] = []
# 51         if lint_errors > 0:
# 52             recommendations.append("Resolve lint errors")
# 53         if maintainability < 70:
# 54             recommendations.append("Increase maintainability")
# 55         if complexity > 10:
# 56             recommendations.append("Reduce complexity")
# 57 
# 58         rec_text = "; ".join(recommendations)
# 59         conv = ConversationLog(
# 60             experiment_id="exp",
# 61             round=0,
# 62             agent_role=AgentRole.MEDIATOR,
# 63             target=self.target,
# 64             content=rec_text,
# 65             originating_agent="mediator",
# 66             intervention=bool(recommendations),
# 67             intervention_type="mediation" if recommendations else None,
# 68             intervention_reason="evaluation discrepancy" if recommendations else None,
# 69         )
# 70         self.conversation_logs.append(conv)
# 71 
# 72         try:
# 73             self.log_conversation(conv)
# 74         except Exception as exc:  #  pragma: no cover - db write failure
# 75             err = ErrorLog(
# 76                 experiment_id="exp",
# 77                 round=0,
# 78                 error_type=type(exc).__name__,
# 79                 message=str(exc),
# 80                 file_path=self.target,
# 81             )
# 82             self.error_logs.append(err)
# 83             self.log_error(err)
# 84 
# 85         after = code
# 86         if rec_text:
# 87             after = code + f"\n#  Mediator Recommendation: {rec_text}"
# 88 
# 89         self.snapshot_writer.write_snapshot(
# 90             experiment_id="exp",
# 91             round=0,
# 92             file_path=self.target,
# 93             before=code,
# 94             after=after,
# 95             symbol=self.target,
# 96             agent_role=AgentRole.MEDIATOR,
# 97         )


# extensions\agents\patch_agent.py

# 1 from __future__ import annotations
# 2 
# 3 import json
# 4 from pathlib import Path
# 5 from typing import Any, List
# 6 
# 7 from ...abstract_classes.agent_base import AgentBase
# 8 from ...enums.agent_enums import AgentRole
# 9 from ...factories.logging_provider import (
# 10     CodeQualityLog,
# 11     ConversationLog,
# 12     ErrorLog,
# 13     LoggingProvider,
# 14 )
# 15 from ...factories.tool_provider import ToolProviderFactory
# 16 from ...utilities.snapshots.snapshot_writer import SnapshotWriter
# 17 from .evaluator_agent import _analyze_radon
# 18 
# 19 
# 20 class PatchAgent(AgentBase):
# 21     """Apply mediator recommendations as code patches."""
# 22 
# 23     def __init__(
# 24         self,
# 25         target: str,
# 26         logger: LoggingProvider | None = None,
# 27         snapshot_writer: SnapshotWriter | None = None,
# 28     ) -> None:
# 29         super().__init__(logger)
# 30         self.target = target
# 31         self.snapshot_writer = snapshot_writer or SnapshotWriter()
# 32         self.black = ToolProviderFactory.create("black")
# 33         self.ruff = ToolProviderFactory.create("ruff")
# 34         self.mypy = ToolProviderFactory.create("mypy")
# 35         self.conversation_logs: List[ConversationLog] = []
# 36         self.error_logs: List[ErrorLog] = []
# 37         self.quality_logs: List[CodeQualityLog] = []
# 38 
# 39     def _apply_patch(self, code: str, op: dict[str, Any]) -> str:
# 40         if op.get("op") == "replace":
# 41             old = op.get("from", "")
# 42             new = op.get("to", "")
# 43             if old not in code:
# 44                 raise ValueError(f"pattern '{old}' not found")
# 45             return code.replace(old, new)
# 46         if op.get("op") == "append":
# 47             return code + op.get("text", "")
# 48         raise ValueError(f"unsupported op: {op.get('op')}")
# 49 
# 50     def _run_agent_logic(self, *args, **kwargs) -> None:  #  noqa: C901 - small
# 51         recs: List[ConversationLog] | None = kwargs.get("recommendations")
# 52         if not recs:
# 53             return
# 54         path = Path(self.target)
# 55         before = path.read_text(encoding="utf-8")
# 56         after = before
# 57         for conv in recs:
# 58             self.conversation_logs.append(conv)
# 59             try:
# 60                 ops = json.loads(conv.content)
# 61             except json.JSONDecodeError as exc:
# 62                 err = ErrorLog(
# 63                     experiment_id="exp",
# 64                     round=0,
# 65                     error_type=type(exc).__name__,
# 66                     message=str(exc),
# 67                     file_path=self.target,
# 68                 )
# 69                 self.error_logs.append(err)
# 70                 self.log_error(err)
# 71                 continue
# 72             self.log_conversation(conv)
# 73             for op in ops:
# 74                 try:
# 75                     after = self._apply_patch(after, op)
# 76                 except Exception as exc:  #  patch failure
# 77                     err = ErrorLog(
# 78                         experiment_id="exp",
# 79                         round=0,
# 80                         error_type=type(exc).__name__,
# 81                         message=str(exc),
# 82                         file_path=self.target,
# 83                     )
# 84                     self.error_logs.append(err)
# 85                     self.log_error(err)
# 86         path.write_text(after, encoding="utf-8")
# 87         try:
# 88             self.black.run(self.target)
# 89             self.ruff.run(self.target)
# 90             self.mypy.run(self.target)
# 91         except Exception as exc:  #  pragma: no cover - tool failure
# 92             err = ErrorLog(
# 93                 experiment_id="exp",
# 94                 round=0,
# 95                 error_type=type(exc).__name__,
# 96                 message=str(exc),
# 97                 file_path=self.target,
# 98             )
# 99             self.error_logs.append(err)
# 100             self.log_error(err)
# 101         final = path.read_text(encoding="utf-8")
# 102         complexity, maintainability = _analyze_radon(final)
# 103         quality = CodeQualityLog(
# 104             experiment_id="exp",
# 105             round=0,
# 106             symbol=self.target,
# 107             lines_of_code=len(final.splitlines()),
# 108             cyclomatic_complexity=complexity,
# 109             maintainability_index=maintainability,
# 110             lint_errors=0,
# 111         )
# 112         self.quality_logs.append(quality)
# 113         self.log_code_quality(quality)
# 114         for err in self.error_logs:
# 115             self.log_error(err)
# 116         self.snapshot_writer.write_snapshot(
# 117             experiment_id="exp",
# 118             round=0,
# 119             file_path=self.target,
# 120             before=before,
# 121             after=final,
# 122             symbol=self.target,
# 123             agent_role=AgentRole.PATCHER,
# 124         )


# extensions\context_providers\__init__.py

# 1 from .dummy_context_provider import DummyContextProvider
# 2 from .symbol_graph_provider import SymbolGraphProvider
# 3 from ...registries.context_providers import CONTEXT_PROVIDER_REGISTRY
# 4 
# 5 CONTEXT_PROVIDER_REGISTRY.register("dummy", DummyContextProvider)
# 6 CONTEXT_PROVIDER_REGISTRY.register("symbol_graph", SymbolGraphProvider)
# 7 
# 8 __all__ = ["DummyContextProvider", "SymbolGraphProvider"]


# extensions\context_providers\dummy_context_provider.py

# 1 from ...abstract_classes.context_provider_base import ContextProviderBase
# 2 
# 3 
# 4 class DummyContextProvider(ContextProviderBase):
# 5     def _get_context(self, *args, **kwargs):
# 6         self._log.info("Dummy context provided")
# 7         return {}


# extensions\context_providers\symbol_graph_provider.py

# 1 from __future__ import annotations
# 2 
# 3 import ast
# 4 from pathlib import Path
# 5 from typing import Any, Dict, List, Set
# 6 
# 7 from ...factories.logging_provider import LoggingProvider
# 8 
# 9 from ...abstract_classes.context_provider_base import ContextProviderBase
# 10 
# 11 
# 12 class SymbolGraphProvider(ContextProviderBase):
# 13     """Generate a simple symbol graph for a Python module."""
# 14 
# 15     def __init__(self, module_path: str, logger: LoggingProvider | None = None) -> None:
# 16         super().__init__(logger)
# 17         self.module_path = Path(module_path)
# 18 
# 19     def _get_context(self) -> Dict[str, Any]:
# 20         """Parse the module and return context information."""
# 21         if not self.module_path.exists():
# 22             self._log.error("Module not found: %s", self.module_path)
# 23             return {}
# 24 
# 25         source = self.module_path.read_text(encoding="utf-8")
# 26         tree = ast.parse(source)
# 27 
# 28         functions: List[str] = []
# 29         classes: Dict[str, List[str]] = {}
# 30         call_map: Dict[str, Set[str]] = {}
# 31 
# 32         for node in tree.body:
# 33             if isinstance(node, ast.FunctionDef):
# 34                 sig = self._format_signature(node)
# 35                 functions.append(sig)
# 36                 call_map[node.name] = self._collect_calls(node)
# 37             elif isinstance(node, ast.ClassDef):
# 38                 method_sigs: List[str] = []
# 39                 for item in node.body:
# 40                     if isinstance(item, ast.FunctionDef):
# 41                         sig = self._format_signature(item)
# 42                         method_sigs.append(sig)
# 43                         key = f"{node.name}.{item.name}"
# 44                         call_map[key] = self._collect_calls(item)
# 45                 classes[node.name] = method_sigs
# 46 
# 47         context = {
# 48             "functions": functions,
# 49             "classes": classes,
# 50             "call_map": {k: sorted(v) for k, v in call_map.items()},
# 51         }
# 52         self._log.info("Context generated for %s", self.module_path)
# 53         return context
# 54 
# 55     def _format_signature(self, func: ast.FunctionDef) -> str:
# 56         args = [arg.arg for arg in func.args.args]
# 57         if func.args.vararg:
# 58             args.append("*" + func.args.vararg.arg)
# 59         for kw in func.args.kwonlyargs:
# 60             args.append(kw.arg)
# 61         if func.args.kwarg:
# 62             args.append("**" + func.args.kwarg.arg)
# 63         return f"{func.name}({', '.join(args)})"
# 64 
# 65     def _collect_calls(self, func: ast.FunctionDef) -> Set[str]:
# 66         calls: Set[str] = set()
# 67         for node in ast.walk(func):
# 68             if isinstance(node, ast.Call):
# 69                 if isinstance(node.func, ast.Name):
# 70                     calls.add(node.func.id)
# 71                 elif isinstance(node.func, ast.Attribute):
# 72                     calls.add(node.func.attr)
# 73         return calls


# extensions\prompt_generators\__init__.py

# 1 from .dummy_prompt_generator import DummyPromptGenerator
# 2 from .basic_prompt_generator import BasicPromptGenerator
# 3 from ...registries.prompt_generators import PROMPT_GENERATOR_REGISTRY
# 4 
# 5 PROMPT_GENERATOR_REGISTRY.register("dummy", DummyPromptGenerator)
# 6 PROMPT_GENERATOR_REGISTRY.register("basic", BasicPromptGenerator)
# 7 
# 8 __all__ = ["DummyPromptGenerator", "BasicPromptGenerator"]


# extensions\prompt_generators\basic_prompt_generator.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Any, Dict
# 4 
# 5 from ...abstract_classes.prompt_generator_base import PromptGeneratorBase
# 6 from ...abstract_classes.context_provider_base import ContextProviderBase
# 7 from ...factories.logging_provider import LoggingProvider
# 8 
# 9 
# 10 class BasicPromptGenerator(PromptGeneratorBase):
# 11     """Combine system and agent templates with code context."""
# 12 
# 13     def __init__(
# 14         self,
# 15         context_provider: ContextProviderBase,
# 16         logger: LoggingProvider | None = None,
# 17     ) -> None:
# 18         super().__init__(logger)
# 19         self.context_provider = context_provider
# 20 
# 21     def _generate_prompt(
# 22         self, agent_config: Dict[str, Any], system_config: Dict[str, Any]
# 23     ) -> str:
# 24         context = self.context_provider.get_context()
# 25         self._log.debug("Raw context: %s", context)
# 26 
# 27         system_template = system_config.get("template", "")
# 28         agent_template = agent_config.get("template", "")
# 29 
# 30         context_snippet = ""
# 31         if context:
# 32             funcs = ", ".join(context.get("functions", []))
# 33             classes = ", ".join(context.get("classes", {}).keys())
# 34             context_snippet = f"Functions: {funcs}\nClasses: {classes}"
# 35 
# 36         prompt = f"{system_template}\n\n{agent_template}\n\n{context_snippet}"
# 37         self._log.info("Prompt generated")
# 38         self._log.debug("Prompt content: %s", prompt)
# 39         return prompt


# extensions\prompt_generators\dummy_prompt_generator.py

# 1 from ...abstract_classes.prompt_generator_base import PromptGeneratorBase
# 2 
# 3 
# 4 class DummyPromptGenerator(PromptGeneratorBase):
# 5     def _generate_prompt(self, *args, **kwargs) -> str:
# 6         self._log.info("Dummy prompt generated")
# 7         return "dummy prompt"


# extensions\scoring_models\__init__.py

# 1 from .dummy_scoring_provider import DummyScoringProvider
# 2 from .basic_scoring_provider import BasicScoringProvider
# 3 from ...registries.scoring_models import SCORING_MODEL_REGISTRY
# 4 
# 5 SCORING_MODEL_REGISTRY.register("dummy", DummyScoringProvider)
# 6 SCORING_MODEL_REGISTRY.register("basic", BasicScoringProvider)
# 7 
# 8 __all__ = ["DummyScoringProvider", "BasicScoringProvider"]


# extensions\scoring_models\basic_scoring_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Any, Dict, List
# 4 
# 5 from ...abstract_classes.scoring_provider_base import ScoringProviderBase
# 6 from ...utilities.metrics import compute_metrics
# 7 
# 8 
# 9 class BasicScoringProvider(ScoringProviderBase):
# 10     """Compute experiment metrics using basic heuristics."""
# 11 
# 12     def _score(self, logs: Dict[str, List[Any]]) -> Dict[str, float]:
# 13         self._log.debug("Scoring with logs: %s", logs)
# 14         evaluation_logs = logs.get("evaluation", [])
# 15         code_quality_logs = logs.get("code_quality", [])
# 16         conversation_logs = logs.get("conversation", [])
# 17         prompt_logs = logs.get("prompt", [])
# 18         state_logs = logs.get("state", [])
# 19 
# 20         metrics = compute_metrics(
# 21             evaluation_logs,
# 22             code_quality_logs,
# 23             conversation_logs,
# 24             prompt_logs,
# 25             state_logs,
# 26         )
# 27         self._log.info("Computed metrics: %s", metrics)
# 28         return metrics


# extensions\scoring_models\dummy_scoring_provider.py

# 1 from ...abstract_classes.scoring_provider_base import ScoringProviderBase
# 2 
# 3 
# 4 class DummyScoringProvider(ScoringProviderBase):
# 5     def _score(self, *args, **kwargs):
# 6         self._log.info("Dummy scoring")
# 7         return 0


# extensions\state_managers\__init__.py

# 1 from .dummy_state_manager import DummyStateManager
# 2 from .state_manager import StateManager
# 3 from ...registries.state_managers import STATE_MANAGER_REGISTRY
# 4 
# 5 STATE_MANAGER_REGISTRY.register("dummy", DummyStateManager)
# 6 STATE_MANAGER_REGISTRY.register("state", StateManager)
# 7 
# 8 __all__ = ["DummyStateManager", "StateManager"]


# extensions\state_managers\dummy_state_manager.py

# 1 from ...abstract_classes.state_manager_base import StateManagerBase
# 2 
# 3 
# 4 class DummyStateManager(StateManagerBase):
# 5     def _run_state_logic(self, *args, **kwargs) -> None:
# 6         self._log.info("Dummy state logic executed")


# extensions\state_managers\state_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ...abstract_classes.state_manager_base import StateManagerBase
# 4 from ...factories.logging_provider import LoggingProvider
# 5 
# 6 
# 7 class StateManager(StateManagerBase):
# 8     """Concrete manager executing actions within each FSM state."""
# 9 
# 10     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 11         super().__init__(logger)
# 12 
# 13     def _run_state_logic(self, *args, **kwargs) -> None:
# 14         state = kwargs.get("state")
# 15         self._log.info("Running state logic for %s", state)


# extensions\system_managers\__init__.py

# 1 from .dummy_system_manager import DummySystemManager
# 2 from .system_manager import SystemManager
# 3 from ...registries.system_managers import SYSTEM_MANAGER_REGISTRY
# 4 
# 5 SYSTEM_MANAGER_REGISTRY.register("dummy", DummySystemManager)
# 6 SYSTEM_MANAGER_REGISTRY.register("system", SystemManager)
# 7 
# 8 __all__ = ["DummySystemManager", "SystemManager"]


# extensions\system_managers\dummy_system_manager.py

# 1 from ...abstract_classes.system_manager_base import SystemManagerBase
# 2 
# 3 
# 4 class DummySystemManager(SystemManagerBase):
# 5     def _run_system_logic(self, *args, **kwargs) -> None:
# 6         self._log.info("Dummy system logic executed")


# extensions\system_managers\system_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import List
# 4 
# 5 from ...abstract_classes.system_manager_base import SystemManagerBase
# 6 from ...enums.system_enums import SystemState, StateTransitionReason
# 7 from ...factories.agent import AgentFactory
# 8 from ...factories.context_provider import ContextProviderFactory
# 9 from ...factories.prompt_manager import PromptGeneratorFactory
# 10 from ...factories.logging_provider import (
# 11     CodeQualityLog,
# 12     PromptLog,
# 13     ScoringLog,
# 14     StateLog,
# 15     StateTransitionLog,
# 16     LoggingProvider,
# 17 )
# 18 from ..state_managers.state_manager import StateManager
# 19 
# 20 
# 21 class SystemManager(SystemManagerBase):
# 22     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 23         super().__init__(logger)
# 24         self.state_manager = StateManager(logger=logger)
# 25         self.current_state = SystemState.START
# 26         self.transition_logs: List[StateTransitionLog] = []
# 27         self.state_logs: List[StateLog] = []
# 28         self.prompt_logs: List[PromptLog] = []
# 29         self.code_quality_logs: List[CodeQualityLog] = []
# 30         self.scoring_logs: List[ScoringLog] = []
# 31 
# 32         self.context_provider = ContextProviderFactory.create(
# 33             "symbol_graph", module_path="sample_module.py"
# 34         )
# 35         self.prompt_generator = PromptGeneratorFactory.create(
# 36             "basic", context_provider=self.context_provider
# 37         )
# 38         self.generator = AgentFactory.create(
# 39             "generator",
# 40             target="sample_module.py",
# 41             logger=logger,
# 42             snapshot_writer=None,
# 43         )
# 44         self.evaluator = AgentFactory.create(
# 45             "evaluator",
# 46             target="sample_module.py",
# 47             logger=logger,
# 48         )
# 49 
# 50     def _transition_to(
# 51         self, next_state: SystemState, reason: StateTransitionReason
# 52     ) -> None:
# 53         log = StateTransitionLog(
# 54             experiment_id="exp",
# 55             round=0,
# 56             from_state=self.current_state,
# 57             to_state=next_state,
# 58             reason=reason,
# 59         )
# 60         self.transition_logs.append(log)
# 61         self._log.info("%s -> %s", self.current_state.value, next_state.value)
# 62         self.log_transition(log)
# 63         self.current_state = next_state
# 64 
# 65     def _run_system_logic(self, *args, **kwargs) -> None:
# 66         sequence = [
# 67             SystemState.GENERATE,
# 68             SystemState.DISCRIMINATE,
# 69             SystemState.MEDIATE,
# 70             SystemState.PATCH,
# 71             SystemState.EVALUATE,
# 72             SystemState.END,
# 73         ]
# 74         for state in sequence:
# 75             self._transition_to(state, reason=StateTransitionReason.FIRST_ROUND)
# 76             if state is not SystemState.END:
# 77                 self.state_manager.run(state=state.value)
# 78                 self.state_logs.append(
# 79                     StateLog(
# 80                         experiment_id="exp",
# 81                         system="system",
# 82                         round=0,
# 83                         state=state,
# 84                         action="run",
# 85                     )
# 86                 )
# 87                 if state == SystemState.GENERATE:
# 88                     self.generator.run()
# 89                     self.prompt_logs.extend(self.generator.prompt_logs)
# 90                 elif state == SystemState.DISCRIMINATE:
# 91                     self.evaluator.run()
# 92                     self.code_quality_logs.extend(self.evaluator.quality_logs)
# 93 
# 94         for trans in self.transition_logs:
# 95             self.log_transition(trans)
# 96         for st in self.state_logs:
# 97             self.log_state(st)
# 98         for pr in self.prompt_logs:
# 99             self.log_prompt(pr)
# 100         for cq in self.code_quality_logs:
# 101             self.log_code_quality(cq)
# 102         for sc in self.scoring_logs:
# 103             self.log_scoring(sc)


# extensions\system_prompts\__init__.py

# 1 


# extensions\tool_providers\__init__.py

# 1 from .dummy_tool_provider import DummyToolProvider
# 2 from .black_runner import BlackToolProvider
# 3 from .mypy_runner import MypyToolProvider
# 4 from .radon_runner import RadonToolProvider
# 5 from .ruff_runner import RuffToolProvider
# 6 from .docformatter_runner import DocFormatterToolProvider
# 7 from ...registries.tool_providers import TOOL_PROVIDER_REGISTRY
# 8 
# 9 TOOL_PROVIDER_REGISTRY.register("dummy", DummyToolProvider)
# 10 TOOL_PROVIDER_REGISTRY.register("black", BlackToolProvider)
# 11 TOOL_PROVIDER_REGISTRY.register("mypy", MypyToolProvider)
# 12 TOOL_PROVIDER_REGISTRY.register("radon", RadonToolProvider)
# 13 TOOL_PROVIDER_REGISTRY.register("ruff", RuffToolProvider)
# 14 TOOL_PROVIDER_REGISTRY.register("docformatter", DocFormatterToolProvider)
# 15 
# 16 __all__ = [
# 17     "DummyToolProvider",
# 18     "BlackToolProvider",
# 19     "MypyToolProvider",
# 20     "RadonToolProvider",
# 21     "RuffToolProvider",
# 22     "DocFormatterToolProvider",
# 23 ]


# extensions\tool_providers\black_runner.py

# 1 from __future__ import annotations
# 2 import subprocess
# 3 import sys
# 4 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 5 
# 6 
# 7 class BlackToolProvider(ToolProviderBase):
# 8     def _run(self, target: str, check: bool = False):
# 9         #  Use quiet mode to suppress emojis and extra output on Windows
# 10         cmd = [sys.executable, "-m", "black", "--quiet", target]
# 11         if check:
# 12             cmd.append("--check")
# 13         proc = subprocess.run(
# 14             cmd, capture_output=True, text=True, encoding="utf-8", errors="ignore"
# 15         )
# 16         if proc.stdout:
# 17             self._log.debug(proc.stdout)
# 18         if proc.stderr:
# 19             self._log.error(proc.stderr)
# 20         if proc.returncode != 0:
# 21             raise RuntimeError(f"black failed: {proc.stderr}")
# 22         return proc


# extensions\tool_providers\docformatter_runner.py

# 1 import subprocess
# 2 import sys
# 3 from app.abstract_classes.tool_provider_base import ToolProviderBase
# 4 
# 5 
# 6 class DocFormatterToolProvider(ToolProviderBase):
# 7     def _run(self, target: str, check: bool = False):
# 8         cmd = [sys.executable, "-m", "docformatter", target, "--in-place"]
# 9         if check:
# 10             cmd.append("--check")
# 11         proc = subprocess.run(cmd, capture_output=True, text=True)
# 12         if proc.stdout:
# 13             self._log.debug(proc.stdout)
# 14         if proc.stderr:
# 15             self._log.error(proc.stderr)
# 16         if proc.returncode != 0:
# 17             raise RuntimeError(f"docformatter failed: {proc.stderr}")
# 18         return proc


# extensions\tool_providers\dummy_tool_provider.py

# 1 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 2 
# 3 
# 4 class DummyToolProvider(ToolProviderBase):
# 5     def _run(self, *args, **kwargs):
# 6         self._log.info("Dummy tool run")
# 7         return True


# extensions\tool_providers\mypy_runner.py

# 1 from __future__ import annotations
# 2 
# 3 import subprocess
# 4 import sys
# 5 
# 6 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 7 
# 8 
# 9 class MypyToolProvider(ToolProviderBase):
# 10     def _run(self, target: str):
# 11         cmd = [sys.executable, "-m", "mypy", target]
# 12         proc = subprocess.run(cmd, capture_output=True, text=True)
# 13         if proc.stdout:
# 14             self._log.debug(proc.stdout)
# 15         if proc.stderr:
# 16             self._log.error(proc.stderr)
# 17         if proc.returncode not in (0, 1):  #  mypy returns 1 if it finds issues
# 18             raise RuntimeError(f"mypy execution error: {proc.stderr}")
# 19         return proc


# extensions\tool_providers\radon_runner.py

# 1 from __future__ import annotations
# 2 import subprocess
# 3 import sys
# 4 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 5 
# 6 
# 7 class RadonToolProvider(ToolProviderBase):
# 8     def _run(self, target: str):
# 9         #  Call radon via -m so it returns a CompletedProcess
# 10         cmd = [sys.executable, "-m", "radon", "cc", target]
# 11         proc = subprocess.run(cmd, capture_output=True, text=True)
# 12 
# 13         if proc.stdout:
# 14             self._log.debug(proc.stdout)
# 15         if proc.stderr:
# 16             self._log.error(proc.stderr)
# 17 
# 18         if proc.returncode != 0:
# 19             raise RuntimeError(f"radon failed: {proc.stderr.strip()}")
# 20 
# 21         return proc


# extensions\tool_providers\ruff_runner.py

# 1 from __future__ import annotations
# 2 
# 3 import subprocess
# 4 import sys
# 5 
# 6 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 7 
# 8 
# 9 class RuffToolProvider(ToolProviderBase):
# 10     def _run(self, target: str):
# 11         cmd = [sys.executable, "-m", "ruff", "check", target]
# 12         proc = subprocess.run(cmd, capture_output=True, text=True)
# 13         if proc.stdout:
# 14             self._log.debug(proc.stdout)
# 15         if proc.stderr:
# 16             self._log.error(proc.stderr)
# 17         if proc.returncode != 0:
# 18             raise RuntimeError(f"ruff failed: {proc.stderr}")
# 19         return proc


# extensions\tool_providers\sonarcloud_runner.py

# 1 from __future__ import annotations
# 2 
# 3 from app.abstract_classes.tool_provider_base import ToolProviderBase
# 4 
# 5 
# 6 class SonarCloudToolProvider(ToolProviderBase):
# 7     def _run(self, project_key: str, organization: str, token: str):
# 8         self._log.info(
# 9             "SonarCloud stub executed for project '%s' in organization '%s'.",
# 10             project_key,
# 11             organization,
# 12         )
# 13         return {
# 14             "status": "success",
# 15             "project_key": project_key,
# 16             "organization": organization,
# 17             "analysis_url": "https://sonarcloud.io/dashboard?id=" + project_key,
# 18         }


# extensions\tools\__init__.py



# factories\__init__.py



# factories\agent.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.agents import AGENT_REGISTRY
# 4 
# 5 
# 6 class AgentFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             AGENT_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = AGENT_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"Agent {name} not registered")
# 19         return cls(**kwargs)


# factories\context_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.context_providers import CONTEXT_PROVIDER_REGISTRY
# 4 
# 5 
# 6 class ContextProviderFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             CONTEXT_PROVIDER_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = CONTEXT_PROVIDER_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"Context provider {name} not registered")
# 19         return cls(**kwargs)


# factories\experiment_config_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Dict, Any
# 4 
# 5 _CONFIGS: Dict[str, Dict[str, Any]] = {}
# 6 
# 7 
# 8 class ExperimentConfigProvider:
# 9     """Simple provider storing experiment configurations in memory."""
# 10 
# 11     @staticmethod
# 12     def register(config_id: int | str, config: Dict[str, Any]) -> None:
# 13         _CONFIGS[str(config_id)] = config
# 14 
# 15     @staticmethod
# 16     def load(config_id: int | str) -> Dict[str, Any]:
# 17         config = _CONFIGS.get(str(config_id))
# 18         if config is None:
# 19             raise KeyError(f"experiment config {config_id} not found")
# 20         return config


# factories\logging_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from datetime import datetime
# 4 import json
# 5 import logging
# 6 import sqlite3
# 7 from dataclasses import asdict, is_dataclass
# 8 from enum import Enum
# 9 from pathlib import Path
# 10 from typing import Any, ClassVar, Iterable
# 11 
# 12 from app.enums.logging_enums import LogType
# 13 from app.utilities.db import get_connection
# 14 from ..utilities.metadata.logging.log_schemas import (
# 15     StateLog,
# 16     StateTransitionLog,
# 17     PromptLog,
# 18     CodeQualityLog,
# 19     ErrorLog,
# 20     ScoringLog,
# 21     ConversationLog,
# 22     ExperimentLog,
# 23 )
# 24 
# 25 
# 26 LOG_MODEL_MAP = {
# 27     LogType.STATE: StateLog,
# 28     LogType.STATE_TRANSITION: StateTransitionLog,
# 29     LogType.PROMPT: PromptLog,
# 30     LogType.CODE_QUALITY: CodeQualityLog,
# 31     LogType.ERROR: ErrorLog,
# 32     LogType.SCORING: ScoringLog,
# 33     LogType.CONVERSATION: ConversationLog,
# 34     LogType.EXPERIMENT: ExperimentLog,
# 35 }
# 36 
# 37 
# 38 class LoggingProvider:
# 39     """Centralized provider for structured logging."""
# 40 
# 41     _instance: ClassVar["LoggingProvider" | None] = None
# 42 
# 43     def __new__(cls, *args, **kwargs):
# 44         if cls._instance is None:
# 45             cls._instance = super().__new__(cls)
# 46         return cls._instance
# 47 
# 48     def __init__(
# 49         self,
# 50         db_path: str | Path = "experiments/codecritic.sqlite3",
# 51         output_path: str | Path | None = None,
# 52         connection: sqlite3.Connection | None = None,
# 53     ) -> None:
# 54         if getattr(self, "_initialized", False):
# 55             return
# 56 
# 57         self.db_path = Path(db_path)
# 58         self.db_path.parent.mkdir(exist_ok=True, parents=True)
# 59 
# 60         from app.utilities.db import init_db, get_connection
# 61 
# 62         if connection:
# 63             self.conn = connection
# 64             init_db(self.conn)  #  Explicit schema initialization for the provided connection
# 65         else:
# 66             self.conn = get_connection()
# 67             init_db()  #  Schema initialization when using default connection
# 68 
# 69         self.output_path = Path(output_path) if output_path else None
# 70         if self.output_path:
# 71             self.output_path.parent.mkdir(parents=True, exist_ok=True)
# 72 
# 73         self._initialized = True
# 74         
# 75     def _serialize(self, obj: Any) -> dict:
# 76         def _safe(v: Any) -> Any:
# 77             if isinstance(v, (Enum, Path)):
# 78                 return str(v)
# 79             elif isinstance(v, datetime):
# 80                 return v.isoformat()
# 81             elif isinstance(v, list):
# 82                 return [_safe(i) for i in v]
# 83             elif isinstance(v, dict):
# 84                 return {k: _safe(val) for k, val in v.items()}
# 85             return v
# 86 
# 87         if not is_dataclass(obj) or isinstance(obj, type):
# 88             raise TypeError(f"Expected dataclass instance, got {type(obj)}")
# 89         return _safe(asdict(obj))
# 90 
# 91     def _insert_many(self, table: str, items: Iterable[dict]) -> None:
# 92         items = list(items)
# 93         if not items:
# 94             return
# 95         keys = list(items[0].keys())
# 96         cols = ",".join(keys)
# 97         placeholders = ",".join(["?"] * len(keys))
# 98         values = [tuple(i[k] for k in keys) for i in items]
# 99         cur = self.conn.cursor()
# 100         cur.executemany(f"INSERT INTO {table} ({cols}) VALUES ({placeholders})", values)
# 101         self.conn.commit()
# 102         if self.output_path:
# 103             with self.output_path.open("a", encoding="utf-8") as fh:
# 104                 for item in items:
# 105                     fh.write(json.dumps(item) + "\n")
# 106 
# 107     def write(self, log_type: LogType, entries: list[Any] | Any) -> None:
# 108         if not isinstance(entries, list):
# 109             entries = [entries]
# 110         model_cls = LOG_MODEL_MAP.get(log_type)
# 111         if model_cls is None:
# 112             raise ValueError(f"Unsupported log type: {log_type}")
# 113         for entry in entries:
# 114             if not isinstance(entry, model_cls):
# 115                 raise TypeError(f"Expected {model_cls.__name__}, got {type(entry)}")
# 116         serialized = [self._serialize(e) for e in entries]
# 117         self._insert_many(log_type.value + "_log", serialized)
# 118 
# 119     #  Convenience wrappers
# 120     def log_state(self, log: StateLog) -> None:
# 121         self.write(LogType.STATE, log)
# 122 
# 123     def log_transition(self, log: StateTransitionLog) -> None:
# 124         self.write(LogType.STATE_TRANSITION, log)
# 125 
# 126     def log_prompt(self, log: PromptLog) -> None:
# 127         self.write(LogType.PROMPT, log)
# 128 
# 129     def log_code_quality(self, log: CodeQualityLog) -> None:
# 130         self.write(LogType.CODE_QUALITY, log)
# 131 
# 132     def log_error(self, log: ErrorLog) -> None:
# 133         self.write(LogType.ERROR, log)
# 134 
# 135     def log_scoring(self, log: ScoringLog) -> None:
# 136         self.write(LogType.SCORING, log)
# 137 
# 138     def log_conversation(self, log: ConversationLog) -> None:
# 139         self.write(LogType.CONVERSATION, log)
# 140 
# 141     def log_experiment(self, log: ExperimentLog) -> None:
# 142         self.write(LogType.EXPERIMENT, log)
# 143 
# 144     def close(self) -> None:
# 145         self.conn.close()
# 146 
# 147 
# 148 class LoggingMixin:
# 149     """Mixin to provide access to a shared LoggingProvider instance."""
# 150 
# 151     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 152         self.logger = logger or LoggingProvider()
# 153         self._log = logging.getLogger(self.__class__.__name__)
# 154 
# 155     def log_state(self, log: StateLog) -> None:
# 156         self.logger.log_state(log)
# 157 
# 158     def log_transition(self, log: StateTransitionLog) -> None:
# 159         self.logger.log_transition(log)
# 160 
# 161     def log_prompt(self, log: PromptLog) -> None:
# 162         self.logger.log_prompt(log)
# 163 
# 164     def log_code_quality(self, log: CodeQualityLog) -> None:
# 165         self.logger.log_code_quality(log)
# 166 
# 167     def log_error(self, log: ErrorLog) -> None:
# 168         self.logger.log_error(log)
# 169 
# 170     def log_scoring(self, log: ScoringLog) -> None:
# 171         self.logger.log_scoring(log)
# 172 
# 173     def log_conversation(self, log: ConversationLog) -> None:
# 174         self.logger.log_conversation(log)
# 175 
# 176     def log_experiment(self, log: ExperimentLog) -> None:
# 177         self.logger.log_experiment(log)


# factories\prompt_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.prompt_generators import PROMPT_GENERATOR_REGISTRY
# 4 
# 5 
# 6 class PromptGeneratorFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             PROMPT_GENERATOR_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = PROMPT_GENERATOR_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"Prompt generator {name} not registered")
# 19         return cls(**kwargs)


# factories\scoring_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.scoring_models import SCORING_MODEL_REGISTRY
# 4 
# 5 
# 6 class ScoringProviderFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             SCORING_MODEL_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = SCORING_MODEL_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"Scoring provider {name} not registered")
# 19         return cls(**kwargs)


# factories\state_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.state_managers import STATE_MANAGER_REGISTRY
# 4 
# 5 
# 6 class StateManagerFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             STATE_MANAGER_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = STATE_MANAGER_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"State manager {name} not registered")
# 19         return cls(**kwargs)


# factories\system_config_provider.py

# 1 from __future__ import annotations
# 2 
# 3 
# 4 class SystemConfigProvider:
# 5     @staticmethod
# 6     def create(config: dict):
# 7         return config


# factories\system_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.system_managers import SYSTEM_MANAGER_REGISTRY
# 4 
# 5 
# 6 class SystemManagerFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             SYSTEM_MANAGER_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = SYSTEM_MANAGER_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"System manager {name} not registered")
# 19         return cls(**kwargs)


# factories\tool_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.tool_providers import TOOL_PROVIDER_REGISTRY
# 4 
# 5 
# 6 class ToolProviderFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             TOOL_PROVIDER_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = TOOL_PROVIDER_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"Tool provider {name} not registered")
# 19         return cls(**kwargs)


# registries\__init__.py

# 1 class Registry:
# 2     def __init__(self):
# 3         self._registry = {}
# 4 
# 5     def register(self, name: str, item):
# 6         existing = self._registry.get(name)
# 7         if existing is not None:
# 8             if existing is item:
# 9                 return
# 10             raise KeyError(f"{name} already registered")
# 11         self._registry[name] = item
# 12 
# 13     def get(self, name: str):
# 14         return self._registry.get(name)
# 15 
# 16     def all(self):
# 17         return dict(self._registry)


# registries\agent_engines\__init__.py

# 1 from .. import Registry
# 2 
# 3 AGENT_ENGINE_REGISTRY = Registry()


# registries\agent_prompts\__init__.py

# 1 from .. import Registry
# 2 
# 3 AGENT_PROMPT_REGISTRY = Registry()


# registries\agents\__init__.py

# 1 from .. import Registry
# 2 
# 3 AGENT_REGISTRY = Registry()


# registries\context_providers\__init__.py

# 1 from .. import Registry
# 2 
# 3 CONTEXT_PROVIDER_REGISTRY = Registry()


# registries\prompt_generators\__init__.py

# 1 from .. import Registry
# 2 
# 3 PROMPT_GENERATOR_REGISTRY = Registry()


# registries\scoring_models\__init__.py

# 1 from .. import Registry
# 2 
# 3 SCORING_MODEL_REGISTRY = Registry()


# registries\state_managers\__init__.py

# 1 from .. import Registry
# 2 
# 3 STATE_MANAGER_REGISTRY = Registry()


# registries\system_managers\__init__.py

# 1 from .. import Registry
# 2 
# 3 SYSTEM_MANAGER_REGISTRY = Registry()


# registries\system_prompts\__init__.py

# 1 from .. import Registry
# 2 
# 3 SYSTEM_PROMPT_REGISTRY = Registry()


# registries\tool_providers\__init__.py

# 1 from .. import Registry
# 2 
# 3 TOOL_PROVIDER_REGISTRY = Registry()


# registries\tools\__init__.py

# 1 from .. import Registry
# 2 
# 3 TOOL_REGISTRY = Registry()


# schemas\__init__.py

# 1 from .agent_engine_schema import AgentEngine
# 2 from .agent_prompt_schema import AgentPrompt
# 3 from .system_prompt_schema import SystemPrompt
# 4 from .context_provider_schema import ContextProvider
# 5 from .tooling_provider_schema import ToolingProvider
# 6 from .file_path_schema import FilePath
# 7 from .agent_config_schema import AgentConfig
# 8 from .prompt_generator_schema import PromptGenerator
# 9 from .scoring_provider_schema import ScoringProvider
# 10 from .state_manager_schema import StateManager
# 11 from .system_config_schema import SystemConfig
# 12 from .experiment_config_schema import ExperimentConfig
# 13 from .series_schema import Series
# 14 
# 15 __all__ = [
# 16     "AgentEngine",
# 17     "AgentPrompt",
# 18     "SystemPrompt",
# 19     "ContextProvider",
# 20     "ToolingProvider",
# 21     "FilePath",
# 22     "AgentConfig",
# 23     "PromptGenerator",
# 24     "ScoringProvider",
# 25     "StateManager",
# 26     "SystemConfig",
# 27     "ExperimentConfig",
# 28     "Series",
# 29 ]


# schemas\agent_config_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator, model_validator
# 8 
# 9 from .agent_engine_schema import AgentEngine
# 10 from .prompt_generator_schema import PromptGenerator
# 11 
# 12 
# 13 class AgentConfig(BaseModel):
# 14     """Schema for the agent_config table."""
# 15 
# 16     id: Optional[int] = None
# 17     name: str
# 18     description: Optional[str] = None
# 19     agent_role: Optional[str] = None
# 20     system_type: Optional[str] = None
# 21     agent_engine_id: Optional[int] = None
# 22     prompt_generator_id: Optional[int] = None
# 23     agent_engine: Optional[AgentEngine] = None
# 24     prompt_generator: Optional[PromptGenerator] = None
# 25     artifact_path: Optional[Path] = None
# 26 
# 27     table_name: str = "agent_config"
# 28 
# 29     @model_validator(mode="after")
# 30     def _sync_ids(self) -> "AgentConfig":
# 31         if self.agent_engine is not None and self.agent_engine_id is None:
# 32             self.agent_engine_id = getattr(self.agent_engine, "id", None)
# 33         if self.prompt_generator is not None and self.prompt_generator_id is None:
# 34             self.prompt_generator_id = getattr(self.prompt_generator, "id", None)
# 35         return self
# 36 
# 37     @field_validator("artifact_path")
# 38     @classmethod
# 39     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 40         if v is None:
# 41             return v
# 42         p = Path(v)
# 43         if not p.is_absolute() and ".." in p.parts:
# 44             raise ValueError("artifact_path must be absolute or project relative")
# 45         return p
# 46 
# 47     def model_dump(self, **kwargs) -> dict:
# 48         return super().model_dump(**kwargs)


# schemas\agent_engine_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class AgentEngine(BaseModel):
# 11     """Schema for the agent_engine table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     model: Optional[str] = None
# 17     engine_config: Optional[str] = None
# 18     artifact_path: Optional[Path] = None
# 19 
# 20     table_name: str = "agent_engine"
# 21 
# 22     @field_validator("artifact_path")
# 23     @classmethod
# 24     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 25         if v is None:
# 26             return v
# 27         p = Path(v)
# 28         if not p.is_absolute() and ".." in p.parts:
# 29             raise ValueError("artifact_path must be absolute or project relative")
# 30         return p
# 31 
# 32     def model_dump(self, **kwargs) -> dict:
# 33         return super().model_dump(**kwargs)


# schemas\agent_prompt_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class AgentPrompt(BaseModel):
# 11     """Schema for the agent_prompt table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     agent_role: Optional[str] = None
# 17     system_type: Optional[str] = None
# 18     artifact_path: Optional[Path] = None
# 19 
# 20     table_name: str = "agent_prompt"
# 21 
# 22     @field_validator("artifact_path")
# 23     @classmethod
# 24     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 25         if v is None:
# 26             return v
# 27         p = Path(v)
# 28         if not p.is_absolute() and ".." in p.parts:
# 29             raise ValueError("artifact_path must be absolute or project relative")
# 30         return p
# 31 
# 32     def model_dump(self, **kwargs) -> dict:
# 33         return super().model_dump(**kwargs)


# schemas\context_provider_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .tooling_provider_schema import ToolingProvider
# 9 
# 10 
# 11 class ContextProvider(BaseModel):
# 12     """Schema for the context_provider table."""
# 13 
# 14     id: Optional[int] = None
# 15     name: str
# 16     description: Optional[str] = None
# 17     system_type: Optional[str] = None
# 18     tooling_provider_id: Optional[int] = None
# 19     tooling_provider: Optional[ToolingProvider] = None
# 20 
# 21     table_name: str = "context_provider"
# 22 
# 23     @model_validator(mode="after")
# 24     def _sync_ids(self) -> "ContextProvider":
# 25         if self.tooling_provider is not None and self.tooling_provider_id is None:
# 26             self.tooling_provider_id = getattr(self.tooling_provider, "id", None)
# 27         return self
# 28 
# 29     def model_dump(self, **kwargs) -> dict:
# 30         return super().model_dump(**kwargs)


# schemas\experiment_config_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .system_config_schema import SystemConfig
# 9 from .scoring_provider_schema import ScoringProvider
# 10 
# 11 
# 12 class ExperimentConfig(BaseModel):
# 13     """Schema for the experiment_config table."""
# 14 
# 15     id: Optional[int] = None
# 16     name: str
# 17     description: Optional[str] = None
# 18     system_manager_id: Optional[int | str] = None
# 19     scoring_model_id: Optional[int | str] = None
# 20 
# 21     system_manager: Optional[SystemConfig] = None
# 22     scoring_model: Optional[ScoringProvider] = None
# 23 
# 24     table_name: str = "experiment_config"
# 25 
# 26     @model_validator(mode="after")
# 27     def _sync_ids(self) -> "ExperimentConfig":
# 28         if self.system_manager is not None and self.system_manager_id is None:
# 29             self.system_manager_id = getattr(self.system_manager, "id", None)
# 30         if self.scoring_model is not None and self.scoring_model_id is None:
# 31             self.scoring_model_id = getattr(self.scoring_model, "id", None)
# 32         return self
# 33 
# 34     def model_dump(self, **kwargs) -> dict:
# 35         return super().model_dump(**kwargs)


# schemas\file_path_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import field_validator
# 7 
# 8 
# 9 class FilePath(BaseModel):
# 10     """Schema for file paths tracked in the database."""
# 11 
# 12     artifact_path: Path
# 13 
# 14     table_name: str = "file_path"
# 15 
# 16     @field_validator("artifact_path")
# 17     @classmethod
# 18     def _check_path(cls, v: Path) -> Path:
# 19         p = Path(v)
# 20         if not p.is_absolute() and ".." in p.parts:
# 21             raise ValueError("artifact_path must be absolute or project relative")
# 22         return p
# 23 
# 24     def model_dump(self, **kwargs) -> dict:
# 25         return super().model_dump(**kwargs)


# schemas\prompt_generator_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator, model_validator
# 8 
# 9 from .agent_prompt_schema import AgentPrompt
# 10 from .system_prompt_schema import SystemPrompt
# 11 from .context_provider_schema import ContextProvider
# 12 
# 13 
# 14 class PromptGenerator(BaseModel):
# 15     """Schema for the prompt_generator table."""
# 16 
# 17     id: Optional[int] = None
# 18     name: str
# 19     description: Optional[str] = None
# 20     agent_prompt_id: Optional[int] = None
# 21     system_prompt_id: Optional[int] = None
# 22     content_provider_id: Optional[int] = None
# 23 
# 24     agent_prompt: Optional[AgentPrompt] = None
# 25     system_prompt: Optional[SystemPrompt] = None
# 26     content_provider: Optional[ContextProvider] = None
# 27     artifact_path: Optional[Path] = None
# 28 
# 29     table_name: str = "prompt_generator"
# 30 
# 31     @model_validator(mode="after")
# 32     def _sync_ids(self) -> "PromptGenerator":
# 33         if self.agent_prompt is not None and self.agent_prompt_id is None:
# 34             self.agent_prompt_id = getattr(self.agent_prompt, "id", None)
# 35         if self.system_prompt is not None and self.system_prompt_id is None:
# 36             self.system_prompt_id = getattr(self.system_prompt, "id", None)
# 37         if self.content_provider is not None and self.content_provider_id is None:
# 38             self.content_provider_id = getattr(self.content_provider, "id", None)
# 39         return self
# 40 
# 41     @field_validator("artifact_path")
# 42     @classmethod
# 43     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 44         if v is None:
# 45             return v
# 46         p = Path(v)
# 47         if not p.is_absolute() and ".." in p.parts:
# 48             raise ValueError("artifact_path must be absolute or project relative")
# 49         return p
# 50 
# 51     def model_dump(self, **kwargs) -> dict:
# 52         return super().model_dump(**kwargs)


# schemas\scoring_provider_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class ScoringProvider(BaseModel):
# 11     """Schema for the scoring_provider table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     artifact_path: Optional[Path] = None
# 17 
# 18     table_name: str = "scoring_provider"
# 19 
# 20     @field_validator("artifact_path")
# 21     @classmethod
# 22     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 23         if v is None:
# 24             return v
# 25         p = Path(v)
# 26         if not p.is_absolute() and ".." in p.parts:
# 27             raise ValueError("artifact_path must be absolute or project relative")
# 28         return p
# 29 
# 30     def model_dump(self, **kwargs) -> dict:
# 31         return super().model_dump(**kwargs)


# schemas\series_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .experiment_config_schema import ExperimentConfig
# 9 
# 10 
# 11 class Series(BaseModel):
# 12     """Schema for the series table."""
# 13 
# 14     id: Optional[int] = None
# 15     experiment_config_id: Optional[int] = None
# 16     experiment_config: Optional[ExperimentConfig] = None
# 17 
# 18     table_name: str = "series"
# 19 
# 20     @model_validator(mode="after")
# 21     def _sync_ids(self) -> "Series":
# 22         if self.experiment_config is not None and self.experiment_config_id is None:
# 23             self.experiment_config_id = getattr(self.experiment_config, "id", None)
# 24         return self
# 25 
# 26     def model_dump(self, **kwargs) -> dict:
# 27         return super().model_dump(**kwargs)


# schemas\state_manager_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class StateManager(BaseModel):
# 11     """Schema for the state_manager table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     system_state: Optional[str] = None
# 17     system_type: Optional[str] = None
# 18     agent_id: Optional[int] = None
# 19     artifact_path: Optional[Path] = None
# 20 
# 21     table_name: str = "state_manager"
# 22 
# 23     @field_validator("artifact_path")
# 24     @classmethod
# 25     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 26         if v is None:
# 27             return v
# 28         p = Path(v)
# 29         if not p.is_absolute() and ".." in p.parts:
# 30             raise ValueError("artifact_path must be absolute or project relative")
# 31         return p
# 32 
# 33     def model_dump(self, **kwargs) -> dict:
# 34         return super().model_dump(**kwargs)


# schemas\system_config_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .state_manager_schema import StateManager
# 9 from .scoring_provider_schema import ScoringProvider
# 10 
# 11 
# 12 class SystemConfig(BaseModel):
# 13     """Schema for the system_config table."""
# 14 
# 15     id: Optional[int] = None
# 16     name: str
# 17     description: Optional[str] = None
# 18     system_type: Optional[str] = None
# 19     state_manager_id: Optional[int] = None
# 20     scoring_model_id: Optional[int] = None
# 21 
# 22     state_manager: Optional[StateManager] = None
# 23     scoring_model: Optional[ScoringProvider] = None
# 24 
# 25     table_name: str = "system_config"
# 26 
# 27     @model_validator(mode="after")
# 28     def _sync_ids(self) -> "SystemConfig":
# 29         if self.state_manager is not None and self.state_manager_id is None:
# 30             self.state_manager_id = getattr(self.state_manager, "id", None)
# 31         if self.scoring_model is not None and self.scoring_model_id is None:
# 32             self.scoring_model_id = getattr(self.scoring_model, "id", None)
# 33         return self
# 34 
# 35     def model_dump(self, **kwargs) -> dict:
# 36         return super().model_dump(**kwargs)


# schemas\system_prompt_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class SystemPrompt(BaseModel):
# 11     """Schema for the system_prompt table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     system_type: Optional[str] = None
# 17     artifact_path: Optional[Path] = None
# 18 
# 19     table_name: str = "system_prompt"
# 20 
# 21     @field_validator("artifact_path")
# 22     @classmethod
# 23     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 24         if v is None:
# 25             return v
# 26         p = Path(v)
# 27         if not p.is_absolute() and ".." in p.parts:
# 28             raise ValueError("artifact_path must be absolute or project relative")
# 29         return p
# 30 
# 31     def model_dump(self, **kwargs) -> dict:
# 32         return super().model_dump(**kwargs)


# schemas\tooling_provider_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class ToolingProvider(BaseModel):
# 11     """Schema for the tooling_provider table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     artifact_path: Optional[Path] = None
# 17 
# 18     table_name: str = "tooling_provider"
# 19 
# 20     @field_validator("artifact_path")
# 21     @classmethod
# 22     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 23         if v is None:
# 24             return v
# 25         p = Path(v)
# 26         if not p.is_absolute() and ".." in p.parts:
# 27             raise ValueError("artifact_path must be absolute or project relative")
# 28         return p
# 29 
# 30     def model_dump(self, **kwargs) -> dict:
# 31         return super().model_dump(**kwargs)


# utilities\__init__.py



# utilities\db.py

# 1 import sqlite3
# 2 from pathlib import Path
# 3 from typing import Iterable, Any
# 4 
# 5 DB_PATH = Path("experiments") / "codecritic.sqlite3"
# 6 _CONN: sqlite3.Connection | None = None
# 7 
# 8 
# 9 def _serialize(obj: Any) -> dict:
# 10     from dataclasses import asdict, is_dataclass
# 11     from enum import Enum
# 12     from pathlib import Path
# 13 
# 14     if not is_dataclass(obj) or isinstance(obj, type):
# 15         raise TypeError(f"Expected dataclass instance, got {type(obj)}")
# 16 
# 17     result = asdict(obj)
# 18     return {
# 19         k: (str(v) if isinstance(v, (Path, Enum)) else v) for k, v in result.items()
# 20     }
# 21 
# 22 
# 23 def get_connection() -> sqlite3.Connection:
# 24     global _CONN
# 25     try:
# 26         if _CONN is None:
# 27             raise RuntimeError
# 28         _CONN.execute("SELECT 1")  #  ping connection
# 29     except (sqlite3.ProgrammingError, RuntimeError):
# 30         _CONN = sqlite3.connect(DB_PATH, check_same_thread=False)
# 31     return _CONN
# 32 
# 33 
# 34 def close_connection() -> None:
# 35     global _CONN
# 36     if _CONN:
# 37         _CONN.close()
# 38         _CONN = None
# 39 
# 40 
# 41 def init_db(conn: sqlite3.Connection | None = None) -> sqlite3.Connection:
# 42     if conn is None:
# 43         conn = get_connection()
# 44     cur = conn.cursor()
# 45     cur.execute(
# 46         """CREATE TABLE IF NOT EXISTS state_log (
# 47             experiment_id TEXT,
# 48             system TEXT,
# 49             round INTEGER,
# 50             state TEXT,
# 51             action TEXT,
# 52             score REAL,
# 53             details TEXT,
# 54             timestamp TEXT
# 55         )"""
# 56     )
# 57     cur.execute(
# 58         """CREATE TABLE IF NOT EXISTS state_transition_log (
# 59             experiment_id TEXT,
# 60             round INTEGER,
# 61             from_state TEXT,
# 62             to_state TEXT,
# 63             reason TEXT,
# 64             timestamp TEXT
# 65         )"""
# 66     )
# 67     cur.execute(
# 68         """CREATE TABLE IF NOT EXISTS prompt_log (
# 69             experiment_id TEXT,
# 70             round INTEGER,
# 71             system TEXT,
# 72             agent_id TEXT,
# 73             agent_role TEXT,
# 74             agent_engine TEXT,
# 75             symbol TEXT,
# 76             prompt TEXT,
# 77             response TEXT,
# 78             attempt_number INTEGER,
# 79             agent_action_outcome TEXT,
# 80             start TEXT,
# 81             stop TEXT
# 82         )"""
# 83     )
# 84     cur.execute(
# 85         """CREATE TABLE IF NOT EXISTS code_quality_log (
# 86             experiment_id TEXT,
# 87             round INTEGER,
# 88             symbol TEXT,
# 89             lines_of_code INTEGER,
# 90             cyclomatic_complexity REAL,
# 91             maintainability_index REAL,
# 92             lint_errors INTEGER,
# 93             timestamp TEXT
# 94         )"""
# 95     )
# 96     cur.execute(
# 97         """CREATE TABLE IF NOT EXISTS scoring_log (
# 98             experiment_id TEXT,
# 99             round INTEGER,
# 100             metric TEXT,
# 101             value REAL,
# 102             timestamp TEXT
# 103         )"""
# 104     )
# 105     cur.execute(
# 106         """CREATE TABLE IF NOT EXISTS error_log (
# 107             experiment_id TEXT,
# 108             round INTEGER,
# 109             error_type TEXT,
# 110             message TEXT,
# 111             file_path TEXT,
# 112             timestamp TEXT
# 113         )"""
# 114     )
# 115     conn.commit()
# 116     return conn
# 117 
# 118 
# 119 def insert_logs(conn: sqlite3.Connection, table: str, logs: Iterable[Any]) -> None:
# 120     logs = list(logs)
# 121     if not logs:
# 122         return
# 123     row = _serialize(logs[0])
# 124     cols = ",".join(row.keys())
# 125     placeholders = ",".join(["?"] * len(row))
# 126     values = [tuple(_serialize(log).values()) for log in logs]
# 127     conn.executemany(
# 128         f"INSERT INTO {table} ({cols}) VALUES ({placeholders})",
# 129         values,
# 130     )
# 131     conn.commit()


# utilities\file_management\__init__.py



# utilities\metadata\__init__.py



# utilities\metadata\footer\__init__.py



# utilities\metadata\logging\__init__.py

# 1 from .log_schemas import (
# 2     StateLog,
# 3     StateTransitionLog,
# 4     PromptLog,
# 5     CodeQualityLog,
# 6     ErrorLog,
# 7     ScoringLog,
# 8 )
# 9 
# 10 __all__ = [
# 11     "StateLog",
# 12     "StateTransitionLog",
# 13     "PromptLog",
# 14     "CodeQualityLog",
# 15     "ErrorLog",
# 16     "ScoringLog",
# 17 ]


# utilities\metadata\logging\log_schemas.py

# 1 from __future__ import annotations
# 2 
# 3 from dataclasses import dataclass, field
# 4 from datetime import datetime, timezone
# 5 from app.enums.agent_enums import AgentRole
# 6 from app.enums.scoring_enums import ScoringMetric
# 7 from app.enums.system_enums import SystemState, StateTransitionReason
# 8 
# 9 
# 10 @dataclass
# 11 class StateLog:
# 12     experiment_id: str
# 13     system: str
# 14     round: int
# 15     state: SystemState
# 16     action: str
# 17     score: float | None = None
# 18     details: str | None = None
# 19     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 20 
# 21 
# 22 @dataclass
# 23 class StateTransitionLog:
# 24     experiment_id: str
# 25     round: int
# 26     from_state: SystemState
# 27     to_state: SystemState
# 28     reason: StateTransitionReason
# 29     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 30 
# 31 
# 32 @dataclass
# 33 class PromptLog:
# 34     experiment_id: str
# 35     round: int
# 36     system: str
# 37     agent_id: str
# 38     agent_role: AgentRole
# 39     agent_engine: str | None = None
# 40     symbol: str | None = None
# 41     prompt: str | None = None
# 42     response: str | None = None
# 43     attempt_number: int = 0
# 44     agent_action_outcome: str | None = None
# 45     start: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 46     stop: datetime | None = None
# 47 
# 48 
# 49 @dataclass
# 50 class CodeQualityLog:
# 51     experiment_id: str
# 52     round: int
# 53     symbol: str
# 54     lines_of_code: int
# 55     cyclomatic_complexity: float
# 56     maintainability_index: float
# 57     lint_errors: int
# 58     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 59 
# 60 
# 61 @dataclass
# 62 class ErrorLog:
# 63     experiment_id: str
# 64     round: int
# 65     error_type: str
# 66     message: str
# 67     file_path: str | None = None
# 68     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 69 
# 70 
# 71 @dataclass
# 72 class ScoringLog:
# 73     experiment_id: str
# 74     round: int
# 75     metric: ScoringMetric
# 76     value: float
# 77     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 78 
# 79 
# 80 @dataclass
# 81 class ConversationLog:
# 82     experiment_id: str
# 83     round: int
# 84     agent_role: AgentRole
# 85     target: str
# 86     content: str
# 87     originating_agent: str
# 88     intervention: bool
# 89     intervention_type: str | None = None
# 90     intervention_reason: str | None = None
# 91     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 92 
# 93 
# 94 @dataclass
# 95 class ExperimentLog:
# 96     experiment_id: str
# 97     description: str
# 98     mode: str
# 99     variant: str
# 100     max_iterations: int
# 101     stop_threshold: float
# 102     model_engine: str
# 103     evaluator_name: str
# 104     evaluator_version: str
# 105     final_score: float
# 106     passed: bool
# 107     reason_for_stop: str
# 108     start: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 109     stop: datetime | None = None


# utilities\metadata\snapshots\__init__.py



# utilities\metrics.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from typing import Any, Dict, List
# 5 
# 6 logger = logging.getLogger(__name__)
# 7 
# 8 #  Unified list of metric names expected in experiment results
# 9 EVALUATION_METRICS = [
# 10     "bug_fix_success_rate",
# 11     "functional_correctness",
# 12     "avg_test_pass_rate",
# 13     "maintainability_index",
# 14     "cyclomatic_complexity",
# 15     "linting_compliance_rate",
# 16     "iterations_to_convergence",
# 17     "intervention_frequency",
# 18     "agent_role_success_rate",
# 19     "retry_success_rate",
# 20     "mediation_success_rate",
# 21 ]
# 22 
# 23 
# 24 def _get(obj: Any, key: str, default: Any = None) -> Any:
# 25     if isinstance(obj, dict):
# 26         return obj.get(key, default)
# 27     return getattr(obj, key, default)
# 28 
# 29 
# 30 def _avg(values: List[float]) -> float:
# 31     return sum(values) / len(values) if values else 0.0
# 32 
# 33 
# 34 def compute_metrics(
# 35     evaluation_logs: List[Any],
# 36     code_quality_logs: List[Any],
# 37     conversation_logs: List[Any],
# 38     prompt_logs: List[Any],
# 39     state_logs: List[Any],
# 40 ) -> Dict[str, float]:
# 41     """Compute experiment metrics from structured logs."""
# 42 
# 43     logger.debug(
# 44         "Computing metrics: evaluations=%d quality=%d conversation=%d prompts=%d states=%d",
# 45         len(evaluation_logs),
# 46         len(code_quality_logs),
# 47         len(conversation_logs),
# 48         len(prompt_logs),
# 49         len(state_logs),
# 50     )
# 51 
# 52     metrics: Dict[str, float] = {}
# 53 
# 54     bug_fixes = [_get(log, "bug_fixed", False) for log in evaluation_logs]
# 55     metrics["bug_fix_success_rate"] = _avg([1.0 for bug in bug_fixes if bug])
# 56 
# 57     func_correct = [_get(log, "all_tests_passed", False) for log in evaluation_logs]
# 58     metrics["functional_correctness"] = _avg([1.0 for passes in func_correct if passes])
# 59 
# 60     pass_rates = []
# 61     for log in evaluation_logs:
# 62         total = _get(log, "tests_total", 0)
# 63         passed = _get(log, "tests_passed", 0)
# 64         if total:
# 65             pass_rates.append(passed / total)
# 66     metrics["avg_test_pass_rate"] = _avg(pass_rates)
# 67 
# 68     mi_values = [_get(log, "maintainability_index", 0.0) for log in code_quality_logs]
# 69     metrics["maintainability_index"] = _avg(mi_values)
# 70 
# 71     cc_values = [_get(log, "cyclomatic_complexity", 0.0) for log in code_quality_logs]
# 72     metrics["cyclomatic_complexity"] = _avg(cc_values)
# 73 
# 74     lint_compliance = [
# 75         1.0 if _get(log, "lint_errors", 1) == 0 else 0.0 for log in code_quality_logs
# 76     ]
# 77     metrics["linting_compliance_rate"] = _avg(lint_compliance)
# 78 
# 79     rounds = [_get(log, "round", 0) for log in state_logs]
# 80     metrics["iterations_to_convergence"] = max(rounds) + 1 if rounds else 0.0
# 81 
# 82     interventions = [
# 83         1.0 for log in conversation_logs if _get(log, "intervention", False)
# 84     ]
# 85     metrics["intervention_frequency"] = _avg(interventions)
# 86 
# 87     outcomes = [_get(log, "agent_action_outcome", None) for log in prompt_logs]
# 88     successes = [1.0 for o in outcomes if o == "success"]
# 89     metrics["agent_role_success_rate"] = _avg(successes)
# 90 
# 91     retry_logs = [log for log in prompt_logs if _get(log, "attempt_number", 0) > 0]
# 92     retry_successes = [
# 93         1.0
# 94         for log in retry_logs
# 95         if _get(log, "agent_action_outcome", None) == "success"
# 96     ]
# 97     metrics["retry_success_rate"] = _avg(retry_successes)
# 98 
# 99     mediation_logs = [
# 100         log
# 101         for log in conversation_logs
# 102         if _get(log, "intervention_type", None) == "mediation"
# 103     ]
# 104     mediation_success = [
# 105         1.0 for log in mediation_logs if _get(log, "intervention", False)
# 106     ]
# 107     metrics["mediation_success_rate"] = _avg(mediation_success)
# 108 
# 109     logger.info("Metrics computed: %s", metrics)
# 110     return metrics


# utilities\pydantic_compat.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import TYPE_CHECKING, Callable
# 4 
# 5 try:  #  pragma: no cover - pydantic v1 compatibility
# 6     from pydantic import BaseModel
# 7 
# 8     if not hasattr(BaseModel, "model_dump"):
# 9         from typing import Callable
# 10 
# 11         def _model_dump(self: BaseModel, **kwargs) -> dict:
# 12             return BaseModel.dict(self, **kwargs)
# 13 
# 14         setattr(BaseModel, "model_dump", _model_dump)  #  type: ignore[attr-defined]
# 15 except Exception:  #  pragma: no cover - ignore if pydantic v2
# 16     pass
# 17 
# 18 if TYPE_CHECKING:
# 19     from pydantic import field_validator, model_validator  #  type: ignore
# 20 else:  #  pragma: no cover - runtime fallback for pydantic v1
# 21     try:
# 22         from pydantic import field_validator, model_validator  #  type: ignore
# 23     except ImportError:  #  pragma: no cover - pydantic v1
# 24         from pydantic import validator, root_validator
# 25 
# 26         def field_validator(*fields: str, **kwargs) -> Callable:
# 27             kwargs.pop("mode", None)
# 28             return validator(*fields, **kwargs)
# 29 
# 30         def model_validator(*fields, **kwargs) -> Callable:
# 31             mode = kwargs.pop("mode", "after")
# 32 
# 33             def decorator(func: Callable) -> Callable:
# 34                 if mode == "after":
# 35 
# 36                     def wrapper(cls, values):
# 37                         obj = cls.construct(**values)
# 38                         func(obj)
# 39                         return obj.dict()
# 40 
# 41                     return root_validator(pre=False, allow_reuse=True)(wrapper)
# 42 
# 43                 def wrapper(cls, values):
# 44                     return func(values)
# 45 
# 46                 return root_validator(pre=True, allow_reuse=True)(wrapper)
# 47 
# 48             return decorator


# utilities\schema\__init__.py

# 1 from .create_schema import initialize_database, create_tables, load_seed_data
# 2 
# 3 __all__ = ["initialize_database", "create_tables", "load_seed_data"]


# utilities\schema\create_schema.py

# 1 from __future__ import annotations
# 2 
# 3 import json
# 4 import sqlite3
# 5 from pathlib import Path
# 6 from typing import Type, Any, Union, get_origin, get_args
# 7 from enum import Enum
# 8 
# 9 from app.schemas import (
# 10     AgentEngine,
# 11     AgentPrompt,
# 12     SystemPrompt,
# 13     ContextProvider,
# 14     ToolingProvider,
# 15     FilePath,
# 16     AgentConfig,
# 17     PromptGenerator,
# 18     ScoringProvider,
# 19     StateManager,
# 20     SystemConfig,
# 21     ExperimentConfig,
# 22     Series,
# 23 )
# 24 from app.utilities import db
# 25 
# 26 SCHEMAS = {
# 27     "agent_engine": AgentEngine,
# 28     "agent_prompt": AgentPrompt,
# 29     "system_prompt": SystemPrompt,
# 30     "context_provider": ContextProvider,
# 31     "tooling_provider": ToolingProvider,
# 32     "file_path": FilePath,
# 33     "agent_config": AgentConfig,
# 34     "prompt_generator": PromptGenerator,
# 35     "scoring_provider": ScoringProvider,
# 36     "state_manager": StateManager,
# 37     "system_config": SystemConfig,
# 38     "experiment_config": ExperimentConfig,
# 39     "series": Series,
# 40 }
# 41 
# 42 _TYPE_MAP = {
# 43     int: "INTEGER",
# 44     str: "TEXT",
# 45     float: "REAL",
# 46     Path: "TEXT",
# 47 }
# 48 
# 49 
# 50 def _sqlite_type(py_type: Type) -> str:
# 51     origin = get_origin(py_type)
# 52     if origin is Union:
# 53         py_type = get_args(py_type)[0]
# 54     return _TYPE_MAP.get(py_type, "TEXT")
# 55 
# 56 
# 57 def _is_optional(annotation: Any) -> bool:
# 58     return get_origin(annotation) is Union and type(None) in get_args(annotation)
# 59 
# 60 
# 61 def create_tables(conn: sqlite3.Connection) -> None:
# 62     cur = conn.cursor()
# 63     for table_name, model_cls in SCHEMAS.items():
# 64         fields = model_cls.__annotations__
# 65         columns = []
# 66         for name, annotation in fields.items():
# 67             col_type = _sqlite_type(annotation)
# 68             if name == "id" and _is_optional(annotation):
# 69                 columns.append(f"{name} INTEGER PRIMARY KEY")
# 70             else:
# 71                 columns.append(f"{name} {col_type}")
# 72         col_sql = ", ".join(columns)
# 73         cur.execute(f"CREATE TABLE IF NOT EXISTS {table_name} ({col_sql})")
# 74     conn.commit()
# 75 
# 76 
# 77 def load_seed_data(
# 78     conn: sqlite3.Connection, seed_dir: Path | str = "experiments/config/seed"
# 79 ) -> None:
# 80     seed_path = Path(seed_dir)
# 81     if not seed_path.exists():
# 82         return
# 83     cur = conn.cursor()
# 84     for file in seed_path.glob("*.json"):
# 85         table_name = file.stem
# 86         model = SCHEMAS.get(table_name)
# 87         if model is None:
# 88             continue
# 89         entries = json.loads(file.read_text())
# 90         if isinstance(entries, dict):
# 91             entries = [entries]
# 92         for entry in entries:
# 93             obj = model(**entry)
# 94             data = obj.model_dump()
# 95             #  Convert enums and paths to strings
# 96             data = {
# 97                 k: str(v) if isinstance(v, (Path, Enum)) else v for k, v in data.items()
# 98             }
# 99             cols = ",".join(data.keys())
# 100             placeholders = ",".join(["?"] * len(data))
# 101             cur.execute(
# 102                 f"INSERT INTO {table_name} ({cols}) VALUES ({placeholders})",
# 103                 list(data.values()),
# 104             )
# 105     conn.commit()
# 106 
# 107 
# 108 def initialize_database(reset: bool = False) -> sqlite3.Connection:
# 109     db_path = Path("experiments/codecritic.sqlite3")
# 110 
# 111     if reset and db_path.exists():
# 112         try:
# 113             import sqlite3
# 114 
# 115             sqlite3.connect(str(db_path)).close()
# 116         except Exception:
# 117             pass
# 118         import gc
# 119 
# 120         gc.collect()
# 121         db_path.unlink()
# 122 
# 123     conn = db.get_connection()
# 124     create_tables(conn)
# 125     load_seed_data(conn)
# 126     return conn
# 127 
# 128 
# 129 if __name__ == "__main__":
# 130     initialize_database()


# utilities\snapshots\__init__.py



# utilities\snapshots\snapshot_writer.py

# 1 from __future__ import annotations
# 2 
# 3 from datetime import datetime, timezone
# 4 from pathlib import Path
# 5 
# 6 from app.enums.agent_enums import AgentRole
# 7 
# 8 
# 9 class SnapshotWriter:
# 10     """Write before/after file snapshots for experiment traceability."""
# 11 
# 12     def __init__(self, root: str | Path = "experiments/artifacts/snapshots") -> None:
# 13         self.root = Path(root)
# 14         self.root.mkdir(parents=True, exist_ok=True)
# 15 
# 16     def write_snapshot(
# 17         self,
# 18         *,
# 19         experiment_id: str,
# 20         round: int,
# 21         file_path: str | Path,
# 22         before: str,
# 23         after: str,
# 24         symbol: str,
# 25         agent_role: AgentRole,
# 26     ) -> None:
# 27         if before == after:
# 28             return
# 29         ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%S%fZ")
# 30         dir_path = self.root / experiment_id / str(round)
# 31         dir_path.mkdir(parents=True, exist_ok=True)
# 32         base = Path(file_path).name
# 33         before_file = dir_path / f"{base}.{ts}.before"
# 34         after_file = dir_path / f"{base}.{ts}.after"
# 35         meta_file = dir_path / f"{base}.{ts}.meta"
# 36         before_file.write_text(before, encoding="utf-8")
# 37         after_file.write_text(after, encoding="utf-8")
# 38         meta_file.write_text(
# 39             f"symbol:{symbol}\nrole:{agent_role.value}\nexperiment:{experiment_id}\ntimestamp:{ts}\n",
# 40             encoding="utf-8",
# 41         )


# utilities\tools\__init__.py



# utilities\tools\black_runner.py

# 1 #  Placeholder for black runner tool


# utilities\tools\doc_formatter.py

# 1 #  Placeholder for documentation formatter


# utilities\tools\mypy_runner.py

# 1 #  Placeholder for mypy runner


# utilities\tools\radon_runner.py

# 1 import sys
# 2 import json
# 3 from radon.complexity import cc_visit
# 4 from pathlib import Path
# 5 
# 6 
# 7 def analyze_file(filepath):
# 8     with open(filepath, "r", encoding="utf-8") as file:
# 9         code = file.read()
# 10     complexity = cc_visit(code)
# 11     result = [
# 12         {"name": c.name, "complexity": c.complexity, "lineno": c.lineno}
# 13         for c in complexity
# 14     ]
# 15     return result
# 16 
# 17 
# 18 if __name__ == "__main__":
# 19     try:
# 20         if len(sys.argv) != 2:
# 21             raise ValueError("Provide exactly one file path.")
# 22         file_path = Path(sys.argv[1])
# 23         if not file_path.exists():
# 24             raise FileNotFoundError("File does not exist.")
# 25         analysis_result = analyze_file(str(file_path))
# 26         print(json.dumps({"result": analysis_result}, ensure_ascii=False))
# 27     except Exception as e:
# 28         print(json.dumps({"error": str(e)}, ensure_ascii=False))
# 29         sys.exit(1)


# utilities\tools\ruff_runner.py

# 1 #  Placeholder for ruff runner


# utilities\tools\sonarcloud_runner.py

# 1 #  Placeholder for sonarcloud runner



# === NON-PYTHON FILES ===

