
# === PYTHON FILES ===

# __init__.py



# abstract_classes\__init__.py



# abstract_classes\agent_base.py

# 1 from __future__ import annotations
# 2 
# 3 from abc import ABC, abstractmethod
# 4 
# 5 from ..factories.logging_provider import LoggingMixin, LoggingProvider
# 6 
# 7 
# 8 class AgentBase(LoggingMixin, ABC):
# 9     """Base class for executing agent logic."""
# 10 
# 11     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 12         super().__init__(logger)
# 13 
# 14     def run(self, *args, **kwargs) -> None:
# 15         self._log.debug("Agent run start")
# 16         self._run_agent_logic(*args, **kwargs)
# 17         self._log.debug("Agent run end")
# 18 
# 19     @abstractmethod
# 20     def _run_agent_logic(self, *args, **kwargs) -> None:
# 21         """Execute agent specific logic."""
# 22         raise NotImplementedError


# abstract_classes\context_provider_base.py

# 1 from __future__ import annotations
# 2 
# 3 from abc import ABC, abstractmethod
# 4 
# 5 from ..factories.logging_provider import LoggingMixin, LoggingProvider
# 6 
# 7 
# 8 class ContextProviderBase(LoggingMixin, ABC):
# 9     """Base class for providing context from symbol graphs."""
# 10 
# 11     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 12         super().__init__(logger)
# 13 
# 14     def get_context(self, *args, **kwargs):
# 15         self._log.debug("Context retrieval start")
# 16         context = self._get_context(*args, **kwargs)
# 17         self._log.debug("Context retrieval end")
# 18         return context
# 19 
# 20     @abstractmethod
# 21     def _get_context(self, *args, **kwargs):
# 22         """Return context information."""
# 23         raise NotImplementedError


# abstract_classes\experiment.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Any, Dict, List
# 4 
# 5 from ..factories.scoring_provider import ScoringProviderFactory
# 6 from ..factories.system_manager import SystemManagerFactory
# 7 from ..factories.experiment_config_provider import ExperimentConfigProvider
# 8 from ..factories.logging_provider import LoggingMixin, LoggingProvider, ScoringLog
# 9 from ..utilities.metrics import EVALUATION_METRICS
# 10 
# 11 
# 12 class Experiment(LoggingMixin):
# 13     """Base experiment handling execution and scoring."""
# 14 
# 15     def __init__(self, config_id: int, logger: LoggingProvider | None = None) -> None:
# 16         super().__init__(logger)
# 17         self.config_id = config_id
# 18         self.config: Dict[str, Any] | None = None
# 19         self.scoring_model_id = "dummy"
# 20 
# 21         self.evaluation_logs: List[Any] = []
# 22         self.code_quality_logs: List[Any] = []
# 23         self.conversation_logs: List[Any] = []
# 24         self.prompt_logs: List[Any] = []
# 25         self.state_logs: List[Any] = []
# 26         self.metrics: Dict[str, float] | None = None
# 27 
# 28     def run(self, *args, **kwargs) -> Dict[str, float]:
# 29         self._log.debug("Experiment run start")
# 30 
# 31         #   load configuration
# 32         self.config = ExperimentConfigProvider.load(self.config_id)
# 33         self.scoring_model_id = self.config.get("scoring_model_id", "dummy")
# 34         system_manager_id = self.config.get("system_manager_id", "dummy")
# 35 
# 36         #   run system manager *with* our injected logger
# 37         manager = SystemManagerFactory.create(
# 38             system_manager_id,
# 39             logger=self.logger,
# 40         )
# 41         manager.run()
# 42 
# 43         #   collect logs from manager
# 44         self.evaluation_logs = getattr(manager, "evaluation_logs", [])
# 45         self.code_quality_logs = getattr(manager, "code_quality_logs", [])
# 46         self.conversation_logs = getattr(manager, "conversation_logs", [])
# 47         self.prompt_logs = getattr(manager, "prompt_logs", [])
# 48         self.state_logs = getattr(manager, "state_logs", [])
# 49 
# 50         logs = {
# 51             "evaluation": self.evaluation_logs,
# 52             "code_quality": self.code_quality_logs,
# 53             "conversation": self.conversation_logs,
# 54             "prompt": self.prompt_logs,
# 55             "state": self.state_logs,
# 56         }
# 57 
# 58         scoring_provider = ScoringProviderFactory.create(
# 59             self.scoring_model_id,
# 60             logger=self.logger,
# 61         )
# 62         metrics = scoring_provider.score(logs)
# 63         for key in EVALUATION_METRICS:
# 64             metrics.setdefault(key, 0.0)
# 65         self._log.info("Experiment metrics: %s", metrics)
# 66 
# 67         for k, v in metrics.items():
# 68             self.log_scoring(
# 69                 ScoringLog(
# 70                     experiment_id=str(self.config_id),
# 71                     round=0,
# 72                     metric=k,
# 73                     value=v,
# 74                 )
# 75             )
# 76 
# 77         self.metrics = metrics
# 78         self._log.debug("Experiment run end")
# 79         return metrics
# 80 
# 81     def _run_experiment_logic(self, *args, **kwargs) -> None:
# 82         """Override to implement experiment steps."""
# 83         raise NotImplementedError


# abstract_classes\prompt_generator_base.py

# 1 from __future__ import annotations
# 2 
# 3 from abc import ABC, abstractmethod
# 4 
# 5 from app.utilities.metadata.logging.log_schemas import ErrorLog
# 6 
# 7 from ..factories.logging_provider import LoggingMixin, LoggingProvider
# 8 
# 9 import json
# 10 from pathlib import Path
# 11 from app.factories.logging_provider import LogType
# 12 
# 13 class PromptGeneratorBase(LoggingMixin, ABC):
# 14     """Base class for generating prompts."""
# 15 
# 16     def __init__(self, context_provider, logger: LoggingProvider | None = None) -> None:
# 17         super().__init__(logger)
# 18         self.context_provider = context_provider
# 19 
# 20     def generate_prompt(self, agent_config, system_config, experiment_id, round):
# 21         self._log.debug("Prompt generation start")
# 22         error_message = None
# 23         success = False
# 24         generated_prompt = ""
# 25 
# 26         try:
# 27             generated_prompt = self._generate_prompt(agent_config, system_config)
# 28             success = True
# 29             return generated_prompt
# 30         except ValueError as ve:
# 31             #  Expected operational errors (like invalid configs)
# 32             error_message = str(ve)
# 33             self._log.warning("Operational prompt generation error: %s", error_message)
# 34         except Exception as e:
# 35             #  Unexpected or critical errors
# 36             error_message = str(e)
# 37             error_log = {
# 38                 "experiment_id": experiment_id,
# 39                 "round": round,
# 40                 "error_type": type(e).__name__,
# 41                 "message": error_message,
# 42                 "file_path": str(Path(__file__).relative_to(Path.cwd())),
# 43             }
# 44             self.logger.write(LogType.ERROR, error_log)
# 45             self._log.error("Critical prompt generation error logged: %s", error_message)
# 46             raise  #  re-raise critical errors explicitly
# 47         finally:
# 48             #  Always log prompt generation attempt, including operational errors
# 49             prompt_log = {
# 50                 "experiment_id": experiment_id,
# 51                 "round": round,
# 52                 "generator_name": self.__class__.__name__,
# 53                 "context_provider_name": self.context_provider.__class__.__name__,
# 54                 "agent_config": json.dumps(agent_config),
# 55                 "system_config": json.dumps(system_config),
# 56                 "generated_prompt": generated_prompt if success else "",
# 57                 "success": success,
# 58                 "error_message": error_message if not success else None
# 59             }
# 60             self.logger.write(LogType.PROMPT_GENERATION, prompt_log)
# 61             self._log.debug("Prompt generation logged")
# 62             
# 63     @abstractmethod
# 64     def _generate_prompt(self, *args, **kwargs) -> str:
# 65         """Return a generated prompt."""
# 66         raise NotImplementedError


# abstract_classes\scoring_model_base.py

# 1 from __future__ import annotations
# 2 
# 3 from abc import ABC, abstractmethod
# 4 from typing import Dict
# 5 
# 6 from ..factories.logging_provider import LoggingMixin, LoggingProvider, ScoringLog
# 7 from ..enums.scoring_enums import ScoringMetric
# 8 
# 9 
# 10 class ScoringModelBase(LoggingMixin, ABC):
# 11     """Base class for more granular scoring models."""
# 12 
# 13     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 14         super().__init__(logger)
# 15 
# 16     def score(self, experiment_id: str, *args, **kwargs) -> Dict[str, float]:
# 17         self._log.debug("Scoring start")
# 18         result = self._score(experiment_id, *args, **kwargs)
# 19         self._log.debug("Scoring end")
# 20         return result
# 21 
# 22     @abstractmethod
# 23     def _score(self, experiment_id: str, *args, **kwargs) -> Dict[str, float]:
# 24         """Compute evaluation metrics."""
# 25         raise NotImplementedError
# 26 
# 27     def _log_metric(
# 28         self, experiment_id: str, metric: ScoringMetric, value: float
# 29     ) -> None:
# 30         self.log_scoring(
# 31             ScoringLog(
# 32                 experiment_id=experiment_id,
# 33                 round=0,
# 34                 metric=metric,
# 35                 value=value,
# 36             )
# 37         )


# abstract_classes\scoring_provider_base.py

# 1 from __future__ import annotations
# 2 
# 3 from abc import ABC, abstractmethod
# 4 
# 5 from ..factories.logging_provider import LoggingMixin, LoggingProvider
# 6 
# 7 
# 8 class ScoringProviderBase(LoggingMixin, ABC):
# 9     """Base class for computing evaluation metrics."""
# 10 
# 11     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 12         super().__init__(logger)
# 13 
# 14     def score(self, *args, **kwargs):
# 15         self._log.debug("Scoring start")
# 16         result = self._score(*args, **kwargs)
# 17         self._log.debug("Scoring end")
# 18         return result
# 19 
# 20     @abstractmethod
# 21     def _score(self, *args, **kwargs):
# 22         """Compute evaluation metrics."""
# 23         raise NotImplementedError


# abstract_classes\state_manager_base.py

# 1 from __future__ import annotations
# 2 
# 3 from abc import ABC, abstractmethod
# 4 
# 5 from ..factories.logging_provider import LoggingMixin, LoggingProvider
# 6 
# 7 
# 8 class StateManagerBase(LoggingMixin, ABC):
# 9     """Base class for managing state-level execution."""
# 10 
# 11     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 12         super().__init__(logger)
# 13 
# 14     def run(self, *args, **kwargs) -> None:
# 15         self._log.debug("StateManager run start")
# 16         self._run_state_logic(*args, **kwargs)
# 17         self._log.debug("StateManager run end")
# 18 
# 19     @abstractmethod
# 20     def _run_state_logic(self, *args, **kwargs) -> None:
# 21         """Execute state specific logic."""
# 22         raise NotImplementedError


# abstract_classes\system_manager_base.py

# 1 from __future__ import annotations
# 2 
# 3 from abc import ABC, abstractmethod
# 4 
# 5 from app.factories.logging_provider import LoggingMixin, LoggingProvider
# 6 
# 7 
# 8 class SystemManagerBase(LoggingMixin, ABC):
# 9     """Base class for coordinating high level system logic."""
# 10 
# 11     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 12         super().__init__(logger)
# 13 
# 14     def run(self, *args, **kwargs) -> None:
# 15         self._log.debug("SystemManager run start")
# 16         self._run_system_logic(*args, **kwargs)
# 17         self._log.debug("SystemManager run end")
# 18 
# 19     @abstractmethod
# 20     def _run_system_logic(self, *args, **kwargs) -> None:
# 21         """Execute system-specific logic and state transitions."""
# 22         raise NotImplementedError


# abstract_classes\tool_provider_base.py

# 1 from __future__ import annotations
# 2 
# 3 from abc import ABC, abstractmethod
# 4 
# 5 from ..factories.logging_provider import LoggingMixin, LoggingProvider
# 6 
# 7 
# 8 class ToolProviderBase(LoggingMixin, ABC):
# 9     """Base class for running external tools."""
# 10 
# 11     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 12         super().__init__(logger)
# 13 
# 14     def run(self, *args, **kwargs):
# 15         self._log.debug("Tool run start")
# 16         result = self._run(*args, **kwargs)
# 17         self._log.debug("Tool run end")
# 18         return result
# 19 
# 20     @abstractmethod
# 21     def _run(self, *args, **kwargs):
# 22         """Run tool-specific logic."""
# 23         raise NotImplementedError


# bootstrap.py

# 1 from __future__ import annotations
# 2 
# 3 from importlib import import_module
# 4 from pathlib import Path
# 5 from typing import Type
# 6 
# 7 from .factories.agent import AgentFactory
# 8 from .factories.prompt_manager import PromptGeneratorFactory
# 9 from .registries.context_providers import CONTEXT_PROVIDER_REGISTRY
# 10 from .factories.tool_provider import ToolProviderFactory
# 11 from .factories.scoring_provider import ScoringProviderFactory
# 12 from .factories.state_manager import StateManagerFactory
# 13 from .factories.experiment_config_provider import ExperimentConfigProvider
# 14 from .abstract_classes.agent_base import AgentBase
# 15 from .abstract_classes.prompt_generator_base import PromptGeneratorBase
# 16 from .abstract_classes.tool_provider_base import ToolProviderBase
# 17 from .abstract_classes.scoring_provider_base import ScoringProviderBase
# 18 from .abstract_classes.state_manager_base import StateManagerBase
# 19 
# 20 from .utilities.schema import initialize_database
# 21 
# 22 
# 23 def _import_class(path: str | Path, base_cls: Type) -> Type:
# 24     mod_path = str(path).replace("\\", "/")
# 25     if mod_path.endswith(".py"):
# 26         mod_path = mod_path[:-3]
# 27     mod_path = mod_path.replace("/", ".")
# 28     module = import_module(f"app.extensions.{mod_path}")
# 29     for attr in module.__dict__.values():
# 30         if (
# 31             isinstance(attr, type)
# 32             and issubclass(attr, base_cls)
# 33             and attr is not base_cls
# 34         ):
# 35             return attr
# 36     raise ImportError(f"No subclass of {base_cls.__name__} found in {module.__name__}")
# 37 
# 38 
# 39 def seed_registries(reset_db: bool = False) -> None:
# 40     conn = initialize_database(reset=reset_db)
# 41     cur = conn.cursor()
# 42 
# 43     #  Tool providers
# 44     for row in cur.execute("SELECT name, artifact_path FROM tooling_provider"):
# 45         name, artifact = row
# 46         path = Path(artifact)
# 47         cls = _import_class(path.as_posix(), ToolProviderBase)
# 48         try:
# 49             ToolProviderFactory.register(name, cls)
# 50         except KeyError:
# 51             pass
# 52 
# 53     #  Context providers
# 54     for row in cur.execute("SELECT name FROM context_provider"):
# 55         (name,) = row
# 56         #  Assume providers are already registered via extensions
# 57         if CONTEXT_PROVIDER_REGISTRY.get(name) is not None:
# 58             continue
# 59 
# 60     #  Prompt generators
# 61     for row in cur.execute("SELECT name, artifact_path FROM prompt_generator"):
# 62         name, artifact = row
# 63         path = Path(artifact)
# 64         cls = _import_class(path.as_posix(), PromptGeneratorBase)
# 65         try:
# 66             PromptGeneratorFactory.register(name, cls)
# 67         except KeyError:
# 68             pass
# 69 
# 70     #  Agents
# 71     for row in cur.execute("SELECT name, artifact_path FROM agent_config"):
# 72         name, artifact = row
# 73         path = Path(artifact)
# 74         cls = _import_class(path.as_posix(), AgentBase)
# 75         try:
# 76             AgentFactory.register(name, cls)
# 77         except KeyError:
# 78             pass
# 79 
# 80     #  Scoring providers
# 81     for row in cur.execute("SELECT name, artifact_path FROM scoring_provider"):
# 82         name, artifact = row
# 83         path = Path(artifact)
# 84         cls = _import_class(path.as_posix(), ScoringProviderBase)
# 85         try:
# 86             ScoringProviderFactory.register(name, cls)
# 87         except KeyError:
# 88             pass
# 89 
# 90     #  State managers
# 91     for row in cur.execute("SELECT name, artifact_path FROM state_manager"):
# 92         name, artifact = row
# 93         path = Path(artifact)
# 94         cls = _import_class(path.as_posix(), StateManagerBase)
# 95         try:
# 96             StateManagerFactory.register(name, cls)
# 97         except KeyError:
# 98             pass
# 99 
# 100     #  System managers are loaded via extensions; only register configs
# 101     for row in cur.execute(
# 102         "SELECT id, system_manager_id, scoring_model_id FROM experiment_config"
# 103     ):
# 104         config_id, system_manager_id, scoring_model_id = row
# 105         config = {
# 106             "system_manager_id": system_manager_id,
# 107             "scoring_model_id": scoring_model_id,
# 108         }
# 109         ExperimentConfigProvider.register(config_id, config)
# 110 
# 111     conn.close()
# 112 
# 113 
# 114 if __name__ == "__main__":
# 115     seed_registries(reset_db=True)


# enums\__init__.py



# enums\agent_enums.py

# 1 from __future__ import annotations
# 2 
# 3 from enum import Enum
# 4 
# 5 
# 6 class AgentRole(str, Enum):
# 7     GENERATOR = "generator"
# 8     DISCRIMINATOR = "discriminator"
# 9     MEDIATOR = "mediator"
# 10     PATCHER = "patcher"
# 11     EVALUATOR = "evaluator"
# 12     RECOMMENDER = "recommender"
# 13 
# 14 
# 15 class AgentState(Enum):
# 16     INIT = "init"
# 17     RUNNING = "running"
# 18     COMPLETE = "complete"


# enums\logging_enums.py

# 1 from enum import Enum
# 2 
# 3 
# 4 class LogType(str, Enum):
# 5     EXPERIMENT = "experiment"
# 6     STATE = "state"
# 7     STATE_TRANSITION = "state_transition"
# 8     PROMPT = "prompt"
# 9     CONVERSATION = "conversation"
# 10     SCORING = "scoring"
# 11     CODE_QUALITY = "code_quality"
# 12     ERROR = "error"
# 13     RECOMMENDATION = "recommendation"
# 14     FEEDBACK = "feedback"
# 15     PROMPT_GENERATION = "prompt_generation"


# enums\scoring_enums.py

# 1 from enum import Enum
# 2 
# 3 
# 4 class ScoringMetric(str, Enum):
# 5     FUNCTIONAL_CORRECTNESS = "functional_correctness"
# 6     MAINTAINABILITY_INDEX = "maintainability_index"
# 7     BUG_FIX_SUCCESS = "bug_fix_success_rate"
# 8     LINTING_COMPLIANCE = "linting_compliance"
# 9     AGENT_SUCCESS_RATE = "agent_role_success_rate"
# 10     RETRY_SUCCESS_RATE = "retry_success_rate"
# 11     COVERAGE = "coverage"
# 12     CYCLOMATIC_COMPLEXITY = "cyclomatic_complexity"
# 13     RECOMMENDATION_QUALITY = "recommendation_quality"


# enums\system_enums.py

# 1 from __future__ import annotations
# 2 
# 3 from enum import Enum
# 4 
# 5 
# 6 class SystemType(str, Enum):
# 7     #  Core Code Transformation
# 8     LINTING = "linting"
# 9     FORMATTING = "formatting"
# 10     DOCSTRING = "docstring"
# 11     TYPE_ANNOTATION = "type_annotation"
# 12     REFACTORING = "refactoring"
# 13     DEAD_CODE_REMOVAL = "dead_code_removal"
# 14     COMPLEXITY_REDUCTION = "complexity_reduction"
# 15 
# 16     #  Testing and Validation
# 17     UNIT_TESTING = "unit_testing"
# 18     EDGE_TESTING = "edge_testing"
# 19     INTEGRATION_TESTING = "integration_testing"
# 20     TEST_FIX_GENERATION = "test_fix_generation"
# 21     MOCK_GENERATION = "mock_generation"
# 22     STUB_EXTRACTION = "stub_extraction"
# 23 
# 24     #  Observability and Instrumentation
# 25     LOGGING_INJECTION = "logging_injection"
# 26     TRACE_ANNOTATION = "trace_annotation"
# 27     ERROR_HANDLING = "error_handling"
# 28 
# 29     #  Documentation and Explanation
# 30     INLINE_COMMENTING = "inline_commenting"
# 31     FUNCTION_SUMMARIZATION = "function_summarization"
# 32     FILE_SUMMARIZATION = "file_summarization"
# 33     CHANGE_SUMMARIZATION = "change_summarization"
# 34 
# 35     #  Review and Oversight
# 36     CODE_REVIEW = "code_review"
# 37     STATIC_ANALYSIS = "static_analysis"
# 38     REGRESSION_ANALYSIS = "regression_analysis"
# 39     MEDIATOR_RECONCILIATION = "mediator_reconciliation"
# 40     SCORING_EVALUATION = "scoring_evaluation"
# 41 
# 42     #  Planning and Organization
# 43     SYMBOL_GRAPH_GENERATION = "symbol_graph_generation"
# 44     DEPENDENCY_MAPPING = "dependency_mapping"
# 45     FILE_RESTRUCTURING = "file_restructuring"
# 46     FUNCTION_SPLITTING = "function_splitting"
# 47 
# 48     #  MLOps-Specific
# 49     DATA_VALIDATION = "data_validation"
# 50     METADATA_ANNOTATION = "metadata_annotation"
# 51     MODEL_DOC_GENERATION = "model_doc_generation"
# 52     PIPELINE_TESTING = "pipeline_testing"
# 53     TRAINING_CONFIG_AUDIT = "training_config_audit"
# 54     FEATURE_TRACEABILITY = "feature_traceability"
# 55 
# 56     #  Production Safety
# 57     SECURITY_CHECK = "security_check"
# 58     POLICY_COMPLIANCE = "policy_compliance"
# 59     LICENSE_CHECK = "license_check"
# 60 
# 61     #  Misc / Cross-cutting
# 62     PATCHING = "patching"
# 63     PROMPT_REFINEMENT = "prompt_refinement"
# 64     AGENT_RECOMMENDATION = "agent_recommendation"
# 65     SEMANTIC_DIFF = "semantic_diff"
# 66     CONTEXT_FILTERING = "context_filtering"
# 67 
# 68 
# 69 class SystemState(Enum):
# 70     """Finite state machine states for the CodeCritic system."""
# 71 
# 72     START = "start"
# 73     GENERATE = "generate"
# 74     DISCRIMINATE = "discriminate"
# 75     MEDIATE = "mediate"
# 76     PATCH = "patch"
# 77     EVALUATE = "evaluate"
# 78     END = "end"
# 79 
# 80 
# 81 class StateTransitionReason(str, Enum):
# 82     FIRST_ROUND = "first_round"
# 83     MAX_ITERATIONS_REACHED = "max_iterations_reached"
# 84     SCORE_THRESHOLD_MET = "score_threshold_met"
# 85     SCORE_STAGNATION = "score_stagnation"
# 86     AGENT_FAILURE = "agent_failure"
# 87     MEDIATOR_OVERRIDE = "mediator_override"
# 88     PATCH_RETRY = "patch_retry"
# 89     CUSTOM_RULE = "custom_rule"
# 90     END_REACHED = "end_reached"


# extensions\__init__.py



# extensions\agent_prompts\__init__.py

# 1 


# extensions\agents\__init__.py

# 1 from .dummy_agent import DummyAgent
# 2 from .generator_agent import GeneratorAgent
# 3 from .evaluator_agent import EvaluatorAgent
# 4 from .mediator_agent import MediatorAgent
# 5 from .patch_agent import PatchAgent
# 6 from .recommendation_agent import RecommendationAgent
# 7 from ...registries.agents import AGENT_REGISTRY
# 8 
# 9 AGENT_REGISTRY.register("dummy", DummyAgent)
# 10 AGENT_REGISTRY.register("generator", GeneratorAgent)
# 11 AGENT_REGISTRY.register("evaluator", EvaluatorAgent)
# 12 AGENT_REGISTRY.register("mediator", MediatorAgent)
# 13 AGENT_REGISTRY.register("patch", PatchAgent)
# 14 AGENT_REGISTRY.register("recommendation", RecommendationAgent)
# 15 
# 16 __all__ = [
# 17     "DummyAgent",
# 18     "GeneratorAgent",
# 19     "EvaluatorAgent",
# 20     "MediatorAgent",
# 21     "PatchAgent",
# 22     "RecommendationAgent",
# 23 ]


# extensions\agents\dummy_agent.py

# 1 from ...abstract_classes.agent_base import AgentBase
# 2 
# 3 
# 4 class DummyAgent(AgentBase):
# 5     def _run_agent_logic(self, *args, **kwargs) -> None:
# 6         self._log.info("Dummy agent logic executed")


# extensions\agents\evaluator_agent.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import List
# 5 from datetime import datetime, timezone
# 6 
# 7 from ...abstract_classes.agent_base import AgentBase
# 8 from ...enums.agent_enums import AgentRole
# 9 from ...factories.tool_provider import ToolProviderFactory
# 10 from ...factories.logging_provider import (
# 11     CodeQualityLog,
# 12     ErrorLog,
# 13     LoggingProvider,
# 14     FeedbackLog,
# 15 )
# 16 from ...utilities.snapshots.snapshot_writer import SnapshotWriter
# 17 
# 18 
# 19 def _analyze_radon(code: str) -> tuple[float, float]:
# 20     """Return cyclomatic complexity and maintainability index for the code."""
# 21     try:
# 22         from radon.complexity import cc_visit
# 23         from radon.metrics import mi_visit
# 24 
# 25         blocks = cc_visit(code)
# 26         cc = sum(b.complexity for b in blocks) / max(len(blocks), 1)
# 27         mi = mi_visit(code, True)
# 28         return float(cc), float(mi)
# 29     except Exception:
# 30         return 0.0, 0.0
# 31 
# 32 
# 33 class EvaluatorAgent(AgentBase):
# 34     """Run static analysis tools and log code quality metrics."""
# 35 
# 36     def __init__(
# 37         self,
# 38         target: str,
# 39         logger: LoggingProvider | None = None,
# 40         snapshot_writer: SnapshotWriter | None = None,
# 41     ) -> None:
# 42         super().__init__(logger)
# 43         self.target = target
# 44         self.snapshot_writer = snapshot_writer or SnapshotWriter()
# 45         self.mypy = ToolProviderFactory.create("mypy")
# 46         self.ruff = ToolProviderFactory.create("ruff")
# 47         self.black = ToolProviderFactory.create("black")
# 48         self.radon = ToolProviderFactory.create("radon")
# 49         self.quality_logs: List[CodeQualityLog] = []
# 50         self.error_logs: List[ErrorLog] = []
# 51 
# 52     def _run_agent_logic(self, *args, **kwargs) -> None:
# 53         feedback = kwargs.get("feedback")
# 54         if feedback:
# 55             for item in feedback:
# 56                 self.log_feedback(
# 57                     FeedbackLog(
# 58                         experiment_id="exp",
# 59                         round=0,
# 60                         source="evaluator",
# 61                         feedback=str(item),
# 62                     )
# 63                 )
# 64         before = Path(self.target).read_text(encoding="utf-8")
# 65         lines = len(before.splitlines())
# 66         complexity = 0.0
# 67         maintainability = 0.0
# 68         lint_return = 0
# 69 
# 70         try:
# 71             self.mypy.run(self.target)
# 72         except Exception as exc:  #  pragma: no cover - tool execution failure
# 73             self.error_logs.append(
# 74                 ErrorLog(
# 75                     experiment_id="exp",
# 76                     round=0,
# 77                     error_type=type(exc).__name__,
# 78                     message=str(exc),
# 79                     file_path=self.target,
# 80                 )
# 81             )
# 82 
# 83         try:
# 84             ruff_proc = self.ruff.run(self.target)
# 85             lint_return = ruff_proc.returncode
# 86         except Exception as exc:  #  pragma: no cover - tool execution failure
# 87             self.error_logs.append(
# 88                 ErrorLog(
# 89                     experiment_id="exp",
# 90                     round=0,
# 91                     error_type=type(exc).__name__,
# 92                     message=str(exc),
# 93                     file_path=self.target,
# 94                 )
# 95             )
# 96             lint_return = 1
# 97 
# 98         try:
# 99             self.black.run(self.target, check=True)
# 100         except Exception as exc:  #  pragma: no cover - tool execution failure
# 101             self.error_logs.append(
# 102                 ErrorLog(
# 103                     experiment_id="exp",
# 104                     round=0,
# 105                     error_type=type(exc).__name__,
# 106                     message=str(exc),
# 107                     file_path=self.target,
# 108                 )
# 109             )
# 110 
# 111         try:
# 112             #  Run radon CLI primarily for logging/debug purposes
# 113             self.radon.run(self.target)
# 114             complexity, maintainability = _analyze_radon(before)
# 115         except Exception as exc:
# 116             self._log.warning("Radon unavailable: %s", exc)
# 117             self.error_logs.append(
# 118                 ErrorLog(
# 119                     experiment_id="exp",
# 120                     round=0,
# 121                     error_type=type(exc).__name__,
# 122                     message=f"Radon analysis failed: {exc}",
# 123                     file_path=self.target,
# 124                     timestamp=datetime.now(timezone.utc),
# 125                 )
# 126             )
# 127 
# 128         log = CodeQualityLog(
# 129             experiment_id="exp",
# 130             round=0,
# 131             symbol=self.target,
# 132             lines_of_code=lines,
# 133             cyclomatic_complexity=complexity,
# 134             maintainability_index=maintainability,
# 135             lint_errors=0 if lint_return == 0 else 1,
# 136         )
# 137         self.quality_logs.append(log)
# 138         self.log_code_quality(log)
# 139         for err in self.error_logs:
# 140             self.log_error(err)
# 141 
# 142         after = Path(self.target).read_text(encoding="utf-8")
# 143         self.snapshot_writer.write_snapshot(
# 144             experiment_id="exp",
# 145             round=0,
# 146             file_path=self.target,
# 147             before=before,
# 148             after=after,
# 149             symbol=self.target,
# 150             agent_role=AgentRole.EVALUATOR,
# 151         )


# extensions\agents\generator_agent.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import List
# 4 from datetime import datetime, timezone
# 5 from pathlib import Path
# 6 
# 7 from ...abstract_classes.agent_base import AgentBase
# 8 from ...enums.agent_enums import AgentRole
# 9 from ...factories.tool_provider import ToolProviderFactory
# 10 from ...factories.logging_provider import (
# 11     ErrorLog,
# 12     PromptLog,
# 13     LoggingProvider,
# 14     FeedbackLog,
# 15 )
# 16 from ...utilities.snapshots.snapshot_writer import SnapshotWriter
# 17 
# 18 
# 19 class GeneratorAgent(AgentBase):
# 20     """Agent responsible for formatting code using black."""
# 21 
# 22     def __init__(
# 23         self,
# 24         target: str,
# 25         logger: LoggingProvider | None = None,
# 26         snapshot_writer: SnapshotWriter | None = None,
# 27     ) -> None:
# 28         super().__init__(logger)
# 29         self.target = target
# 30         self.formatter = ToolProviderFactory.create("black")
# 31         self.docformatter = ToolProviderFactory.create("docformatter")
# 32         self.prompt_logs: List[PromptLog] = []
# 33         self.error_logs: List[ErrorLog] = []
# 34         self.snapshot_writer = snapshot_writer or SnapshotWriter()
# 35 
# 36     def _run_agent_logic(self, *args, **kwargs) -> None:
# 37         feedback = kwargs.get("feedback")
# 38         if feedback:
# 39             for item in feedback:
# 40                 self.log_feedback(
# 41                     FeedbackLog(
# 42                         experiment_id="exp",
# 43                         round=0,
# 44                         source="generator",
# 45                         feedback=str(item),
# 46                     )
# 47                 )
# 48         before = Path(self.target).read_text(encoding="utf-8")
# 49         log = PromptLog(
# 50             experiment_id="exp",
# 51             round=0,
# 52             system="system",
# 53             agent_id="generator",
# 54             agent_role=AgentRole.GENERATOR,
# 55             symbol=self.target,
# 56             prompt="format code",
# 57             response=None,
# 58             attempt_number=0,
# 59             agent_action_outcome="started",
# 60         )
# 61         self.prompt_logs.append(log)
# 62         try:
# 63             self.formatter.run(self.target)
# 64             self.docformatter.run(self.target)
# 65             log.agent_action_outcome = "success"
# 66             self._log.info("Formatted %s", self.target)
# 67         except Exception as exc:
# 68             log.agent_action_outcome = "error"
# 69             err = ErrorLog(
# 70                 experiment_id="exp",
# 71                 round=0,
# 72                 error_type=type(exc).__name__,
# 73                 message=str(exc),
# 74                 file_path=self.target,
# 75             )
# 76             self.error_logs.append(err)
# 77             self._log.exception("Formatting failed for %s", self.target)
# 78         finally:
# 79             log.stop = datetime.now(timezone.utc)
# 80             after = Path(self.target).read_text(encoding="utf-8")
# 81             self.snapshot_writer.write_snapshot(
# 82                 experiment_id="exp",
# 83                 round=0,
# 84                 file_path=self.target,
# 85                 before=before,
# 86                 after=after,
# 87                 symbol=self.target,
# 88                 agent_role=AgentRole.GENERATOR,
# 89             )
# 90             self.log_prompt(log)
# 91             for err in self.error_logs:
# 92                 self.log_error(err)


# extensions\agents\mediator_agent.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import List, Any
# 4 
# 5 from ...abstract_classes.agent_base import AgentBase
# 6 from ...enums.agent_enums import AgentRole
# 7 from ...factories.logging_provider import (
# 8     ConversationLog,
# 9     ErrorLog,
# 10     LoggingProvider,
# 11     FeedbackLog,
# 12 )
# 13 from ...utilities.snapshots.snapshot_writer import SnapshotWriter
# 14 
# 15 
# 16 class MediatorAgent(AgentBase):
# 17     """Agent that reconciles discrepancies between generator and evaluator outputs."""
# 18 
# 19     def __init__(
# 20         self,
# 21         target: str = "generated.py",
# 22         logger: LoggingProvider | None = None,
# 23         snapshot_writer: SnapshotWriter | None = None,
# 24     ) -> None:
# 25         super().__init__(logger)
# 26         self.target = target
# 27         self.snapshot_writer = snapshot_writer or SnapshotWriter()
# 28         self.conversation_logs: List[ConversationLog] = []
# 29         self.error_logs: List[ErrorLog] = []
# 30 
# 31     def _run_agent_logic(self, *args, **kwargs) -> None:
# 32         feedback = kwargs.get("feedback")
# 33         if feedback:
# 34             for item in feedback:
# 35                 self.log_feedback(
# 36                     FeedbackLog(
# 37                         experiment_id="exp",
# 38                         round=0,
# 39                         source="mediator",
# 40                         feedback=str(item),
# 41                     )
# 42                 )
# 43         code: str | None = kwargs.get("generator_output")
# 44         metrics: dict[str, Any] | None = kwargs.get("evaluation_metrics")
# 45 
# 46         if code is None or metrics is None:
# 47             err = ErrorLog(
# 48                 experiment_id="exp",
# 49                 round=0,
# 50                 error_type="ValueError",
# 51                 message="Missing generator output or evaluation metrics",
# 52                 file_path=self.target,
# 53             )
# 54             self.error_logs.append(err)
# 55             self.log_error(err)
# 56             return
# 57 
# 58         lint_errors = int(metrics.get("lint_errors", 0))
# 59         maintainability = float(metrics.get("maintainability_index", 100.0))
# 60         complexity = float(metrics.get("cyclomatic_complexity", 0.0))
# 61 
# 62         recommendations: List[str] = []
# 63         if lint_errors > 0:
# 64             recommendations.append("Resolve lint errors")
# 65         if maintainability < 70:
# 66             recommendations.append("Increase maintainability")
# 67         if complexity > 10:
# 68             recommendations.append("Reduce complexity")
# 69 
# 70         rec_text = "; ".join(recommendations)
# 71         conv = ConversationLog(
# 72             experiment_id="exp",
# 73             round=0,
# 74             agent_role=AgentRole.MEDIATOR,
# 75             target=self.target,
# 76             content=rec_text,
# 77             originating_agent="mediator",
# 78             intervention=bool(recommendations),
# 79             intervention_type="mediation" if recommendations else None,
# 80             intervention_reason="evaluation discrepancy" if recommendations else None,
# 81         )
# 82         self.conversation_logs.append(conv)
# 83 
# 84         try:
# 85             self.log_conversation(conv)
# 86         except Exception as exc:  #  pragma: no cover - db write failure
# 87             err = ErrorLog(
# 88                 experiment_id="exp",
# 89                 round=0,
# 90                 error_type=type(exc).__name__,
# 91                 message=str(exc),
# 92                 file_path=self.target,
# 93             )
# 94             self.error_logs.append(err)
# 95             self.log_error(err)
# 96 
# 97         after = code
# 98         if rec_text:
# 99             after = code + f"\n#  Mediator Recommendation: {rec_text}"
# 100 
# 101         self.snapshot_writer.write_snapshot(
# 102             experiment_id="exp",
# 103             round=0,
# 104             file_path=self.target,
# 105             before=code,
# 106             after=after,
# 107             symbol=self.target,
# 108             agent_role=AgentRole.MEDIATOR,
# 109         )


# extensions\agents\patch_agent.py

# 1 from __future__ import annotations
# 2 
# 3 import json
# 4 from pathlib import Path
# 5 from typing import Any, List
# 6 
# 7 from ...abstract_classes.agent_base import AgentBase
# 8 from ...enums.agent_enums import AgentRole
# 9 from ...factories.logging_provider import (
# 10     CodeQualityLog,
# 11     ConversationLog,
# 12     ErrorLog,
# 13     LoggingProvider,
# 14     FeedbackLog,
# 15 )
# 16 from ...factories.tool_provider import ToolProviderFactory
# 17 from ...utilities.snapshots.snapshot_writer import SnapshotWriter
# 18 from .evaluator_agent import _analyze_radon
# 19 
# 20 
# 21 class PatchAgent(AgentBase):
# 22     """Apply mediator recommendations as code patches."""
# 23 
# 24     def __init__(
# 25         self,
# 26         target: str,
# 27         logger: LoggingProvider | None = None,
# 28         snapshot_writer: SnapshotWriter | None = None,
# 29     ) -> None:
# 30         super().__init__(logger)
# 31         self.target = target
# 32         self.snapshot_writer = snapshot_writer or SnapshotWriter()
# 33         self.black = ToolProviderFactory.create("black")
# 34         self.ruff = ToolProviderFactory.create("ruff")
# 35         self.mypy = ToolProviderFactory.create("mypy")
# 36         self.conversation_logs: List[ConversationLog] = []
# 37         self.error_logs: List[ErrorLog] = []
# 38         self.quality_logs: List[CodeQualityLog] = []
# 39 
# 40     def _apply_patch(self, code: str, op: dict[str, Any]) -> str:
# 41         if op.get("op") == "replace":
# 42             old = op.get("from", "")
# 43             new = op.get("to", "")
# 44             if old not in code:
# 45                 raise ValueError(f"pattern '{old}' not found")
# 46             return code.replace(old, new)
# 47         if op.get("op") == "append":
# 48             return code + op.get("text", "")
# 49         raise ValueError(f"unsupported op: {op.get('op')}")
# 50 
# 51     def _run_agent_logic(self, *args, **kwargs) -> None:  #  noqa: C901 - small
# 52         feedback = kwargs.get("feedback")
# 53         if feedback:
# 54             for item in feedback:
# 55                 self.log_feedback(
# 56                     FeedbackLog(
# 57                         experiment_id="exp",
# 58                         round=0,
# 59                         source="patch",
# 60                         feedback=str(item),
# 61                     )
# 62                 )
# 63 
# 64         recs: List[ConversationLog] | None = kwargs.get("recommendations")
# 65         if not recs:
# 66             return
# 67         path = Path(self.target)
# 68         before = path.read_text(encoding="utf-8")
# 69         after = before
# 70         for conv in recs:
# 71             self.conversation_logs.append(conv)
# 72             try:
# 73                 ops = json.loads(conv.content)
# 74             except json.JSONDecodeError as exc:
# 75                 err = ErrorLog(
# 76                     experiment_id="exp",
# 77                     round=0,
# 78                     error_type=type(exc).__name__,
# 79                     message=str(exc),
# 80                     file_path=self.target,
# 81                 )
# 82                 self.error_logs.append(err)
# 83                 self.log_error(err)
# 84                 continue
# 85             self.log_conversation(conv)
# 86             for op in ops:
# 87                 try:
# 88                     after = self._apply_patch(after, op)
# 89                 except Exception as exc:  #  patch failure
# 90                     err = ErrorLog(
# 91                         experiment_id="exp",
# 92                         round=0,
# 93                         error_type=type(exc).__name__,
# 94                         message=str(exc),
# 95                         file_path=self.target,
# 96                     )
# 97                     self.error_logs.append(err)
# 98                     self.log_error(err)
# 99         path.write_text(after, encoding="utf-8")
# 100         try:
# 101             self.black.run(self.target)
# 102             self.ruff.run(self.target)
# 103             self.mypy.run(self.target)
# 104         except Exception as exc:  #  pragma: no cover - tool failure
# 105             err = ErrorLog(
# 106                 experiment_id="exp",
# 107                 round=0,
# 108                 error_type=type(exc).__name__,
# 109                 message=str(exc),
# 110                 file_path=self.target,
# 111             )
# 112             self.error_logs.append(err)
# 113             self.log_error(err)
# 114         final = path.read_text(encoding="utf-8")
# 115         complexity, maintainability = _analyze_radon(final)
# 116         quality = CodeQualityLog(
# 117             experiment_id="exp",
# 118             round=0,
# 119             symbol=self.target,
# 120             lines_of_code=len(final.splitlines()),
# 121             cyclomatic_complexity=complexity,
# 122             maintainability_index=maintainability,
# 123             lint_errors=0,
# 124         )
# 125         self.quality_logs.append(quality)
# 126         self.log_code_quality(quality)
# 127         for err in self.error_logs:
# 128             self.log_error(err)
# 129         self.snapshot_writer.write_snapshot(
# 130             experiment_id="exp",
# 131             round=0,
# 132             file_path=self.target,
# 133             before=before,
# 134             after=final,
# 135             symbol=self.target,
# 136             agent_role=AgentRole.PATCHER,
# 137         )


# extensions\agents\recommendation_agent.py

# 1 from __future__ import annotations
# 2 
# 3 import json
# 4 from typing import Any, Dict, List
# 5 
# 6 from ...abstract_classes.agent_base import AgentBase
# 7 from ...enums.agent_enums import AgentRole
# 8 from ...abstract_classes.context_provider_base import ContextProviderBase
# 9 from ...factories.logging_provider import (
# 10     ConversationLog,
# 11     CodeQualityLog,
# 12     ScoringLog,
# 13     ErrorLog,
# 14     RecommendationLog,
# 15     LoggingProvider,
# 16     FeedbackLog,
# 17 )
# 18 from ...utilities.snapshots.snapshot_writer import SnapshotWriter
# 19 
# 20 
# 21 class RecommendationAgent(AgentBase):
# 22     """Analyze historical logs and generate code improvement recommendations."""
# 23 
# 24     def __init__(
# 25         self,
# 26         target: str,
# 27         logger: LoggingProvider | None = None,
# 28         snapshot_writer: SnapshotWriter | None = None,
# 29         context_provider: ContextProviderBase | None = None,
# 30     ) -> None:
# 31         super().__init__(logger)
# 32         self.target = target
# 33         self.snapshot_writer = snapshot_writer or SnapshotWriter()
# 34         self.context_provider = context_provider
# 35         self.recommendation_logs: List[RecommendationLog] = []
# 36         self.error_logs: List[ErrorLog] = []
# 37         self.conversation_logs: List[ConversationLog] = []
# 38 
# 39     #  ------------------------------------------------------------------
# 40     #  Internal helpers
# 41     #  ------------------------------------------------------------------
# 42     def _analyze_symbol_graph(
# 43         self, graph: Dict[str, Dict[str, Any]]
# 44     ) -> List[Dict[str, Any]]:
# 45         """Generate recommendations based on a symbol graph."""
# 46         recs: List[Dict[str, Any]] = []
# 47 
# 48         #  Build call relationships by symbol name
# 49         callers: Dict[str, set[str]] = {}
# 50         definitions: Dict[str, Dict[str, Any]] = {}
# 51         for qual, info in graph.items():
# 52             definitions[qual] = info
# 53             for called in info.get("calls", []):
# 54                 callers.setdefault(called, set()).add(qual)
# 55 
# 56         #  Unused imports and symbols
# 57         for qual, info in definitions.items():
# 58             name = info.get("name")
# 59             typ = info.get("type")
# 60             if typ == "import" and name not in callers:
# 61                 recs.append(
# 62                     {
# 63                         "action": "remove_unused_import",
# 64                         "target": qual,
# 65                         "rationale": f"Import '{name}' is never referenced",
# 66                     }
# 67                 )
# 68             if typ in {"function", "async_function", "class"} and name not in callers:
# 69                 recs.append(
# 70                     {
# 71                         "action": "remove_unused_symbol",
# 72                         "target": qual,
# 73                         "rationale": f"Symbol '{name}' is defined but not used",
# 74                     }
# 75                 )
# 76 
# 77         #  Circular dependencies detection
# 78         graph_by_name = {
# 79             info.get("name"): qual
# 80             for qual, info in definitions.items()
# 81             if info.get("type") in {"function", "async_function", "class"}
# 82         }
# 83         edges: Dict[str, set[str]] = {}
# 84         for qual, info in definitions.items():
# 85             if info.get("type") not in {"function", "async_function", "class"}:
# 86                 continue
# 87             for called in info.get("calls", []):
# 88                 dest = graph_by_name.get(called)
# 89                 if dest:
# 90                     edges.setdefault(qual, set()).add(dest)
# 91 
# 92         def _find_cycles() -> List[List[str]]:
# 93             cycles: List[List[str]] = []
# 94             path: List[str] = []
# 95 
# 96             def visit(node: str, stack: set[str]) -> None:
# 97                 stack.add(node)
# 98                 path.append(node)
# 99                 for nxt in edges.get(node, set()):
# 100                     if nxt in stack:
# 101                         idx = path.index(nxt)
# 102                         cycles.append(path[idx:] + [nxt])
# 103                     else:
# 104                         visit(nxt, stack)
# 105                 stack.remove(node)
# 106                 path.pop()
# 107 
# 108             for n in list(edges):
# 109                 visit(n, set())
# 110             return cycles
# 111 
# 112         for cycle in _find_cycles():
# 113             if len(cycle) > 1:
# 114                 recs.append(
# 115                     {
# 116                         "action": "refactor_circular_dependency",
# 117                         "symbols": cycle,
# 118                         "rationale": " -> ".join(cycle),
# 119                     }
# 120                 )
# 121 
# 122         return recs
# 123 
# 124     def _run_agent_logic(self, *args, **kwargs) -> None:
# 125         feedback = kwargs.get("feedback")
# 126         if feedback:
# 127             for item in feedback:
# 128                 self.log_feedback(
# 129                     FeedbackLog(
# 130                         experiment_id="exp",
# 131                         round=0,
# 132                         source="recommendation",
# 133                         feedback=str(item),
# 134                     )
# 135                 )
# 136         conv_logs: List[ConversationLog] = kwargs.get("conversation_log", [])
# 137         qual_logs: List[CodeQualityLog] = kwargs.get("code_quality_log", [])
# 138         scoring_logs: List[ScoringLog] = kwargs.get("scoring_log", [])
# 139 
# 140         recommendations: List[str] = []
# 141         context_parts: List[str] = []
# 142         structured_recs: List[Dict[str, Any]] = []
# 143 
# 144         ctx = kwargs.get("context")
# 145         if ctx is None and self.context_provider is not None:
# 146             ctx = self.context_provider.get_context()
# 147         graph = {}
# 148         if isinstance(ctx, dict):
# 149             graph = ctx.get("symbol_graph", {})
# 150         if graph:
# 151             structured_recs.extend(self._analyze_symbol_graph(graph))
# 152             if structured_recs:
# 153                 context_parts.append("symbol graph analysis")
# 154 
# 155         for qlog in qual_logs:
# 156             if qlog.lint_errors > 0:
# 157                 recommendations.append("Fix lint errors")
# 158                 context_parts.append("lint issues detected")
# 159             if qlog.maintainability_index < 70:
# 160                 recommendations.append("Refactor to improve maintainability")
# 161                 context_parts.append("low maintainability")
# 162             if qlog.cyclomatic_complexity > 10:
# 163                 recommendations.append("Simplify to reduce complexity")
# 164                 context_parts.append("high complexity")
# 165 
# 166         for slog in scoring_logs:
# 167             if slog.value < 0.5:
# 168                 recommendations.append(f"Improve score for {slog.metric.value}")
# 169                 context_parts.append(f"low score {slog.metric.value}")
# 170 
# 171         for clog in conv_logs:
# 172             if clog.intervention and clog.content:
# 173                 recommendations.append(clog.content)
# 174                 context_parts.append("prior intervention")
# 175 
# 176         unique: List[str] = []
# 177         for r in recommendations:
# 178             if r not in unique:
# 179                 unique.append(r)
# 180 
# 181         for text in unique:
# 182             structured_recs.append({"action": "general", "message": text})
# 183 
# 184         if not structured_recs:
# 185             return
# 186 
# 187         rec_text = json.dumps(structured_recs, indent=2)
# 188         ctx_text = "; ".join(context_parts)
# 189 
# 190         rec_log = RecommendationLog(
# 191             experiment_id="exp",
# 192             round=0,
# 193             symbol=self.target,
# 194             file_path=self.target,
# 195             line_start=0,
# 196             line_end=0,
# 197             recommendation=rec_text,
# 198             context=ctx_text,
# 199         )
# 200         self.recommendation_logs.append(rec_log)
# 201 
# 202         conv_log = ConversationLog(
# 203             experiment_id="exp",
# 204             round=0,
# 205             agent_role=AgentRole.RECOMMENDER,
# 206             target=self.target,
# 207             content=rec_text,
# 208             originating_agent="recommender",
# 209             intervention=bool(structured_recs),
# 210             intervention_type="recommendation" if structured_recs else None,
# 211             intervention_reason="analysis" if structured_recs else None,
# 212         )
# 213         self.conversation_logs.append(conv_log)
# 214 
# 215         try:
# 216             self.log_recommendation(rec_log)
# 217             self.log_conversation(conv_log)
# 218         except Exception as exc:  #  pragma: no cover - db failure
# 219             err = ErrorLog(
# 220                 experiment_id="exp",
# 221                 round=0,
# 222                 error_type=type(exc).__name__,
# 223                 message=str(exc),
# 224                 file_path=self.target,
# 225             )
# 226             self.error_logs.append(err)
# 227             self.log_error(err)
# 228 
# 229         before = ctx_text
# 230         after = rec_text
# 231         self.snapshot_writer.write_snapshot(
# 232             experiment_id="exp",
# 233             round=0,
# 234             file_path=self.target,
# 235             before=before,
# 236             after=after,
# 237             symbol=self.target,
# 238             agent_role=AgentRole.RECOMMENDER,
# 239         )


# extensions\context_providers\__init__.py

# 1 from .dummy_context_provider import DummyContextProvider
# 2 from .symbol_graph_provider import SymbolGraphProvider
# 3 from ...registries.context_providers import CONTEXT_PROVIDER_REGISTRY
# 4 
# 5 CONTEXT_PROVIDER_REGISTRY.register("dummy", DummyContextProvider)
# 6 CONTEXT_PROVIDER_REGISTRY.register("symbol_graph", SymbolGraphProvider)
# 7 
# 8 __all__ = ["DummyContextProvider", "SymbolGraphProvider"]


# extensions\context_providers\dummy_context_provider.py

# 1 from ...abstract_classes.context_provider_base import ContextProviderBase
# 2 
# 3 
# 4 class DummyContextProvider(ContextProviderBase):
# 5     def _get_context(self, *args, **kwargs):
# 6         self._log.info("Dummy context provided")
# 7         return {}


# extensions\context_providers\symbol_graph_provider.py

# 1 from __future__ import annotations
# 2 
# 3 import ast
# 4 import json
# 5 from pathlib import Path
# 6 from typing import Any, Dict, List, MutableMapping, Tuple
# 7 
# 8 from ...abstract_classes.context_provider_base import ContextProviderBase
# 9 from ...factories.logging_provider import LoggingProvider
# 10 from ...utilities.snapshots.snapshot_writer import SnapshotWriter
# 11 from ...enums.agent_enums import AgentRole
# 12 
# 13 
# 14 #  cache of previously built graphs: {path: (mtime, graph)}
# 15 _GRAPH_CACHE: Dict[str, Tuple[float, Dict[str, Dict[str, Any]]]] = {}
# 16 
# 17 
# 18 class SymbolGraphProvider(ContextProviderBase):
# 19     """Generate a detailed symbol graph for a Python module or package."""
# 20 
# 21     def __init__(
# 22         self,
# 23         target: str | None = None,
# 24         module_path: str | None = None,
# 25         logger: LoggingProvider | None = None,
# 26         snapshot_writer: SnapshotWriter | None = None,
# 27     ) -> None:
# 28         super().__init__(logger)
# 29         #  allow both 'target' and legacy 'module_path'
# 30         path_str = module_path if module_path is not None else target
# 31         if path_str is None:
# 32             raise ValueError("SymbolGraphProvider requires 'target' or 'module_path'")
# 33         self.target = Path(path_str)
# 34         self.snapshot_writer = snapshot_writer or SnapshotWriter()
# 35         self._graph: Dict[str, Dict[str, Any]] = {}
# 36 
# 37     #  ------------------------------------------------------------------
# 38     #  Public API
# 39     #  ------------------------------------------------------------------
# 40     def resolve_symbol(self, name: str) -> Dict[str, Any] | None:
# 41         """Return details for a fully qualified symbol."""
# 42         return self._graph.get(name)
# 43 
# 44     #  ------------------------------------------------------------------
# 45     #  ContextProviderBase implementation
# 46     #  ------------------------------------------------------------------
# 47     def _get_context(self) -> Dict[str, Any]:
# 48         if not self.target.exists():
# 49             self._log.error("Target not found: %s", self.target)
# 50             return {}
# 51 
# 52         key = str(self.target)
# 53         mtime = self._target_mtime()
# 54         cached = _GRAPH_CACHE.get(key)
# 55 
# 56         if cached and cached[0] == mtime:
# 57             self._graph = cached[1]
# 58             self._log.debug("Symbol graph cache hit for %s", self.target)
# 59         else:
# 60             self._graph.clear()
# 61 
# 62             paths = [self.target]
# 63             if self.target.is_dir():
# 64                 paths = list(self.target.rglob("*.py"))
# 65 
# 66             for path in paths:
# 67                 self._parse_file(path)
# 68 
# 69             _GRAPH_CACHE[key] = (mtime, dict(self._graph))
# 70             self._log.debug("Symbol graph cached for %s", self.target)
# 71 
# 72         self._log.info("Symbol graph built for %s", self.target)
# 73 
# 74         try:
# 75             self.snapshot_writer.write_snapshot(
# 76                 experiment_id="context",
# 77                 round=0,
# 78                 file_path=f"{self.target}.symbol_graph.json",
# 79                 before="",
# 80                 after=json.dumps(self._graph, indent=2),
# 81                 symbol=str(self.target),
# 82                 agent_role=AgentRole.GENERATOR,
# 83             )
# 84         except Exception:
# 85             self._log.exception("Failed to snapshot symbol graph")
# 86 
# 87         return {"symbol_graph": self._graph}
# 88 
# 89     #  ------------------------------------------------------------------
# 90     #  Internal helpers
# 91     #  ------------------------------------------------------------------
# 92     def _parse_file(self, path: Path) -> None:
# 93         source = path.read_text(encoding="utf-8")
# 94         tree = ast.parse(source, filename=str(path))
# 95         module_name = self._module_name(path)
# 96 
# 97         visitor = _SymbolGraphVisitor(module_name, str(path), self._graph)
# 98         visitor.visit(tree)
# 99 
# 100     def _target_mtime(self) -> float:
# 101         if self.target.is_file():
# 102             return self.target.stat().st_mtime
# 103         mtimes = [p.stat().st_mtime for p in self.target.rglob("*.py")]
# 104         return max(mtimes, default=0.0)
# 105 
# 106     def _module_name(self, path: Path) -> str:
# 107         if self.target.is_file():
# 108             return path.stem
# 109         rel = path.relative_to(self.target)
# 110         return ".".join(rel.with_suffix("").parts)
# 111 
# 112 
# 113 class _SymbolGraphVisitor(ast.NodeVisitor):
# 114     """AST visitor building a symbol graph."""
# 115 
# 116     def __init__(
# 117         self, module: str, file_path: str, graph: MutableMapping[str, Dict[str, Any]]
# 118     ) -> None:
# 119         self.module = module
# 120         self.file_path = file_path
# 121         self.graph = graph
# 122         self.scope: List[str] = []
# 123         self.current: str | None = None
# 124 
# 125     def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
# 126         #  Record function definition and track calls
# 127         qual = self._qualify(node.name)
# 128         info: Dict[str, Any] = {
# 129             "name": node.name,
# 130             "type": "function",
# 131             "file": self.file_path,
# 132             "lineno": node.lineno,
# 133             "col_offset": node.col_offset,
# 134             "end_lineno": getattr(node, "end_lineno", node.lineno),
# 135             "end_col_offset": getattr(node, "end_col_offset", node.col_offset),
# 136             "scope": (
# 137                 ".".join([self.module, *self.scope]) if self.scope else self.module
# 138             ),
# 139             "calls": [],
# 140         }
# 141         self.graph[qual] = info
# 142         #  Enter function scope
# 143         self.scope.append(node.name)
# 144         prev_current = self.current
# 145         self.current = qual
# 146         self.generic_visit(node)
# 147         #  Exit function scope
# 148         self.scope.pop()
# 149         self.current = prev_current
# 150 
# 151     def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> None:
# 152         #  Record async function definition and track calls
# 153         qual = self._qualify(node.name)
# 154         info: Dict[str, Any] = {
# 155             "name": node.name,
# 156             "type": "async_function",
# 157             "file": self.file_path,
# 158             "lineno": node.lineno,
# 159             "col_offset": node.col_offset,
# 160             "end_lineno": getattr(node, "end_lineno", node.lineno),
# 161             "end_col_offset": getattr(node, "end_col_offset", node.col_offset),
# 162             "scope": (
# 163                 ".".join([self.module, *self.scope]) if self.scope else self.module
# 164             ),
# 165             "calls": [],
# 166         }
# 167         self.graph[qual] = info
# 168         #  Enter async function scope
# 169         self.scope.append(node.name)
# 170         prev_current = self.current
# 171         self.current = qual
# 172         self.generic_visit(node)
# 173         #  Exit async function scope
# 174         self.scope.pop()
# 175         self.current = prev_current
# 176 
# 177     def _process_function_or_async_function(
# 178         self, node: ast.FunctionDef | ast.AsyncFunctionDef
# 179     ) -> None:
# 180         qual = self._qualify(node.name)
# 181         info: Dict[str, Any] = {
# 182             "name": node.name,
# 183             "type": (
# 184                 "async_function"
# 185                 if isinstance(node, ast.AsyncFunctionDef)
# 186                 else "function"
# 187             ),
# 188             "file": self.file_path,
# 189             "lineno": node.lineno,
# 190             "col_offset": node.col_offset,
# 191             "end_lineno": getattr(node, "end_lineno", node.lineno),
# 192             "end_col_offset": getattr(node, "end_col_offset", node.col_offset),
# 193             "scope": (
# 194                 ".".join([self.module, *self.scope]) if self.scope else self.module
# 195             ),
# 196             "calls": [],
# 197         }
# 198         self.graph[qual] = info
# 199         self.scope.append(node.name)
# 200         self.current = qual
# 201 
# 202     def visit_ClassDef(self, node: ast.ClassDef) -> None:
# 203         qual = self._qualify(node.name)
# 204         self._record(qual, "class", node)
# 205         self.scope.append(node.name)
# 206         self.current = qual
# 207         self.generic_visit(node)
# 208         self.scope.pop()
# 209         self.current = (
# 210             ".".join([self.module, *self.scope]) if self.scope else self.module
# 211         )
# 212         self.generic_visit(node)
# 213 
# 214     def visit_Assign(self, node: ast.Assign) -> None:
# 215         for target in node.targets:
# 216             if isinstance(target, ast.Name):
# 217                 self._record(self._qualify(target.id), "variable", target)
# 218         self.generic_visit(node)
# 219 
# 220     def visit_Import(self, node: ast.Import) -> None:
# 221         for alias in node.names:
# 222             name = alias.asname or alias.name
# 223             entry = self._record(self._qualify(name), "import", node)
# 224             entry["target"] = alias.name
# 225         self.generic_visit(node)
# 226 
# 227     def visit_ImportFrom(self, node: ast.ImportFrom) -> None:
# 228         for alias in node.names:
# 229             name = alias.asname or alias.name
# 230             entry = self._record(self._qualify(name), "import", node)
# 231             entry["target"] = (
# 232                 f"{node.module}.{alias.name}" if node.module else alias.name
# 233             )
# 234         self.generic_visit(node)
# 235 
# 236     def visit_Call(self, node: ast.Call) -> None:
# 237         #  Determine current symbol context
# 238         current = self.current or self.module
# 239         #  Identify called name
# 240         if isinstance(node.func, ast.Name):
# 241             called_name = node.func.id
# 242         elif isinstance(node.func, ast.Attribute):
# 243             called_name = node.func.attr
# 244         else:
# 245             called_name = None
# 246         if called_name:
# 247             self.graph.setdefault(current, {}).setdefault("calls", []).append(
# 248                 called_name
# 249             )
# 250         self.generic_visit(node)
# 251 
# 252     def _qualify(self, name: str) -> str:
# 253         return ".".join([self.module, *self.scope, name])
# 254 
# 255     def _record(self, qual: str, typ: str, node: ast.AST) -> Dict[str, Any]:
# 256         lineno = getattr(node, "lineno", None)
# 257         col_offset = getattr(node, "col_offset", None)
# 258         end_lineno = getattr(node, "end_lineno", lineno)
# 259         end_col_offset = getattr(node, "end_col_offset", col_offset)
# 260         info: Dict[str, Any] = {
# 261             "type": typ,
# 262             "file": self.file_path,
# 263             "lineno": lineno,
# 264             "col_offset": col_offset,
# 265             "end_lineno": end_lineno,
# 266             "end_col_offset": end_col_offset,
# 267             "scope": (
# 268                 ".".join([self.module, *self.scope]) if self.scope else self.module
# 269             ),
# 270             "calls": [],
# 271         }
# 272         self.graph[qual] = info
# 273         return info


# extensions\prompt_generators\__init__.py

# 1 from .dummy_prompt_generator import DummyPromptGenerator
# 2 from .basic_prompt_generator import BasicPromptGenerator
# 3 from ...registries.prompt_generators import PROMPT_GENERATOR_REGISTRY
# 4 
# 5 PROMPT_GENERATOR_REGISTRY.register("dummy", DummyPromptGenerator)
# 6 PROMPT_GENERATOR_REGISTRY.register("basic", BasicPromptGenerator)
# 7 
# 8 __all__ = ["DummyPromptGenerator", "BasicPromptGenerator"]


# extensions\prompt_generators\basic_prompt_generator.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Any, Dict
# 4 
# 5 from ...abstract_classes.prompt_generator_base import PromptGeneratorBase
# 6 from ...abstract_classes.context_provider_base import ContextProviderBase
# 7 from ...factories.logging_provider import LoggingProvider
# 8 
# 9 
# 10 class BasicPromptGenerator(PromptGeneratorBase):
# 11     """Combine system and agent templates with code context."""
# 12 
# 13     def __init__(
# 14         self,
# 15         context_provider: ContextProviderBase,
# 16         logger: LoggingProvider | None = None,
# 17     ) -> None:
# 18         super().__init__(logger)
# 19         self.context_provider = context_provider
# 20 
# 21     def _generate_prompt(
# 22         self, agent_config: Dict[str, Any], system_config: Dict[str, Any]
# 23     ) -> str:
# 24         context = self.context_provider.get_context()
# 25         self._log.debug("Raw context: %s", context)
# 26 
# 27         system_template = system_config.get("template", "")
# 28         agent_template = agent_config.get("template", "")
# 29 
# 30         context_snippet = ""
# 31         if context:
# 32             funcs = ", ".join(context.get("functions", []))
# 33             classes = ", ".join(context.get("classes", {}).keys())
# 34             context_snippet = f"Functions: {funcs}\nClasses: {classes}"
# 35 
# 36         prompt = f"{agent_template}\n\n{system_template}\n\n{context_snippet}"
# 37         self._log.info("Prompt generated")
# 38         self._log.debug("Prompt content: %s", prompt)
# 39         return prompt


# extensions\prompt_generators\dummy_prompt_generator.py

# 1 from ...abstract_classes.prompt_generator_base import PromptGeneratorBase
# 2 
# 3 
# 4 class DummyPromptGenerator(PromptGeneratorBase):
# 5     def _generate_prompt(self, *args, **kwargs) -> str:
# 6         self._log.info("Dummy prompt generated")
# 7         return "dummy prompt"


# extensions\scoring_models\__init__.py

# 1 from .dummy_scoring_provider import DummyScoringProvider
# 2 from .basic_scoring_provider import BasicScoringProvider
# 3 from .advanced_scoring_provider import AdvancedScoringProvider
# 4 from .adaptive_scoring_provider import AdaptiveScoringProvider
# 5 from ...registries.scoring_models import SCORING_MODEL_REGISTRY
# 6 
# 7 SCORING_MODEL_REGISTRY.register("dummy", DummyScoringProvider)
# 8 SCORING_MODEL_REGISTRY.register("basic", BasicScoringProvider)
# 9 SCORING_MODEL_REGISTRY.register("advanced", AdvancedScoringProvider)
# 10 SCORING_MODEL_REGISTRY.register("adaptive", AdaptiveScoringProvider)
# 11 
# 12 __all__ = [
# 13     "DummyScoringProvider",
# 14     "BasicScoringProvider",
# 15     "AdvancedScoringProvider",
# 16     "AdaptiveScoringProvider",
# 17 ]


# extensions\scoring_models\adaptive_scoring_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Any, Dict, List
# 4 
# 5 from .advanced_scoring_provider import AdvancedScoringProvider
# 6 from ...utilities.feedback import FeedbackRepository
# 7 from ...factories.logging_provider import LoggingProvider
# 8 
# 9 
# 10 class AdaptiveScoringProvider(AdvancedScoringProvider):
# 11     """Scoring provider that adjusts weights based on feedback."""
# 12 
# 13     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 14         super().__init__(logger)
# 15         self.weights: Dict[str, float] = {
# 16             "maintainability_index": 1.0,
# 17             "cyclomatic_complexity": 1.0,
# 18             "recommendation_quality": 1.0,
# 19         }
# 20 
# 21     def _score(
# 22         self, logs: Dict[str, List[Any]], experiment_id: str = "exp"
# 23     ) -> Dict[str, float]:
# 24         metrics = super()._score(logs, experiment_id)
# 25 
# 26         for fb in FeedbackRepository.get_feedback(experiment_id):
# 27             text = fb.feedback.lower()
# 28             if "maintainability" in text:
# 29                 self.weights["maintainability_index"] += 0.1
# 30             if "complexity" in text:
# 31                 self.weights["cyclomatic_complexity"] += 0.1
# 32             if "recommendation" in text:
# 33                 self.weights["recommendation_quality"] += 0.1
# 34 
# 35         weighted = {
# 36             key: value * self.weights.get(key, 1.0) for key, value in metrics.items()
# 37         }
# 38         return weighted


# extensions\scoring_models\advanced_scoring_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Any, Dict, List
# 4 
# 5 from ...abstract_classes.scoring_provider_base import ScoringProviderBase
# 6 from .code_quality_scorer import CodeQualityScorer
# 7 from .recommendation_quality_scorer import RecommendationQualityScorer
# 8 
# 9 
# 10 class AdvancedScoringProvider(ScoringProviderBase):
# 11     """Combine quality and recommendation scoring into a single provider."""
# 12 
# 13     def __init__(self, logger=None) -> None:
# 14         super().__init__(logger)
# 15         self.code_quality = CodeQualityScorer(logger)
# 16         self.recommendation = RecommendationQualityScorer(logger)
# 17 
# 18     def _score(
# 19         self, logs: Dict[str, List[Any]], experiment_id: str = "exp"
# 20     ) -> Dict[str, float]:
# 21         code_quality_logs = logs.get("code_quality", [])
# 22         recommendation_logs = logs.get("recommendation", [])
# 23 
# 24         metrics: Dict[str, float] = {}
# 25         metrics.update(self.code_quality.score(experiment_id, code_quality_logs))
# 26         metrics.update(self.recommendation.score(experiment_id, recommendation_logs))
# 27         return metrics


# extensions\scoring_models\basic_scoring_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Any, Dict, List
# 4 
# 5 from ...abstract_classes.scoring_provider_base import ScoringProviderBase
# 6 from ...utilities.metrics import compute_metrics
# 7 
# 8 
# 9 class BasicScoringProvider(ScoringProviderBase):
# 10     """Compute experiment metrics using basic heuristics."""
# 11 
# 12     def _score(self, logs: Dict[str, List[Any]]) -> Dict[str, float]:
# 13         self._log.debug("Scoring with logs: %s", logs)
# 14         evaluation_logs = logs.get("evaluation", [])
# 15         code_quality_logs = logs.get("code_quality", [])
# 16         conversation_logs = logs.get("conversation", [])
# 17         prompt_logs = logs.get("prompt", [])
# 18         state_logs = logs.get("state", [])
# 19 
# 20         metrics = compute_metrics(
# 21             evaluation_logs,
# 22             code_quality_logs,
# 23             conversation_logs,
# 24             prompt_logs,
# 25             state_logs,
# 26         )
# 27         self._log.info("Computed metrics: %s", metrics)
# 28         return metrics


# extensions\scoring_models\code_quality_scorer.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Dict, List
# 4 
# 5 from ...abstract_classes.scoring_model_base import ScoringModelBase
# 6 from ...factories.logging_provider import CodeQualityLog
# 7 from ...enums.scoring_enums import ScoringMetric
# 8 
# 9 
# 10 class CodeQualityScorer(ScoringModelBase):
# 11     """Evaluate code quality logs and compute quality metrics."""
# 12 
# 13     def _score(
# 14         self, experiment_id: str, logs: List[CodeQualityLog]
# 15     ) -> Dict[str, float]:
# 16         if not logs:
# 17             mi = 0.0
# 18             cc = 0.0
# 19         else:
# 20             mi = sum(log.maintainability_index for log in logs) / len(logs)
# 21             cc = sum(log.cyclomatic_complexity for log in logs) / len(logs)
# 22 
# 23         self._log_metric(experiment_id, ScoringMetric.MAINTAINABILITY_INDEX, mi)
# 24         self._log_metric(experiment_id, ScoringMetric.CYCLOMATIC_COMPLEXITY, cc)
# 25 
# 26         return {
# 27             ScoringMetric.MAINTAINABILITY_INDEX.value: mi,
# 28             ScoringMetric.CYCLOMATIC_COMPLEXITY.value: cc,
# 29         }


# extensions\scoring_models\dummy_scoring_provider.py

# 1 from ...abstract_classes.scoring_provider_base import ScoringProviderBase
# 2 
# 3 
# 4 class DummyScoringProvider(ScoringProviderBase):
# 5     def _score(self, *args, **kwargs):
# 6         self._log.info("Dummy scoring")
# 7         return 0


# extensions\scoring_models\recommendation_quality_scorer.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Dict, List
# 4 
# 5 from ...abstract_classes.scoring_model_base import ScoringModelBase
# 6 from ...factories.logging_provider import RecommendationLog
# 7 from ...enums.scoring_enums import ScoringMetric
# 8 
# 9 
# 10 class RecommendationQualityScorer(ScoringModelBase):
# 11     """Evaluate recommendations for context relevance."""
# 12 
# 13     def _score(
# 14         self, experiment_id: str, logs: List[RecommendationLog]
# 15     ) -> Dict[str, float]:
# 16         if not logs:
# 17             quality = 0.0
# 18         else:
# 19             with_context = [1.0 for log in logs if log.context]
# 20             quality = sum(with_context) / len(logs)
# 21 
# 22         self._log_metric(experiment_id, ScoringMetric.RECOMMENDATION_QUALITY, quality)
# 23 
# 24         return {ScoringMetric.RECOMMENDATION_QUALITY.value: quality}


# extensions\state_managers\__init__.py

# 1 from .dummy_state_manager import DummyStateManager
# 2 from .state_manager import StateManager
# 3 from ...registries.state_managers import STATE_MANAGER_REGISTRY
# 4 
# 5 STATE_MANAGER_REGISTRY.register("dummy", DummyStateManager)
# 6 STATE_MANAGER_REGISTRY.register("state", StateManager)
# 7 
# 8 __all__ = ["DummyStateManager", "StateManager"]


# extensions\state_managers\dummy_state_manager.py

# 1 from ...abstract_classes.state_manager_base import StateManagerBase
# 2 
# 3 
# 4 class DummyStateManager(StateManagerBase):
# 5     def _run_state_logic(self, *args, **kwargs) -> None:
# 6         self._log.info("Dummy state logic executed")


# extensions\state_managers\state_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ...abstract_classes.state_manager_base import StateManagerBase
# 4 from ...factories.logging_provider import LoggingProvider
# 5 
# 6 
# 7 class StateManager(StateManagerBase):
# 8     """Concrete manager executing actions within each FSM state."""
# 9 
# 10     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 11         super().__init__(logger)
# 12 
# 13     def _run_state_logic(self, *args, **kwargs) -> None:
# 14         state = kwargs.get("state")
# 15         self._log.info("Running state logic for %s", state)


# extensions\system_managers\__init__.py

# 1 from .dummy_system_manager import DummySystemManager
# 2 from .system_manager import SystemManager
# 3 from ...registries.system_managers import SYSTEM_MANAGER_REGISTRY
# 4 
# 5 SYSTEM_MANAGER_REGISTRY.register("dummy", DummySystemManager)
# 6 SYSTEM_MANAGER_REGISTRY.register("system", SystemManager)
# 7 
# 8 __all__ = ["DummySystemManager", "SystemManager"]


# extensions\system_managers\dummy_system_manager.py

# 1 from ...abstract_classes.system_manager_base import SystemManagerBase
# 2 
# 3 
# 4 class DummySystemManager(SystemManagerBase):
# 5     def _run_system_logic(self, *args, **kwargs) -> None:
# 6         self._log.info("Dummy system logic executed")


# extensions\system_managers\system_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import List
# 4 
# 5 from ...abstract_classes.system_manager_base import SystemManagerBase
# 6 from ...enums.system_enums import SystemState, StateTransitionReason
# 7 from ...factories.agent import AgentFactory
# 8 from ...factories.context_provider import ContextProviderFactory
# 9 from ...factories.prompt_manager import PromptGeneratorFactory
# 10 from ...factories.logging_provider import (
# 11     CodeQualityLog,
# 12     PromptLog,
# 13     ScoringLog,
# 14     StateLog,
# 15     StateTransitionLog,
# 16     LoggingProvider,
# 17 )
# 18 from ..state_managers.state_manager import StateManager
# 19 
# 20 
# 21 class SystemManager(SystemManagerBase):
# 22     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 23         super().__init__(logger)
# 24         self.state_manager = StateManager(logger=logger)
# 25         self.current_state = SystemState.START
# 26         self.transition_logs: List[StateTransitionLog] = []
# 27         self.state_logs: List[StateLog] = []
# 28         self.prompt_logs: List[PromptLog] = []
# 29         self.code_quality_logs: List[CodeQualityLog] = []
# 30         self.scoring_logs: List[ScoringLog] = []
# 31 
# 32         self.context_provider = ContextProviderFactory.create(
# 33             "symbol_graph", module_path="sample_module.py"
# 34         )
# 35         self.prompt_generator = PromptGeneratorFactory.create(
# 36             "basic", context_provider=self.context_provider
# 37         )
# 38         self.generator = AgentFactory.create(
# 39             "generator",
# 40             target="sample_module.py",
# 41             logger=logger,
# 42             snapshot_writer=None,
# 43         )
# 44         self.evaluator = AgentFactory.create(
# 45             "evaluator",
# 46             target="sample_module.py",
# 47             logger=logger,
# 48         )
# 49 
# 50     def _transition_to(
# 51         self, next_state: SystemState, reason: StateTransitionReason
# 52     ) -> None:
# 53         log = StateTransitionLog(
# 54             experiment_id="exp",
# 55             round=0,
# 56             from_state=self.current_state,
# 57             to_state=next_state,
# 58             reason=reason,
# 59         )
# 60         self.transition_logs.append(log)
# 61         self._log.info("%s -> %s", self.current_state.value, next_state.value)
# 62         self.log_transition(log)
# 63         self.current_state = next_state
# 64 
# 65     def _run_system_logic(self, *args, **kwargs) -> None:
# 66         sequence = [
# 67             SystemState.GENERATE,
# 68             SystemState.DISCRIMINATE,
# 69             SystemState.MEDIATE,
# 70             SystemState.PATCH,
# 71             SystemState.EVALUATE,
# 72             SystemState.END,
# 73         ]
# 74         for state in sequence:
# 75             self._transition_to(state, reason=StateTransitionReason.FIRST_ROUND)
# 76             if state is not SystemState.END:
# 77                 self.state_manager.run(state=state.value)
# 78                 self.state_logs.append(
# 79                     StateLog(
# 80                         experiment_id="exp",
# 81                         system="system",
# 82                         round=0,
# 83                         state=state,
# 84                         action="run",
# 85                     )
# 86                 )
# 87                 if state == SystemState.GENERATE:
# 88                     self.generator.run()
# 89                     self.prompt_logs.extend(self.generator.prompt_logs)
# 90                 elif state == SystemState.DISCRIMINATE:
# 91                     self.evaluator.run()
# 92                     self.code_quality_logs.extend(self.evaluator.quality_logs)
# 93 
# 94         for trans in self.transition_logs:
# 95             self.log_transition(trans)
# 96         for st in self.state_logs:
# 97             self.log_state(st)
# 98         for pr in self.prompt_logs:
# 99             self.log_prompt(pr)
# 100         for cq in self.code_quality_logs:
# 101             self.log_code_quality(cq)
# 102         for sc in self.scoring_logs:
# 103             self.log_scoring(sc)


# extensions\system_prompts\__init__.py

# 1 


# extensions\tool_providers\__init__.py

# 1 from .dummy_tool_provider import DummyToolProvider
# 2 from .black_runner import BlackToolProvider
# 3 from .mypy_runner import MypyToolProvider
# 4 from .radon_runner import RadonToolProvider
# 5 from .ruff_runner import RuffToolProvider
# 6 from .docformatter_runner import DocFormatterToolProvider
# 7 from ...registries.tool_providers import TOOL_PROVIDER_REGISTRY
# 8 
# 9 TOOL_PROVIDER_REGISTRY.register("dummy", DummyToolProvider)
# 10 TOOL_PROVIDER_REGISTRY.register("black", BlackToolProvider)
# 11 TOOL_PROVIDER_REGISTRY.register("mypy", MypyToolProvider)
# 12 TOOL_PROVIDER_REGISTRY.register("radon", RadonToolProvider)
# 13 TOOL_PROVIDER_REGISTRY.register("ruff", RuffToolProvider)
# 14 TOOL_PROVIDER_REGISTRY.register("docformatter", DocFormatterToolProvider)
# 15 
# 16 __all__ = [
# 17     "DummyToolProvider",
# 18     "BlackToolProvider",
# 19     "MypyToolProvider",
# 20     "RadonToolProvider",
# 21     "RuffToolProvider",
# 22     "DocFormatterToolProvider",
# 23 ]


# extensions\tool_providers\black_runner.py

# 1 from __future__ import annotations
# 2 import subprocess
# 3 import sys
# 4 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 5 
# 6 
# 7 class BlackToolProvider(ToolProviderBase):
# 8     def _run(self, target: str, check: bool = False):
# 9         #  Use quiet mode to suppress emojis and extra output on Windows
# 10         cmd = [sys.executable, "-m", "black", "--quiet", target]
# 11         if check:
# 12             cmd.append("--check")
# 13         proc = subprocess.run(
# 14             cmd, capture_output=True, text=True, encoding="utf-8", errors="ignore"
# 15         )
# 16         if proc.stdout:
# 17             self._log.debug(proc.stdout)
# 18         if proc.stderr:
# 19             self._log.error(proc.stderr)
# 20         if proc.returncode != 0:
# 21             raise RuntimeError(f"black failed: {proc.stderr}")
# 22         return proc


# extensions\tool_providers\docformatter_runner.py

# 1 import subprocess
# 2 import sys
# 3 from app.abstract_classes.tool_provider_base import ToolProviderBase
# 4 
# 5 
# 6 class DocFormatterToolProvider(ToolProviderBase):
# 7     def _run(self, target: str, check: bool = False):
# 8         cmd = [sys.executable, "-m", "docformatter", target, "--in-place"]
# 9         if check:
# 10             cmd.append("--check")
# 11         proc = subprocess.run(cmd, capture_output=True, text=True)
# 12         if proc.stdout:
# 13             self._log.debug(proc.stdout)
# 14         if proc.stderr:
# 15             self._log.error(proc.stderr)
# 16         if proc.returncode != 0:
# 17             raise RuntimeError(f"docformatter failed: {proc.stderr}")
# 18         return proc


# extensions\tool_providers\dummy_tool_provider.py

# 1 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 2 
# 3 
# 4 class DummyToolProvider(ToolProviderBase):
# 5     def _run(self, *args, **kwargs):
# 6         self._log.info("Dummy tool run")
# 7         return True


# extensions\tool_providers\mypy_runner.py

# 1 from __future__ import annotations
# 2 
# 3 import subprocess
# 4 import sys
# 5 
# 6 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 7 
# 8 
# 9 class MypyToolProvider(ToolProviderBase):
# 10     def _run(self, target: str):
# 11         cmd = [sys.executable, "-m", "mypy", target]
# 12         proc = subprocess.run(cmd, capture_output=True, text=True)
# 13         if proc.stdout:
# 14             self._log.debug(proc.stdout)
# 15         if proc.stderr:
# 16             self._log.error(proc.stderr)
# 17         if proc.returncode not in (0, 1):  #  mypy returns 1 if it finds issues
# 18             raise RuntimeError(f"mypy execution error: {proc.stderr}")
# 19         return proc


# extensions\tool_providers\radon_runner.py

# 1 from __future__ import annotations
# 2 import subprocess
# 3 import sys
# 4 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 5 
# 6 
# 7 class RadonToolProvider(ToolProviderBase):
# 8     def _run(self, target: str):
# 9         #  Call radon via -m so it returns a CompletedProcess
# 10         cmd = [sys.executable, "-m", "radon", "cc", target]
# 11         proc = subprocess.run(cmd, capture_output=True, text=True)
# 12 
# 13         if proc.stdout:
# 14             self._log.debug(proc.stdout)
# 15         if proc.stderr:
# 16             self._log.error(proc.stderr)
# 17 
# 18         if proc.returncode != 0:
# 19             raise RuntimeError(f"radon failed: {proc.stderr.strip()}")
# 20 
# 21         return proc


# extensions\tool_providers\ruff_runner.py

# 1 from __future__ import annotations
# 2 
# 3 import subprocess
# 4 import sys
# 5 
# 6 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 7 
# 8 
# 9 class RuffToolProvider(ToolProviderBase):
# 10     def _run(self, target: str):
# 11         cmd = [sys.executable, "-m", "ruff", "check", target]
# 12         proc = subprocess.run(cmd, capture_output=True, text=True)
# 13         if proc.stdout:
# 14             self._log.debug(proc.stdout)
# 15         if proc.stderr:
# 16             self._log.error(proc.stderr)
# 17         if proc.returncode != 0:
# 18             raise RuntimeError(f"ruff failed: {proc.stderr}")
# 19         return proc


# extensions\tool_providers\sonarcloud_runner.py

# 1 from __future__ import annotations
# 2 
# 3 from app.abstract_classes.tool_provider_base import ToolProviderBase
# 4 
# 5 
# 6 class SonarCloudToolProvider(ToolProviderBase):
# 7     def _run(self, project_key: str, organization: str, token: str):
# 8         self._log.info(
# 9             "SonarCloud stub executed for project '%s' in organization '%s'.",
# 10             project_key,
# 11             organization,
# 12         )
# 13         return {
# 14             "status": "success",
# 15             "project_key": project_key,
# 16             "organization": organization,
# 17             "analysis_url": "https://sonarcloud.io/dashboard?id=" + project_key,
# 18         }


# extensions\tools\__init__.py



# factories\__init__.py



# factories\agent.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.agents import AGENT_REGISTRY
# 4 
# 5 
# 6 class AgentFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             AGENT_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = AGENT_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"Agent {name} not registered")
# 19         return cls(**kwargs)


# factories\context_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.context_providers import CONTEXT_PROVIDER_REGISTRY
# 4 
# 5 
# 6 class ContextProviderFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             CONTEXT_PROVIDER_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = CONTEXT_PROVIDER_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"Context provider {name} not registered")
# 19         return cls(**kwargs)


# factories\experiment_config_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Dict, Any
# 4 
# 5 _CONFIGS: Dict[str, Dict[str, Any]] = {}
# 6 
# 7 
# 8 class ExperimentConfigProvider:
# 9     """Simple provider storing experiment configurations in memory."""
# 10 
# 11     @staticmethod
# 12     def register(config_id: int | str, config: Dict[str, Any]) -> None:
# 13         _CONFIGS[str(config_id)] = config
# 14 
# 15     @staticmethod
# 16     def load(config_id: int | str) -> Dict[str, Any]:
# 17         config = _CONFIGS.get(str(config_id))
# 18         if config is None:
# 19             raise KeyError(f"experiment config {config_id} not found")
# 20         return config


# factories\logging_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from datetime import datetime
# 4 import json
# 5 import logging
# 6 import sqlite3
# 7 from dataclasses import asdict, is_dataclass
# 8 from enum import Enum
# 9 from pathlib import Path
# 10 from typing import Any, ClassVar, Iterable
# 11 
# 12 from app.enums.logging_enums import LogType
# 13 from ..utilities.metadata.logging.log_schemas import (
# 14     PromptGenerationLog,
# 15     StateLog,
# 16     StateTransitionLog,
# 17     PromptLog,
# 18     CodeQualityLog,
# 19     ErrorLog,
# 20     ScoringLog,
# 21     ConversationLog,
# 22     ExperimentLog,
# 23     RecommendationLog,
# 24     FeedbackLog,
# 25 )
# 26 from ..utilities.feedback import FeedbackRepository
# 27 
# 28 
# 29 LOG_MODEL_MAP = {
# 30     LogType.STATE: StateLog,
# 31     LogType.STATE_TRANSITION: StateTransitionLog,
# 32     LogType.PROMPT: PromptLog,
# 33     LogType.CODE_QUALITY: CodeQualityLog,
# 34     LogType.ERROR: ErrorLog,
# 35     LogType.SCORING: ScoringLog,
# 36     LogType.CONVERSATION: ConversationLog,
# 37     LogType.EXPERIMENT: ExperimentLog,
# 38     LogType.RECOMMENDATION: RecommendationLog,
# 39     LogType.FEEDBACK: FeedbackLog,
# 40     LogType.PROMPT_GENERATION: PromptGenerationLog
# 41 }
# 42 
# 43 
# 44 class LoggingProvider:
# 45     """Centralized provider for structured logging."""
# 46 
# 47     _instance: ClassVar["LoggingProvider" | None] = None
# 48 
# 49     def __new__(cls, *args, **kwargs):
# 50         if cls._instance is None:
# 51             cls._instance = super().__new__(cls)
# 52         return cls._instance
# 53 
# 54     def __init__(
# 55         self,
# 56         db_path: str | Path = "experiments/codecritic.sqlite3",
# 57         output_path: str | Path | None = None,
# 58         connection: sqlite3.Connection | None = None,
# 59     ) -> None:
# 60         if getattr(self, "_initialized", False):
# 61             return
# 62 
# 63         self.db_path = Path(db_path)
# 64         self.db_path.parent.mkdir(exist_ok=True, parents=True)
# 65 
# 66         from app.utilities.db import get_connection, init_db
# 67 
# 68         if connection:
# 69             self.conn = connection
# 70             init_db(conn=self.conn)  #  Fix applied here
# 71         else:
# 72             self.conn = get_connection()
# 73             init_db(conn=self.conn)  #  Fix applied here
# 74 
# 75         self.output_path = Path(output_path) if output_path else None
# 76         if self.output_path:
# 77             self.output_path.parent.mkdir(parents=True, exist_ok=True)
# 78 
# 79         self._initialized = True
# 80 
# 81     def _serialize(self, obj: Any) -> dict:
# 82         def _safe(v: Any) -> Any:
# 83             if isinstance(v, (Enum, Path)):
# 84                 return str(v)
# 85             elif isinstance(v, datetime):
# 86                 return v.isoformat()
# 87             elif isinstance(v, list):
# 88                 return [_safe(i) for i in v]
# 89             elif isinstance(v, dict):
# 90                 return {k: _safe(val) for k, val in v.items()}
# 91             return v
# 92 
# 93         if not is_dataclass(obj) or isinstance(obj, type):
# 94             raise TypeError(f"Expected dataclass instance, got {type(obj)}")
# 95         return _safe(asdict(obj))
# 96 
# 97     def _insert_many(self, table: str, items: Iterable[dict]) -> None:
# 98         items = list(items)
# 99         if not items:
# 100             return
# 101         keys = list(items[0].keys())
# 102         cols = ",".join(keys)
# 103         placeholders = ",".join(["?"] * len(keys))
# 104         values = [tuple(i[k] for k in keys) for i in items]
# 105         cur = self.conn.cursor()
# 106         cur.executemany(f"INSERT INTO {table} ({cols}) VALUES ({placeholders})", values)
# 107         self.conn.commit()
# 108         if self.output_path:
# 109             with self.output_path.open("a", encoding="utf-8") as fh:
# 110                 for item in items:
# 111                     fh.write(json.dumps(item) + "\n")
# 112 
# 113     def write(self, log_type: LogType, entries: list[Any] | Any) -> None:
# 114         if not isinstance(entries, list):
# 115             entries = [entries]
# 116         model_cls = LOG_MODEL_MAP.get(log_type)
# 117         if model_cls is None:
# 118             raise ValueError(f"Unsupported log type: {log_type}")
# 119         for entry in entries:
# 120             if not isinstance(entry, model_cls):
# 121                 raise TypeError(f"Expected {model_cls.__name__}, got {type(entry)}")
# 122         serialized = [self._serialize(e) for e in entries]
# 123         self._insert_many(log_type.value + "_log", serialized)
# 124 
# 125     #  Convenience wrappers
# 126     def log_state(self, log: StateLog) -> None:
# 127         self.write(LogType.STATE, log)
# 128 
# 129     def log_transition(self, log: StateTransitionLog) -> None:
# 130         self.write(LogType.STATE_TRANSITION, log)
# 131 
# 132     def log_prompt(self, log: PromptLog) -> None:
# 133         self.write(LogType.PROMPT, log)
# 134 
# 135     def log_code_quality(self, log: CodeQualityLog) -> None:
# 136         self.write(LogType.CODE_QUALITY, log)
# 137 
# 138     def log_error(self, log: ErrorLog) -> None:
# 139         self.write(LogType.ERROR, log)
# 140 
# 141     def log_scoring(self, log: ScoringLog) -> None:
# 142         self.write(LogType.SCORING, log)
# 143 
# 144     def log_conversation(self, log: ConversationLog) -> None:
# 145         self.write(LogType.CONVERSATION, log)
# 146 
# 147     def log_experiment(self, log: ExperimentLog) -> None:
# 148         self.write(LogType.EXPERIMENT, log)
# 149 
# 150     def log_recommendation(self, log: RecommendationLog) -> None:
# 151         self.write(LogType.RECOMMENDATION, log)
# 152 
# 153     def log_feedback(self, log: FeedbackLog) -> None:
# 154         self.write(LogType.FEEDBACK, log)
# 155         FeedbackRepository.add_feedback(log)
# 156 
# 157     def close(self) -> None:
# 158         self.conn.close()
# 159 
# 160 
# 161 class LoggingMixin:
# 162     """Mixin to provide access to a shared LoggingProvider instance."""
# 163 
# 164     def __init__(self, logger: LoggingProvider | None = None) -> None:
# 165         self.logger = logger or LoggingProvider()
# 166         self._log = logging.getLogger(self.__class__.__name__)
# 167 
# 168     def log_state(self, log: StateLog) -> None:
# 169         self.logger.log_state(log)
# 170 
# 171     def log_transition(self, log: StateTransitionLog) -> None:
# 172         self.logger.log_transition(log)
# 173 
# 174     def log_prompt(self, log: PromptLog) -> None:
# 175         self.logger.log_prompt(log)
# 176 
# 177     def log_code_quality(self, log: CodeQualityLog) -> None:
# 178         self.logger.log_code_quality(log)
# 179 
# 180     def log_error(self, log: ErrorLog) -> None:
# 181         self.logger.log_error(log)
# 182 
# 183     def log_scoring(self, log: ScoringLog) -> None:
# 184         self.logger.log_scoring(log)
# 185 
# 186     def log_conversation(self, log: ConversationLog) -> None:
# 187         self.logger.log_conversation(log)
# 188 
# 189     def log_experiment(self, log: ExperimentLog) -> None:
# 190         self.logger.log_experiment(log)
# 191 
# 192     def log_recommendation(self, log: RecommendationLog) -> None:
# 193         self.logger.log_recommendation(log)
# 194 
# 195     def log_feedback(self, log: FeedbackLog) -> None:
# 196         self.logger.log_feedback(log)


# factories\prompt_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.prompt_generators import PROMPT_GENERATOR_REGISTRY
# 4 
# 5 
# 6 class PromptGeneratorFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             PROMPT_GENERATOR_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = PROMPT_GENERATOR_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"Prompt generator {name} not registered")
# 19         return cls(**kwargs)


# factories\scoring_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.scoring_models import SCORING_MODEL_REGISTRY
# 4 
# 5 
# 6 class ScoringProviderFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             SCORING_MODEL_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = SCORING_MODEL_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"Scoring provider {name} not registered")
# 19         return cls(**kwargs)


# factories\state_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.state_managers import STATE_MANAGER_REGISTRY
# 4 
# 5 
# 6 class StateManagerFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             STATE_MANAGER_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = STATE_MANAGER_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"State manager {name} not registered")
# 19         return cls(**kwargs)


# factories\system_config_provider.py

# 1 from __future__ import annotations
# 2 
# 3 
# 4 class SystemConfigProvider:
# 5     @staticmethod
# 6     def create(config: dict):
# 7         return config


# factories\system_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.system_managers import SYSTEM_MANAGER_REGISTRY
# 4 
# 5 
# 6 class SystemManagerFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             SYSTEM_MANAGER_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = SYSTEM_MANAGER_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"System manager {name} not registered")
# 19         return cls(**kwargs)


# factories\tool_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.tool_providers import TOOL_PROVIDER_REGISTRY
# 4 
# 5 
# 6 class ToolProviderFactory:
# 7     @staticmethod
# 8     def register(name: str, cls) -> None:
# 9         try:
# 10             TOOL_PROVIDER_REGISTRY.register(name, cls)
# 11         except KeyError:
# 12             pass
# 13 
# 14     @staticmethod
# 15     def create(name: str, **kwargs):
# 16         cls = TOOL_PROVIDER_REGISTRY.get(name)
# 17         if cls is None:
# 18             raise KeyError(f"Tool provider {name} not registered")
# 19         return cls(**kwargs)


# registries\__init__.py

# 1 class Registry:
# 2     def __init__(self):
# 3         self._registry = {}
# 4 
# 5     def register(self, name: str, item):
# 6         existing = self._registry.get(name)
# 7         if existing is not None:
# 8             if existing is item:
# 9                 return
# 10             raise KeyError(f"{name} already registered")
# 11         self._registry[name] = item
# 12 
# 13     def get(self, name: str):
# 14         return self._registry.get(name)
# 15 
# 16     def all(self):
# 17         return dict(self._registry)


# registries\agent_engines\__init__.py

# 1 from .. import Registry
# 2 
# 3 AGENT_ENGINE_REGISTRY = Registry()


# registries\agent_prompts\__init__.py

# 1 from .. import Registry
# 2 
# 3 AGENT_PROMPT_REGISTRY = Registry()


# registries\agents\__init__.py

# 1 from .. import Registry
# 2 
# 3 AGENT_REGISTRY = Registry()


# registries\context_providers\__init__.py

# 1 from .. import Registry
# 2 
# 3 CONTEXT_PROVIDER_REGISTRY = Registry()


# registries\prompt_generators\__init__.py

# 1 from .. import Registry
# 2 
# 3 PROMPT_GENERATOR_REGISTRY = Registry()


# registries\scoring_models\__init__.py

# 1 from .. import Registry
# 2 
# 3 SCORING_MODEL_REGISTRY = Registry()


# registries\state_managers\__init__.py

# 1 from .. import Registry
# 2 
# 3 STATE_MANAGER_REGISTRY = Registry()


# registries\system_managers\__init__.py

# 1 from .. import Registry
# 2 
# 3 SYSTEM_MANAGER_REGISTRY = Registry()


# registries\system_prompts\__init__.py

# 1 from .. import Registry
# 2 
# 3 SYSTEM_PROMPT_REGISTRY = Registry()


# registries\tool_providers\__init__.py

# 1 from .. import Registry
# 2 
# 3 TOOL_PROVIDER_REGISTRY = Registry()


# registries\tools\__init__.py

# 1 from .. import Registry
# 2 
# 3 TOOL_REGISTRY = Registry()


# schemas\__init__.py

# 1 from .agent_engine_schema import AgentEngine
# 2 from .agent_prompt_schema import AgentPrompt
# 3 from .system_prompt_schema import SystemPrompt
# 4 from .context_provider_schema import ContextProvider
# 5 from .tooling_provider_schema import ToolingProvider
# 6 from .file_path_schema import FilePath
# 7 from .agent_config_schema import AgentConfig
# 8 from .prompt_generator_schema import PromptGenerator
# 9 from .scoring_provider_schema import ScoringProvider
# 10 from .state_manager_schema import StateManager
# 11 from .system_config_schema import SystemConfig
# 12 from .experiment_config_schema import ExperimentConfig
# 13 from .series_schema import Series
# 14 
# 15 __all__ = [
# 16     "AgentEngine",
# 17     "AgentPrompt",
# 18     "SystemPrompt",
# 19     "ContextProvider",
# 20     "ToolingProvider",
# 21     "FilePath",
# 22     "AgentConfig",
# 23     "PromptGenerator",
# 24     "ScoringProvider",
# 25     "StateManager",
# 26     "SystemConfig",
# 27     "ExperimentConfig",
# 28     "Series",
# 29 ]


# schemas\agent_config_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator, model_validator
# 8 
# 9 from .agent_engine_schema import AgentEngine
# 10 from .prompt_generator_schema import PromptGenerator
# 11 
# 12 
# 13 class AgentConfig(BaseModel):
# 14     """Schema for the agent_config table."""
# 15 
# 16     id: Optional[int] = None
# 17     name: str
# 18     description: Optional[str] = None
# 19     agent_role: Optional[str] = None
# 20     system_type: Optional[str] = None
# 21     agent_engine_id: Optional[int] = None
# 22     prompt_generator_id: Optional[int] = None
# 23     agent_engine: Optional[AgentEngine] = None
# 24     prompt_generator: Optional[PromptGenerator] = None
# 25     artifact_path: Optional[Path] = None
# 26 
# 27     table_name: str = "agent_config"
# 28 
# 29     @model_validator(mode="after")
# 30     def _sync_ids(self) -> "AgentConfig":
# 31         if self.agent_engine is not None and self.agent_engine_id is None:
# 32             self.agent_engine_id = getattr(self.agent_engine, "id", None)
# 33         if self.prompt_generator is not None and self.prompt_generator_id is None:
# 34             self.prompt_generator_id = getattr(self.prompt_generator, "id", None)
# 35         return self
# 36 
# 37     @field_validator("artifact_path")
# 38     @classmethod
# 39     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 40         if v is None:
# 41             return v
# 42         p = Path(v)
# 43         if not p.is_absolute() and ".." in p.parts:
# 44             raise ValueError("artifact_path must be absolute or project relative")
# 45         return p
# 46 
# 47     def model_dump(self, **kwargs) -> dict:  #  type: ignore[misc]
# 48         return super().model_dump(**kwargs)  #  type: ignore[misc]


# schemas\agent_engine_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class AgentEngine(BaseModel):
# 11     """Schema for the agent_engine table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     model: Optional[str] = None
# 17     engine_config: Optional[str] = None
# 18     artifact_path: Optional[Path] = None
# 19 
# 20     table_name: str = "agent_engine"
# 21 
# 22     @field_validator("artifact_path")
# 23     @classmethod
# 24     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 25         if v is None:
# 26             return v
# 27         p = Path(v)
# 28         if not p.is_absolute() and ".." in p.parts:
# 29             raise ValueError("artifact_path must be absolute or project relative")
# 30         return p
# 31 
# 32     def model_dump(self, **kwargs) -> dict:  #  type: ignore[misc]
# 33         return super().model_dump(**kwargs)  #  type: ignore[misc]


# schemas\agent_prompt_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class AgentPrompt(BaseModel):
# 11     """Schema for the agent_prompt table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     agent_role: Optional[str] = None
# 17     system_type: Optional[str] = None
# 18     artifact_path: Optional[Path] = None
# 19 
# 20     table_name: str = "agent_prompt"
# 21 
# 22     @field_validator("artifact_path")
# 23     @classmethod
# 24     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 25         if v is None:
# 26             return v
# 27         p = Path(v)
# 28         if not p.is_absolute() and ".." in p.parts:
# 29             raise ValueError("artifact_path must be absolute or project relative")
# 30         return p
# 31 
# 32     def model_dump(self, **kwargs) -> dict:  #  type: ignore[misc]
# 33         return super().model_dump(**kwargs)  #  type: ignore[misc]


# schemas\context_provider_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .tooling_provider_schema import ToolingProvider
# 9 
# 10 
# 11 class ContextProvider(BaseModel):
# 12     """Schema for the context_provider table."""
# 13 
# 14     id: Optional[int] = None
# 15     name: str
# 16     description: Optional[str] = None
# 17     system_type: Optional[str] = None
# 18     tooling_provider_id: Optional[int] = None
# 19     tooling_provider: Optional[ToolingProvider] = None
# 20 
# 21     table_name: str = "context_provider"
# 22 
# 23     @model_validator(mode="after")
# 24     def _sync_ids(self) -> "ContextProvider":
# 25         if self.tooling_provider is not None and self.tooling_provider_id is None:
# 26             self.tooling_provider_id = getattr(self.tooling_provider, "id", None)
# 27         return self
# 28 
# 29     def model_dump(self, **kwargs) -> dict:  #  type: ignore[misc]
# 30         return super().model_dump(**kwargs)  #  type: ignore[misc]


# schemas\experiment_config_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .system_config_schema import SystemConfig
# 9 from .scoring_provider_schema import ScoringProvider
# 10 
# 11 
# 12 class ExperimentConfig(BaseModel):
# 13     """Schema for the experiment_config table."""
# 14 
# 15     id: Optional[int] = None
# 16     name: str
# 17     description: Optional[str] = None
# 18     system_manager_id: Optional[int | str] = None
# 19     scoring_model_id: Optional[int | str] = None
# 20 
# 21     system_manager: Optional[SystemConfig] = None
# 22     scoring_model: Optional[ScoringProvider] = None
# 23 
# 24     table_name: str = "experiment_config"
# 25 
# 26     @model_validator(mode="after")
# 27     def _sync_ids(self) -> "ExperimentConfig":
# 28         if self.system_manager is not None and self.system_manager_id is None:
# 29             self.system_manager_id = getattr(self.system_manager, "id", None)
# 30         if self.scoring_model is not None and self.scoring_model_id is None:
# 31             self.scoring_model_id = getattr(self.scoring_model, "id", None)
# 32         return self
# 33 
# 34     def model_dump(self, **kwargs) -> dict:  #  type: ignore[misc]
# 35         return super().model_dump(**kwargs)  #  type: ignore[misc]


# schemas\file_path_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import field_validator
# 7 
# 8 
# 9 class FilePath(BaseModel):
# 10     """Schema for file paths tracked in the database."""
# 11 
# 12     artifact_path: Path
# 13 
# 14     table_name: str = "file_path"
# 15 
# 16     @field_validator("artifact_path")
# 17     @classmethod
# 18     def _check_path(cls, v: Path) -> Path:
# 19         p = Path(v)
# 20         if not p.is_absolute() and ".." in p.parts:
# 21             raise ValueError("artifact_path must be absolute or project relative")
# 22         return p
# 23 
# 24     def model_dump(self, **kwargs) -> dict:  #  type: ignore[misc]
# 25         return super().model_dump(**kwargs)  #  type: ignore[misc]


# schemas\prompt_generator_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator, model_validator
# 8 
# 9 from .agent_prompt_schema import AgentPrompt
# 10 from .system_prompt_schema import SystemPrompt
# 11 from .context_provider_schema import ContextProvider
# 12 
# 13 
# 14 class PromptGenerator(BaseModel):
# 15     """Schema for the prompt_generator table."""
# 16 
# 17     id: Optional[int] = None
# 18     name: str
# 19     description: Optional[str] = None
# 20     agent_prompt_id: Optional[int] = None
# 21     system_prompt_id: Optional[int] = None
# 22     content_provider_id: Optional[int] = None
# 23 
# 24     agent_prompt: Optional[AgentPrompt] = None
# 25     system_prompt: Optional[SystemPrompt] = None
# 26     content_provider: Optional[ContextProvider] = None
# 27     artifact_path: Optional[Path] = None
# 28 
# 29     table_name: str = "prompt_generator"
# 30 
# 31     @model_validator(mode="after")
# 32     def _sync_ids(self) -> "PromptGenerator":
# 33         if self.agent_prompt is not None and self.agent_prompt_id is None:
# 34             self.agent_prompt_id = getattr(self.agent_prompt, "id", None)
# 35         if self.system_prompt is not None and self.system_prompt_id is None:
# 36             self.system_prompt_id = getattr(self.system_prompt, "id", None)
# 37         if self.content_provider is not None and self.content_provider_id is None:
# 38             self.content_provider_id = getattr(self.content_provider, "id", None)
# 39         return self
# 40 
# 41     @field_validator("artifact_path")
# 42     @classmethod
# 43     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 44         if v is None:
# 45             return v
# 46         p = Path(v)
# 47         if not p.is_absolute() and ".." in p.parts:
# 48             raise ValueError("artifact_path must be absolute or project relative")
# 49         return p
# 50 
# 51     def model_dump(self, **kwargs) -> dict:  #  type: ignore[misc]
# 52         return super().model_dump(**kwargs)  #  type: ignore[misc]


# schemas\scoring_provider_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class ScoringProvider(BaseModel):
# 11     """Schema for the scoring_provider table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     artifact_path: Optional[Path] = None
# 17 
# 18     table_name: str = "scoring_provider"
# 19 
# 20     @field_validator("artifact_path")
# 21     @classmethod
# 22     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 23         if v is None:
# 24             return v
# 25         p = Path(v)
# 26         if not p.is_absolute() and ".." in p.parts:
# 27             raise ValueError("artifact_path must be absolute or project relative")
# 28         return p
# 29 
# 30     def model_dump(self, **kwargs) -> dict:  #  type: ignore[misc]
# 31         return super().model_dump(**kwargs)  #  type: ignore[misc]


# schemas\series_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .experiment_config_schema import ExperimentConfig
# 9 
# 10 
# 11 class Series(BaseModel):
# 12     """Schema for the series table."""
# 13 
# 14     id: Optional[int] = None
# 15     experiment_config_id: Optional[int] = None
# 16     experiment_config: Optional[ExperimentConfig] = None
# 17 
# 18     table_name: str = "series"
# 19 
# 20     @model_validator(mode="after")
# 21     def _sync_ids(self) -> "Series":
# 22         if self.experiment_config is not None and self.experiment_config_id is None:
# 23             self.experiment_config_id = getattr(self.experiment_config, "id", None)
# 24         return self
# 25 
# 26     def model_dump(self, **kwargs) -> dict:  #  type: ignore[misc]
# 27         return super().model_dump(**kwargs)  #  type: ignore[misc]


# schemas\state_manager_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class StateManager(BaseModel):
# 11     """Schema for the state_manager table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     system_state: Optional[str] = None
# 17     system_type: Optional[str] = None
# 18     agent_id: Optional[int] = None
# 19     artifact_path: Optional[Path] = None
# 20 
# 21     table_name: str = "state_manager"
# 22 
# 23     @field_validator("artifact_path")
# 24     @classmethod
# 25     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 26         if v is None:
# 27             return v
# 28         p = Path(v)
# 29         if not p.is_absolute() and ".." in p.parts:
# 30             raise ValueError("artifact_path must be absolute or project relative")
# 31         return p
# 32 
# 33     def model_dump(self, **kwargs) -> dict:  #  type: ignore[misc]
# 34         return super().model_dump(**kwargs)  #  type: ignore[misc]


# schemas\system_config_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .state_manager_schema import StateManager
# 9 from .scoring_provider_schema import ScoringProvider
# 10 
# 11 
# 12 class SystemConfig(BaseModel):
# 13     """Schema for the system_config table."""
# 14 
# 15     id: Optional[int] = None
# 16     name: str
# 17     description: Optional[str] = None
# 18     system_type: Optional[str] = None
# 19     state_manager_id: Optional[int] = None
# 20     scoring_model_id: Optional[int] = None
# 21 
# 22     state_manager: Optional[StateManager] = None
# 23     scoring_model: Optional[ScoringProvider] = None
# 24 
# 25     table_name: str = "system_config"
# 26 
# 27     @model_validator(mode="after")
# 28     def _sync_ids(self) -> "SystemConfig":
# 29         if self.state_manager is not None and self.state_manager_id is None:
# 30             self.state_manager_id = getattr(self.state_manager, "id", None)
# 31         if self.scoring_model is not None and self.scoring_model_id is None:
# 32             self.scoring_model_id = getattr(self.scoring_model, "id", None)
# 33         return self
# 34 
# 35     def model_dump(self, **kwargs) -> dict:  #  type: ignore[misc]
# 36         return super().model_dump(**kwargs)  #  type: ignore[misc]


# schemas\system_prompt_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class SystemPrompt(BaseModel):
# 11     """Schema for the system_prompt table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     system_type: Optional[str] = None
# 17     artifact_path: Optional[Path] = None
# 18 
# 19     table_name: str = "system_prompt"
# 20 
# 21     @field_validator("artifact_path")
# 22     @classmethod
# 23     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 24         if v is None:
# 25             return v
# 26         p = Path(v)
# 27         if not p.is_absolute() and ".." in p.parts:
# 28             raise ValueError("artifact_path must be absolute or project relative")
# 29         return p
# 30 
# 31     def model_dump(self, **kwargs) -> dict:  #  type: ignore[misc]
# 32         return super().model_dump(**kwargs)  #  type: ignore[misc]


# schemas\tooling_provider_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class ToolingProvider(BaseModel):
# 11     """Schema for the tooling_provider table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     artifact_path: Optional[Path] = None
# 17 
# 18     table_name: str = "tooling_provider"
# 19 
# 20     @field_validator("artifact_path")
# 21     @classmethod
# 22     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 23         if v is None:
# 24             return v
# 25         p = Path(v)
# 26         if not p.is_absolute() and ".." in p.parts:
# 27             raise ValueError("artifact_path must be absolute or project relative")
# 28         return p
# 29 
# 30     def model_dump(self, **kwargs) -> dict:  #  type: ignore[misc]
# 31         return super().model_dump(**kwargs)  #  type: ignore[misc]


# utilities\__init__.py



# utilities\db.py

# 1 import sqlite3
# 2 from pathlib import Path
# 3 from typing import Any
# 4 
# 5 DB_PATH = Path("experiments") / "codecritic.sqlite3"
# 6 _CONN: sqlite3.Connection | None = None
# 7 
# 8 
# 9 def _serialize(obj: Any) -> dict:
# 10     from dataclasses import asdict, is_dataclass
# 11     from enum import Enum
# 12     from pathlib import Path
# 13 
# 14     if not is_dataclass(obj) or isinstance(obj, type):
# 15         raise TypeError(f"Expected dataclass instance, got {type(obj)}")
# 16 
# 17     result = asdict(obj)
# 18     return {
# 19         k: (str(v) if isinstance(v, (Path, Enum)) else v) for k, v in result.items()
# 20     }
# 21 
# 22 
# 23 def get_connection() -> sqlite3.Connection:
# 24     global _CONN
# 25     try:
# 26         if _CONN is None:
# 27             raise RuntimeError
# 28         _CONN.execute("SELECT 1")  #  ping connection
# 29     except (sqlite3.ProgrammingError, RuntimeError):
# 30         _CONN = sqlite3.connect(DB_PATH, check_same_thread=False)
# 31     return _CONN
# 32 
# 33 
# 34 def close_connection() -> None:
# 35     global _CONN
# 36     if _CONN:
# 37         _CONN.close()
# 38         _CONN = None
# 39 
# 40 
# 41 def init_db(conn: sqlite3.Connection | None = None) -> sqlite3.Connection:
# 42     if conn is None:
# 43         conn = get_connection()
# 44     cur = conn.cursor()
# 45 
# 46     cur.execute(
# 47         """CREATE TABLE IF NOT EXISTS state_log (
# 48             experiment_id TEXT,
# 49             system TEXT,
# 50             round INTEGER,
# 51             state TEXT,
# 52             action TEXT,
# 53             score REAL,
# 54             details TEXT,
# 55             timestamp TEXT
# 56         )"""
# 57     )
# 58 
# 59     cur.execute(
# 60         """CREATE TABLE IF NOT EXISTS state_transition_log (
# 61             experiment_id TEXT,
# 62             round INTEGER,
# 63             from_state TEXT,
# 64             to_state TEXT,
# 65             reason TEXT,
# 66             timestamp TEXT
# 67         )"""
# 68     )
# 69 
# 70     cur.execute(
# 71         """CREATE TABLE IF NOT EXISTS prompt_log (
# 72             experiment_id TEXT,
# 73             round INTEGER,
# 74             system TEXT,
# 75             agent_id TEXT,
# 76             agent_role TEXT,
# 77             agent_engine TEXT,
# 78             symbol TEXT,
# 79             prompt TEXT,
# 80             response TEXT,
# 81             attempt_number INTEGER,
# 82             agent_action_outcome TEXT,
# 83             start TEXT,
# 84             stop TEXT
# 85         )"""
# 86     )
# 87 
# 88     cur.execute(
# 89         """CREATE TABLE IF NOT EXISTS code_quality_log (
# 90             experiment_id TEXT,
# 91             round INTEGER,
# 92             symbol TEXT,
# 93             lines_of_code INTEGER,
# 94             cyclomatic_complexity REAL,
# 95             maintainability_index REAL,
# 96             lint_errors INTEGER,
# 97             timestamp TEXT
# 98         )"""
# 99     )
# 100 
# 101     cur.execute(
# 102         """CREATE TABLE IF NOT EXISTS scoring_log (
# 103             experiment_id TEXT,
# 104             round INTEGER,
# 105             metric TEXT,
# 106             value REAL,
# 107             timestamp TEXT
# 108         )"""
# 109     )
# 110 
# 111     cur.execute(
# 112         """CREATE TABLE IF NOT EXISTS error_log (
# 113             experiment_id TEXT,
# 114             round INTEGER,
# 115             error_type TEXT,
# 116             message TEXT,
# 117             file_path TEXT,
# 118             timestamp TEXT
# 119         )"""
# 120     )
# 121 
# 122     cur.execute(
# 123         """CREATE TABLE IF NOT EXISTS prompt_generation_log (
# 124             experiment_id TEXT,
# 125             round INTEGER,
# 126             generator_name TEXT,
# 127             context_provider_name TEXT,
# 128             agent_config TEXT,
# 129             system_config TEXT,
# 130             generated_prompt TEXT,
# 131             success INTEGER,
# 132             error_message TEXT,
# 133             timestamp TEXT
# 134         )"""
# 135     )
# 136 
# 137 
# 138     #  ✅ Add this missing table definition
# 139     cur.execute(
# 140         """CREATE TABLE IF NOT EXISTS conversation_log (
# 141             experiment_id TEXT,
# 142             round INTEGER,
# 143             agent_role TEXT,
# 144             target TEXT,
# 145             content TEXT,
# 146             originating_agent TEXT,
# 147             intervention INTEGER,
# 148             intervention_type TEXT,
# 149             intervention_reason TEXT,
# 150             timestamp TEXT
# 151         )"""
# 152     )
# 153 
# 154     cur.execute(
# 155         """CREATE TABLE IF NOT EXISTS recommendation_log (
# 156             experiment_id TEXT,
# 157             round INTEGER,
# 158             symbol TEXT,
# 159             file_path TEXT,
# 160             line_start INTEGER,
# 161             line_end INTEGER,
# 162             recommendation TEXT,
# 163             context TEXT,
# 164             timestamp TEXT
# 165         )"""
# 166     )
# 167 
# 168     cur.execute(
# 169         """CREATE TABLE IF NOT EXISTS feedback_log (
# 170             experiment_id TEXT,
# 171             round INTEGER,
# 172             source TEXT,
# 173             feedback TEXT,
# 174             timestamp TEXT
# 175         )"""
# 176     )
# 177 
# 178     conn.commit()
# 179     return conn


# utilities\feedback.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import ClassVar, Dict, List
# 4 
# 5 from .metadata.logging.log_schemas import FeedbackLog
# 6 
# 7 
# 8 class FeedbackRepository:
# 9     """In-memory store for feedback logs."""
# 10 
# 11     _data: ClassVar[Dict[str, List[FeedbackLog]]] = {}
# 12 
# 13     @classmethod
# 14     def add_feedback(cls, log: FeedbackLog) -> None:
# 15         cls._data.setdefault(log.experiment_id, []).append(log)
# 16 
# 17     @classmethod
# 18     def get_feedback(cls, experiment_id: str) -> List[FeedbackLog]:
# 19         return list(cls._data.get(experiment_id, []))
# 20 
# 21     @classmethod
# 22     def clear(cls) -> None:
# 23         cls._data.clear()


# utilities\file_management\__init__.py



# utilities\metadata\__init__.py



# utilities\metadata\footer\__init__.py



# utilities\metadata\logging\__init__.py

# 1 from .log_schemas import (
# 2     StateLog,
# 3     StateTransitionLog,
# 4     PromptLog,
# 5     CodeQualityLog,
# 6     ErrorLog,
# 7     ScoringLog,
# 8 )
# 9 
# 10 __all__ = [
# 11     "StateLog",
# 12     "StateTransitionLog",
# 13     "PromptLog",
# 14     "CodeQualityLog",
# 15     "ErrorLog",
# 16     "ScoringLog",
# 17 ]


# utilities\metadata\logging\log_schemas.py

# 1 from __future__ import annotations
# 2 
# 3 from dataclasses import dataclass, field
# 4 from datetime import datetime, timezone
# 5 from app.enums.agent_enums import AgentRole
# 6 from app.enums.scoring_enums import ScoringMetric
# 7 from app.enums.system_enums import SystemState, StateTransitionReason
# 8 
# 9 
# 10 @dataclass
# 11 class StateLog:
# 12     experiment_id: str
# 13     system: str
# 14     round: int
# 15     state: SystemState
# 16     action: str
# 17     score: float | None = None
# 18     details: str | None = None
# 19     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 20 
# 21 
# 22 @dataclass
# 23 class StateTransitionLog:
# 24     experiment_id: str
# 25     round: int
# 26     from_state: SystemState
# 27     to_state: SystemState
# 28     reason: StateTransitionReason
# 29     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 30 
# 31 
# 32 @dataclass
# 33 class PromptLog:
# 34     experiment_id: str
# 35     round: int
# 36     system: str
# 37     agent_id: str
# 38     agent_role: AgentRole
# 39     agent_engine: str | None = None
# 40     symbol: str | None = None
# 41     prompt: str | None = None
# 42     response: str | None = None
# 43     attempt_number: int = 0
# 44     agent_action_outcome: str | None = None
# 45     start: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 46     stop: datetime | None = None
# 47 
# 48 
# 49 @dataclass
# 50 class CodeQualityLog:
# 51     experiment_id: str
# 52     round: int
# 53     symbol: str
# 54     lines_of_code: int
# 55     cyclomatic_complexity: float
# 56     maintainability_index: float
# 57     lint_errors: int
# 58     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 59 
# 60 
# 61 @dataclass
# 62 class ErrorLog:
# 63     experiment_id: str
# 64     round: int
# 65     error_type: str
# 66     message: str
# 67     file_path: str | None = None
# 68     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 69 
# 70 
# 71 @dataclass
# 72 class ScoringLog:
# 73     experiment_id: str
# 74     round: int
# 75     metric: ScoringMetric
# 76     value: float
# 77     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 78 
# 79 
# 80 @dataclass
# 81 class ConversationLog:
# 82     experiment_id: str
# 83     round: int
# 84     agent_role: AgentRole
# 85     target: str
# 86     content: str
# 87     originating_agent: str
# 88     intervention: bool
# 89     intervention_type: str | None = None
# 90     intervention_reason: str | None = None
# 91     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 92 
# 93 
# 94 @dataclass
# 95 class ExperimentLog:
# 96     experiment_id: str
# 97     description: str
# 98     mode: str
# 99     variant: str
# 100     max_iterations: int
# 101     stop_threshold: float
# 102     model_engine: str
# 103     evaluator_name: str
# 104     evaluator_version: str
# 105     final_score: float
# 106     passed: bool
# 107     reason_for_stop: str
# 108     start: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 109     stop: datetime | None = None
# 110 
# 111 
# 112 @dataclass
# 113 class RecommendationLog:
# 114     experiment_id: str
# 115     round: int
# 116     symbol: str
# 117     file_path: str
# 118     line_start: int
# 119     line_end: int
# 120     recommendation: str
# 121     context: str
# 122     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 123 
# 124 
# 125 @dataclass
# 126 class FeedbackLog:
# 127     """Capture feedback for adaptive learning."""
# 128 
# 129     experiment_id: str
# 130     round: int
# 131     source: str
# 132     feedback: str
# 133     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 134 
# 135 @dataclass
# 136 class PromptGenerationLog:
# 137     experiment_id: str
# 138     round: int
# 139     generator_name: str
# 140     context_provider_name: str
# 141     agent_config: dict
# 142     system_config: dict
# 143     generated_prompt: str
# 144     success: bool
# 145     error_message: str | None = None
# 146     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))


# utilities\metadata\snapshots\__init__.py



# utilities\metrics.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from typing import Any, Dict, List
# 5 
# 6 logger = logging.getLogger(__name__)
# 7 
# 8 #  Unified list of metric names expected in experiment results
# 9 EVALUATION_METRICS = [
# 10     "bug_fix_success_rate",
# 11     "functional_correctness",
# 12     "avg_test_pass_rate",
# 13     "maintainability_index",
# 14     "cyclomatic_complexity",
# 15     "linting_compliance_rate",
# 16     "iterations_to_convergence",
# 17     "intervention_frequency",
# 18     "agent_role_success_rate",
# 19     "retry_success_rate",
# 20     "mediation_success_rate",
# 21 ]
# 22 
# 23 
# 24 def _get(obj: Any, key: str, default: Any = None) -> Any:
# 25     if isinstance(obj, dict):
# 26         return obj.get(key, default)
# 27     return getattr(obj, key, default)
# 28 
# 29 
# 30 def _avg(values: List[float]) -> float:
# 31     return sum(values) / len(values) if values else 0.0
# 32 
# 33 
# 34 def compute_metrics(
# 35     evaluation_logs: List[Any],
# 36     code_quality_logs: List[Any],
# 37     conversation_logs: List[Any],
# 38     prompt_logs: List[Any],
# 39     state_logs: List[Any],
# 40 ) -> Dict[str, float]:
# 41     """Compute experiment metrics from structured logs."""
# 42 
# 43     logger.debug(
# 44         "Computing metrics: evaluations=%d quality=%d conversation=%d prompts=%d states=%d",
# 45         len(evaluation_logs),
# 46         len(code_quality_logs),
# 47         len(conversation_logs),
# 48         len(prompt_logs),
# 49         len(state_logs),
# 50     )
# 51 
# 52     metrics: Dict[str, float] = {}
# 53 
# 54     bug_fixes = [_get(log, "bug_fixed", False) for log in evaluation_logs]
# 55     metrics["bug_fix_success_rate"] = _avg([1.0 for bug in bug_fixes if bug])
# 56 
# 57     func_correct = [_get(log, "all_tests_passed", False) for log in evaluation_logs]
# 58     metrics["functional_correctness"] = _avg([1.0 for passes in func_correct if passes])
# 59 
# 60     pass_rates = []
# 61     for log in evaluation_logs:
# 62         total = _get(log, "tests_total", 0)
# 63         passed = _get(log, "tests_passed", 0)
# 64         if total:
# 65             pass_rates.append(passed / total)
# 66     metrics["avg_test_pass_rate"] = _avg(pass_rates)
# 67 
# 68     mi_values = [_get(log, "maintainability_index", 0.0) for log in code_quality_logs]
# 69     metrics["maintainability_index"] = _avg(mi_values)
# 70 
# 71     cc_values = [_get(log, "cyclomatic_complexity", 0.0) for log in code_quality_logs]
# 72     metrics["cyclomatic_complexity"] = _avg(cc_values)
# 73 
# 74     lint_compliance = [
# 75         1.0 if _get(log, "lint_errors", 1) == 0 else 0.0 for log in code_quality_logs
# 76     ]
# 77     metrics["linting_compliance_rate"] = _avg(lint_compliance)
# 78 
# 79     rounds = [_get(log, "round", 0) for log in state_logs]
# 80     metrics["iterations_to_convergence"] = max(rounds) + 1 if rounds else 0.0
# 81 
# 82     interventions = [
# 83         1.0 for log in conversation_logs if _get(log, "intervention", False)
# 84     ]
# 85     metrics["intervention_frequency"] = _avg(interventions)
# 86 
# 87     outcomes = [_get(log, "agent_action_outcome", None) for log in prompt_logs]
# 88     successes = [1.0 for o in outcomes if o == "success"]
# 89     metrics["agent_role_success_rate"] = _avg(successes)
# 90 
# 91     retry_logs = [log for log in prompt_logs if _get(log, "attempt_number", 0) > 0]
# 92     retry_successes = [
# 93         1.0
# 94         for log in retry_logs
# 95         if _get(log, "agent_action_outcome", None) == "success"
# 96     ]
# 97     metrics["retry_success_rate"] = _avg(retry_successes)
# 98 
# 99     mediation_logs = [
# 100         log
# 101         for log in conversation_logs
# 102         if _get(log, "intervention_type", None) == "mediation"
# 103     ]
# 104     mediation_success = [
# 105         1.0 for log in mediation_logs if _get(log, "intervention", False)
# 106     ]
# 107     metrics["mediation_success_rate"] = _avg(mediation_success)
# 108 
# 109     logger.info("Metrics computed: %s", metrics)
# 110     return metrics


# utilities\pydantic_compat.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import TYPE_CHECKING, Callable
# 4 
# 5 try:  #  pragma: no cover - pydantic v1 compatibility
# 6     from pydantic import BaseModel
# 7 
# 8     if not hasattr(BaseModel, "model_dump"):
# 9         from typing import Callable
# 10 
# 11         def _model_dump(self: BaseModel, **kwargs) -> dict:
# 12             return BaseModel.dict(self, **kwargs)
# 13 
# 14         setattr(BaseModel, "model_dump", _model_dump)  #  type: ignore[attr-defined]
# 15 except Exception:  #  pragma: no cover - ignore if pydantic v2
# 16     pass
# 17 
# 18 if TYPE_CHECKING:
# 19     from pydantic import field_validator, model_validator  #  type: ignore
# 20 else:  #  pragma: no cover - runtime fallback for pydantic v1
# 21     try:
# 22         from pydantic import field_validator, model_validator  #  type: ignore
# 23     except ImportError:  #  pragma: no cover - pydantic v1
# 24         from pydantic import validator, root_validator
# 25 
# 26         def field_validator(*fields: str, **kwargs) -> Callable:
# 27             kwargs.pop("mode", None)
# 28             return validator(*fields, **kwargs)
# 29 
# 30         def model_validator(*fields, **kwargs) -> Callable:
# 31             mode = kwargs.pop("mode", "after")
# 32 
# 33             def decorator(func: Callable) -> Callable:
# 34                 if mode == "after":
# 35 
# 36                     def wrapper(cls, values):
# 37                         obj = cls.construct(**values)
# 38                         func(obj)
# 39                         return obj.dict()
# 40 
# 41                     return root_validator(pre=False, allow_reuse=True)(wrapper)
# 42 
# 43                 def wrapper(cls, values):
# 44                     return func(values)
# 45 
# 46                 return root_validator(pre=True, allow_reuse=True)(wrapper)
# 47 
# 48             return decorator


# utilities\schema\__init__.py

# 1 from .create_schema import initialize_database, create_tables, load_seed_data
# 2 
# 3 __all__ = ["initialize_database", "create_tables", "load_seed_data"]


# utilities\schema\create_schema.py

# 1 from __future__ import annotations
# 2 
# 3 import json
# 4 import sqlite3
# 5 from pathlib import Path
# 6 from typing import Type, Any, Union, get_origin, get_args
# 7 from enum import Enum
# 8 
# 9 from app.schemas import (
# 10     AgentEngine,
# 11     AgentPrompt,
# 12     SystemPrompt,
# 13     ContextProvider,
# 14     ToolingProvider,
# 15     FilePath,
# 16     AgentConfig,
# 17     PromptGenerator,
# 18     ScoringProvider,
# 19     StateManager,
# 20     SystemConfig,
# 21     ExperimentConfig,
# 22     Series,
# 23 )
# 24 from app.utilities import db
# 25 
# 26 SCHEMAS = {
# 27     "agent_engine": AgentEngine,
# 28     "agent_prompt": AgentPrompt,
# 29     "system_prompt": SystemPrompt,
# 30     "context_provider": ContextProvider,
# 31     "tooling_provider": ToolingProvider,
# 32     "file_path": FilePath,
# 33     "agent_config": AgentConfig,
# 34     "prompt_generator": PromptGenerator,
# 35     "scoring_provider": ScoringProvider,
# 36     "state_manager": StateManager,
# 37     "system_config": SystemConfig,
# 38     "experiment_config": ExperimentConfig,
# 39     "series": Series,
# 40 }
# 41 
# 42 _TYPE_MAP = {
# 43     int: "INTEGER",
# 44     str: "TEXT",
# 45     float: "REAL",
# 46     Path: "TEXT",
# 47 }
# 48 
# 49 
# 50 def _sqlite_type(py_type: Type) -> str:
# 51     origin = get_origin(py_type)
# 52     if origin is Union:
# 53         py_type = get_args(py_type)[0]
# 54     return _TYPE_MAP.get(py_type, "TEXT")
# 55 
# 56 
# 57 def _is_optional(annotation: Any) -> bool:
# 58     return get_origin(annotation) is Union and type(None) in get_args(annotation)
# 59 
# 60 
# 61 def create_tables(conn: sqlite3.Connection) -> None:
# 62     cur = conn.cursor()
# 63     for table_name, model_cls in SCHEMAS.items():
# 64         fields = model_cls.__annotations__
# 65         columns = []
# 66         for name, annotation in fields.items():
# 67             col_type = _sqlite_type(annotation)
# 68             if name == "id" and _is_optional(annotation):
# 69                 columns.append(f"{name} INTEGER PRIMARY KEY")
# 70             else:
# 71                 columns.append(f"{name} {col_type}")
# 72         col_sql = ", ".join(columns)
# 73         cur.execute(f"CREATE TABLE IF NOT EXISTS {table_name} ({col_sql})")
# 74     conn.commit()
# 75 
# 76 
# 77 def load_seed_data(
# 78     conn: sqlite3.Connection, seed_dir: Path | str = "experiments/config/seed"
# 79 ) -> None:
# 80     seed_path = Path(seed_dir)
# 81     if not seed_path.exists():
# 82         return
# 83     cur = conn.cursor()
# 84     for file in seed_path.glob("*.json"):
# 85         table_name = file.stem
# 86         model = SCHEMAS.get(table_name)
# 87         if model is None:
# 88             continue
# 89         entries = json.loads(file.read_text())
# 90         if isinstance(entries, dict):
# 91             entries = [entries]
# 92         for entry in entries:
# 93             obj = model(**entry)
# 94             data = obj.model_dump()
# 95             #  Convert enums and paths to strings
# 96             data = {
# 97                 k: str(v) if isinstance(v, (Path, Enum)) else v for k, v in data.items()
# 98             }
# 99             cols = ",".join(data.keys())
# 100             placeholders = ",".join(["?"] * len(data))
# 101             cur.execute(
# 102                 f"INSERT INTO {table_name} ({cols}) VALUES ({placeholders})",
# 103                 list(data.values()),
# 104             )
# 105     conn.commit()
# 106 
# 107 
# 108 def initialize_database(reset: bool = False) -> sqlite3.Connection:
# 109     """
# 110     Initialize (and optionally reset) the experiment database.
# 111     If reset=True, closes any open connection and deletes the existing file.
# 112     Then re-creates tables (and loads seed data, if any).
# 113     """
# 114     if reset and db.DB_PATH.exists():
# 115         db.close_connection()  #  close the global connection if it’s open
# 116         db.DB_PATH.unlink()  #  now safe to delete the file
# 117 
# 118     conn = db.get_connection()
# 119     create_tables(conn)
# 120     load_seed_data(conn)  #  uncomment if you have seed data
# 121     return conn
# 122 
# 123 
# 124 if __name__ == "__main__":
# 125     initialize_database()


# utilities\snapshots\__init__.py



# utilities\snapshots\snapshot_writer.py

# 1 from __future__ import annotations
# 2 
# 3 from datetime import datetime, timezone
# 4 from pathlib import Path
# 5 
# 6 from app.enums.agent_enums import AgentRole
# 7 
# 8 
# 9 class SnapshotWriter:
# 10     """Write before/after file snapshots for experiment traceability."""
# 11 
# 12     def __init__(self, root: str | Path = "experiments/artifacts/snapshots") -> None:
# 13         self.root = Path(root)
# 14         self.root.mkdir(parents=True, exist_ok=True)
# 15 
# 16     def write_snapshot(
# 17         self,
# 18         *,
# 19         experiment_id: str,
# 20         round: int,
# 21         file_path: str | Path,
# 22         before: str,
# 23         after: str,
# 24         symbol: str,
# 25         agent_role: AgentRole,
# 26     ) -> None:
# 27         if before == after:
# 28             return
# 29         ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%S%fZ")
# 30         dir_path = self.root / experiment_id / str(round)
# 31         dir_path.mkdir(parents=True, exist_ok=True)
# 32         base = Path(file_path).name
# 33         before_file = dir_path / f"{base}.{ts}.before"
# 34         after_file = dir_path / f"{base}.{ts}.after"
# 35         meta_file = dir_path / f"{base}.{ts}.meta"
# 36         before_file.write_text(before, encoding="utf-8")
# 37         after_file.write_text(after, encoding="utf-8")
# 38         meta_file.write_text(
# 39             f"symbol:{symbol}\nrole:{agent_role.value}\nexperiment:{experiment_id}\ntimestamp:{ts}\n",
# 40             encoding="utf-8",
# 41         )


# utilities\tools\__init__.py



# utilities\tools\black_runner.py

# 1 #  Placeholder for black runner tool


# utilities\tools\doc_formatter.py

# 1 #  Placeholder for documentation formatter


# utilities\tools\mypy_runner.py

# 1 #  Placeholder for mypy runner


# utilities\tools\radon_runner.py

# 1 import sys
# 2 import json
# 3 from radon.complexity import cc_visit
# 4 from pathlib import Path
# 5 
# 6 
# 7 def analyze_file(filepath):
# 8     with open(filepath, "r", encoding="utf-8") as file:
# 9         code = file.read()
# 10     complexity = cc_visit(code)
# 11     result = [
# 12         {"name": c.name, "complexity": c.complexity, "lineno": c.lineno}
# 13         for c in complexity
# 14     ]
# 15     return result
# 16 
# 17 
# 18 if __name__ == "__main__":
# 19     try:
# 20         if len(sys.argv) != 2:
# 21             raise ValueError("Provide exactly one file path.")
# 22         file_path = Path(sys.argv[1])
# 23         if not file_path.exists():
# 24             raise FileNotFoundError("File does not exist.")
# 25         analysis_result = analyze_file(str(file_path))
# 26         print(json.dumps({"result": analysis_result}, ensure_ascii=False))
# 27     except Exception as e:
# 28         print(json.dumps({"error": str(e)}, ensure_ascii=False))
# 29         sys.exit(1)


# utilities\tools\ruff_runner.py

# 1 #  Placeholder for ruff runner


# utilities\tools\sonarcloud_runner.py

# 1 #  Placeholder for sonarcloud runner


# visualization\__init__.py



# visualization\dashboard.py

# 1 from __future__ import annotations
# 2 
# 3 import sqlite3
# 4 from pathlib import Path
# 5 from typing import Dict
# 6 
# 7 import pandas as pd
# 8 import plotly.express as px
# 9 import plotly.graph_objects as go
# 10 
# 11 
# 12 _LOG_TABLES = {
# 13     "state": "state_log",
# 14     "transition": "state_transition_log",
# 15     "prompt": "prompt_log",
# 16     "scoring": "scoring_log",
# 17     "recommendation": "recommendation_log",
# 18 }
# 19 
# 20 
# 21 def load_logs(db_path: str | Path) -> Dict[str, pd.DataFrame]:
# 22     """Load structured logs from the experiment database."""
# 23     path = Path(db_path)
# 24     if not path.exists():
# 25         raise FileNotFoundError(path)
# 26     conn = sqlite3.connect(path)
# 27     logs: Dict[str, pd.DataFrame] = {}
# 28     for key, table in _LOG_TABLES.items():
# 29         try:
# 30             df = pd.read_sql_query(f"SELECT * FROM {table}", conn)
# 31         except Exception:
# 32             df = pd.DataFrame()
# 33         logs[key] = df
# 34     conn.close()
# 35     return logs
# 36 
# 37 
# 38 def build_state_transition_sankey(df: pd.DataFrame) -> go.Figure:
# 39     """Return a Sankey diagram for state transitions."""
# 40     if df.empty:
# 41         return go.Figure()
# 42     states = pd.unique(pd.concat([df["from_state"], df["to_state"]]))
# 43     indices = {state: i for i, state in enumerate(states)}
# 44     source = [indices[s] for s in df["from_state"]]
# 45     target = [indices[t] for t in df["to_state"]]
# 46     value = [1] * len(df)
# 47     return go.Figure(
# 48         go.Sankey(
# 49             node={"label": list(states)},
# 50             link={"source": source, "target": target, "value": value},
# 51         )
# 52     )
# 53 
# 54 
# 55 def build_scoring_line_plot(df: pd.DataFrame) -> go.Figure:
# 56     """Return a line chart for scoring metrics over rounds."""
# 57     if df.empty:
# 58         return go.Figure()
# 59     return px.line(df, x="round", y="value", color="metric")
# 60 
# 61 
# 62 def build_recommendation_bar_plot(df: pd.DataFrame) -> go.Figure:
# 63     """Return a bar plot summarizing recommendation counts per symbol."""
# 64     if df.empty:
# 65         return go.Figure()
# 66     counts = df.groupby("symbol").size().reset_index(name="count")
# 67     return px.bar(counts, x="symbol", y="count")



# === NON-PYTHON FILES ===

