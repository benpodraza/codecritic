
# === PYTHON FILES ===

# __init__.py



# abstract_classes\__init__.py



# abstract_classes\agent_base.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from abc import ABC, abstractmethod
# 5 
# 6 
# 7 class AgentBase(ABC):
# 8     """Base class for executing agent logic."""
# 9 
# 10     def __init__(self) -> None:
# 11         self.logger = logging.getLogger(self.__class__.__name__)
# 12 
# 13     def run(self, *args, **kwargs) -> None:
# 14         self.logger.debug("Agent run start")
# 15         self._run_agent_logic(*args, **kwargs)
# 16         self.logger.debug("Agent run end")
# 17 
# 18     @abstractmethod
# 19     def _run_agent_logic(self, *args, **kwargs) -> None:
# 20         """Execute agent specific logic."""
# 21         raise NotImplementedError


# abstract_classes\context_provider_base.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from abc import ABC, abstractmethod
# 5 
# 6 
# 7 class ContextProviderBase(ABC):
# 8     """Base class for providing context from symbol graphs."""
# 9 
# 10     def __init__(self) -> None:
# 11         self.logger = logging.getLogger(self.__class__.__name__)
# 12 
# 13     def get_context(self, *args, **kwargs):
# 14         self.logger.debug("Context retrieval start")
# 15         context = self._get_context(*args, **kwargs)
# 16         self.logger.debug("Context retrieval end")
# 17         return context
# 18 
# 19     @abstractmethod
# 20     def _get_context(self, *args, **kwargs):
# 21         """Return context information."""
# 22         raise NotImplementedError


# abstract_classes\experiment.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from typing import Any, Dict, List
# 5 
# 6 from ..factories.scoring_provider import ScoringProviderFactory
# 7 from ..factories.system_manager import SystemManagerFactory
# 8 from ..factories.experiment_config_provider import ExperimentConfigProvider
# 9 from ..utilities.db import init_db, insert_logs
# 10 from ..utilities.metadata.logging import ScoringLog
# 11 from ..utilities.metrics import EVALUATION_METRICS
# 12 
# 13 
# 14 class Experiment:
# 15     """Base experiment handling execution and scoring."""
# 16 
# 17     def __init__(self, config_id: int) -> None:
# 18         self.logger = logging.getLogger(self.__class__.__name__)
# 19         self.config_id = config_id
# 20         self.config: Dict[str, Any] | None = None
# 21         self.scoring_model_id = "dummy"
# 22 
# 23         self.evaluation_logs: List[Any] = []
# 24         self.code_quality_logs: List[Any] = []
# 25         self.conversation_logs: List[Any] = []
# 26         self.prompt_logs: List[Any] = []
# 27         self.state_logs: List[Any] = []
# 28         self.metrics: Dict[str, float] | None = None
# 29 
# 30     def run(self, *args, **kwargs) -> Dict[str, float]:
# 31         self.logger.debug("Experiment run start")
# 32 
# 33         #  load configuration
# 34         self.config = ExperimentConfigProvider.load(self.config_id)
# 35         self.scoring_model_id = self.config.get("scoring_model_id", "dummy")
# 36         system_manager_id = self.config.get("system_manager_id", "dummy")
# 37 
# 38         #  run system manager
# 39         manager = SystemManagerFactory.create(system_manager_id)
# 40         manager.run()
# 41 
# 42         #  collect logs from manager
# 43         self.evaluation_logs = getattr(manager, "evaluation_logs", [])
# 44         self.code_quality_logs = getattr(manager, "code_quality_logs", [])
# 45         self.conversation_logs = getattr(manager, "conversation_logs", [])
# 46         self.prompt_logs = getattr(manager, "prompt_logs", [])
# 47         self.state_logs = getattr(manager, "state_logs", [])
# 48 
# 49         logs = {
# 50             "evaluation": self.evaluation_logs,
# 51             "code_quality": self.code_quality_logs,
# 52             "conversation": self.conversation_logs,
# 53             "prompt": self.prompt_logs,
# 54             "state": self.state_logs,
# 55         }
# 56 
# 57         scoring_provider = ScoringProviderFactory.create(self.scoring_model_id)
# 58         metrics = scoring_provider.score(logs)
# 59         for key in EVALUATION_METRICS:
# 60             metrics.setdefault(key, 0.0)
# 61         self.logger.info("Experiment metrics: %s", metrics)
# 62 
# 63         #  persist scoring logs
# 64         conn = init_db()
# 65         scoring_logs = [
# 66             ScoringLog(
# 67                 experiment_id="exp",
# 68                 round=0,
# 69                 metric=k,
# 70                 value=v,
# 71             )
# 72             for k, v in metrics.items()
# 73         ]
# 74         insert_logs(conn, "scoring_log", scoring_logs)
# 75         conn.close()
# 76 
# 77         self.metrics = metrics
# 78         self.logger.debug("Experiment run end")
# 79         return metrics
# 80 
# 81     def _run_experiment_logic(self, *args, **kwargs) -> None:
# 82         """Override to implement experiment steps."""
# 83         raise NotImplementedError


# abstract_classes\prompt_generator_base.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from abc import ABC, abstractmethod
# 5 
# 6 
# 7 class PromptGeneratorBase(ABC):
# 8     """Base class for generating prompts."""
# 9 
# 10     def __init__(self) -> None:
# 11         self.logger = logging.getLogger(self.__class__.__name__)
# 12 
# 13     def generate_prompt(self, *args, **kwargs) -> str:
# 14         self.logger.debug("Prompt generation start")
# 15         prompt = self._generate_prompt(*args, **kwargs)
# 16         self.logger.debug("Prompt generation end")
# 17         return prompt
# 18 
# 19     @abstractmethod
# 20     def _generate_prompt(self, *args, **kwargs) -> str:
# 21         """Return a generated prompt."""
# 22         raise NotImplementedError


# abstract_classes\scoring_provider_base.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from abc import ABC, abstractmethod
# 5 
# 6 
# 7 class ScoringProviderBase(ABC):
# 8     """Base class for computing evaluation metrics."""
# 9 
# 10     def __init__(self) -> None:
# 11         self.logger = logging.getLogger(self.__class__.__name__)
# 12 
# 13     def score(self, *args, **kwargs):
# 14         self.logger.debug("Scoring start")
# 15         result = self._score(*args, **kwargs)
# 16         self.logger.debug("Scoring end")
# 17         return result
# 18 
# 19     @abstractmethod
# 20     def _score(self, *args, **kwargs):
# 21         """Compute evaluation metrics."""
# 22         raise NotImplementedError


# abstract_classes\state_manager_base.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from abc import ABC, abstractmethod
# 5 
# 6 
# 7 class StateManagerBase(ABC):
# 8     """Base class for managing state-level execution."""
# 9 
# 10     def __init__(self) -> None:
# 11         self.logger = logging.getLogger(self.__class__.__name__)
# 12 
# 13     def run(self, *args, **kwargs) -> None:
# 14         self.logger.debug("StateManager run start")
# 15         self._run_state_logic(*args, **kwargs)
# 16         self.logger.debug("StateManager run end")
# 17 
# 18     @abstractmethod
# 19     def _run_state_logic(self, *args, **kwargs) -> None:
# 20         """Execute state specific logic."""
# 21         raise NotImplementedError


# abstract_classes\system_manager_base.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from abc import ABC, abstractmethod
# 5 
# 6 
# 7 class SystemManagerBase(ABC):
# 8     """Base class for coordinating high level system logic."""
# 9 
# 10     def __init__(self) -> None:
# 11         self.logger = logging.getLogger(self.__class__.__name__)
# 12 
# 13     def run(self, *args, **kwargs) -> None:
# 14         self.logger.debug("SystemManager run start")
# 15         self._run_system_logic(*args, **kwargs)
# 16         self.logger.debug("SystemManager run end")
# 17 
# 18     @abstractmethod
# 19     def _run_system_logic(self, *args, **kwargs) -> None:
# 20         """Execute system-specific logic and state transitions."""
# 21         raise NotImplementedError


# abstract_classes\tool_provider_base.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from abc import ABC, abstractmethod
# 5 
# 6 
# 7 class ToolProviderBase(ABC):
# 8     """Base class for running external tools."""
# 9 
# 10     def __init__(self) -> None:
# 11         self.logger = logging.getLogger(self.__class__.__name__)
# 12 
# 13     def run(self, *args, **kwargs):
# 14         self.logger.debug("Tool run start")
# 15         result = self._run(*args, **kwargs)
# 16         self.logger.debug("Tool run end")
# 17         return result
# 18 
# 19     @abstractmethod
# 20     def _run(self, *args, **kwargs):
# 21         """Run tool-specific logic."""
# 22         raise NotImplementedError


# enums\__init__.py



# enums\agent_enums.py

# 1 from __future__ import annotations
# 2 
# 3 from enum import Enum
# 4 
# 5 
# 6 class AgentRole(Enum):
# 7     CRITIC = "critic"
# 8     TESTER = "tester"
# 9     FIXER = "fixer"


# enums\system_enums.py

# 1 from __future__ import annotations
# 2 
# 3 from enum import Enum
# 4 
# 5 
# 6 class SystemState(Enum):
# 7     INIT = "init"
# 8     RUNNING = "running"
# 9     COMPLETE = "complete"
# 10 
# 11 
# 12 class FSMState(Enum):
# 13     """Finite state machine states for the CodeCritic system."""
# 14 
# 15     START = "start"
# 16     GENERATE = "generate"
# 17     DISCRIMINATE = "discriminate"
# 18     MEDIATE = "mediate"
# 19     PATCHOR = "patchor"
# 20     RECOMMENDER = "recommender"
# 21     END = "end"


# extensions\__init__.py



# extensions\agent_prompts\__init__.py

# 1 


# extensions\agents\__init__.py

# 1 from .dummy_agent import DummyAgent
# 2 from .generator_agent import GeneratorAgent
# 3 from .evaluator_agent import EvaluatorAgent
# 4 from ...registries.agents import AGENT_REGISTRY
# 5 
# 6 AGENT_REGISTRY.register("dummy", DummyAgent)
# 7 AGENT_REGISTRY.register("generator", GeneratorAgent)
# 8 AGENT_REGISTRY.register("evaluator", EvaluatorAgent)
# 9 
# 10 __all__ = ["DummyAgent", "GeneratorAgent", "EvaluatorAgent"]


# extensions\agents\dummy_agent.py

# 1 from ...abstract_classes.agent_base import AgentBase
# 2 
# 3 
# 4 class DummyAgent(AgentBase):
# 5     def _run_agent_logic(self, *args, **kwargs) -> None:
# 6         self.logger.info("Dummy agent logic executed")


# extensions\agents\evaluator_agent.py

# 1 from __future__ import annotations
# 2 from typing import List
# 3 from ...abstract_classes.agent_base import AgentBase
# 4 from ...factories.tool_provider import ToolProviderFactory
# 5 from ...utilities.metadata.logging import CodeQualityLog, ErrorLog
# 6 
# 7 
# 8 class EvaluatorAgent(AgentBase):
# 9     def __init__(self, target: str) -> None:
# 10         super().__init__()
# 11         self.target = target
# 12         self.mypy = ToolProviderFactory.create("mypy")
# 13         self.ruff = ToolProviderFactory.create("ruff")
# 14         self.radon = ToolProviderFactory.create("radon")
# 15         self.quality_logs: List[CodeQualityLog] = []
# 16         self.error_logs: List[ErrorLog] = []
# 17 
# 18     def _run_agent_logic(self, *args, **kwargs) -> None:
# 19         complexity = 0.0
# 20         try:
# 21             self.mypy.run(self.target)
# 22             ruff_proc = self.ruff.run(self.target)
# 23             radon_proc = self.radon.run(self.target)
# 24 
# 25             #  Parse "Complexity: X.Y" from radon output
# 26             for line in radon_proc.stdout.splitlines():
# 27                 if "Complexity:" in line:
# 28                     try:
# 29                         complexity = float(line.split("Complexity:")[1].strip())
# 30                     except ValueError:
# 31                         pass
# 32                     break
# 33 
# 34         except Exception as exc:
# 35             self.logger.warning("Radon unavailable: %s", exc)
# 36 
# 37         lines = sum(1 for _ in open(self.target, "r", encoding="utf-8"))
# 38         log = CodeQualityLog(
# 39             experiment_id="exp",
# 40             round=0,
# 41             symbol=self.target,
# 42             lines_of_code=lines,
# 43             cyclomatic_complexity=complexity,
# 44             maintainability_index=0.0,
# 45             lint_errors=0 if ruff_proc.returncode == 0 else 1,
# 46         )
# 47         self.quality_logs.append(log)


# extensions\agents\generator_agent.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import List
# 4 from datetime import datetime, timezone
# 5 
# 6 from ...abstract_classes.agent_base import AgentBase
# 7 from ...enums.agent_enums import AgentRole
# 8 from ...factories.tool_provider import ToolProviderFactory
# 9 from ...utilities.metadata.logging import ErrorLog, PromptLog
# 10 
# 11 
# 12 class GeneratorAgent(AgentBase):
# 13     """Agent responsible for formatting code using black."""
# 14 
# 15     def __init__(self, target: str) -> None:
# 16         super().__init__()
# 17         self.target = target
# 18         self.formatter = ToolProviderFactory.create("black")
# 19         self.prompt_logs: List[PromptLog] = []
# 20         self.error_logs: List[ErrorLog] = []
# 21 
# 22     def _run_agent_logic(
# 23         self, *args, **kwargs
# 24     ) -> None:  #  pragma: no cover - simple logging
# 25         log = PromptLog(
# 26             experiment_id="exp",
# 27             round=0,
# 28             system="system",
# 29             agent_id="generator",
# 30             agent_role=AgentRole.FIXER.value,
# 31             symbol=self.target,
# 32             prompt="format code",
# 33             response=None,
# 34             attempt_number=0,
# 35             agent_action_outcome="started",
# 36         )
# 37         self.prompt_logs.append(log)
# 38         try:
# 39             self.formatter.run(self.target)
# 40             log.agent_action_outcome = "success"
# 41             self.logger.info("Formatted %s", self.target)
# 42         except Exception as exc:  #  pragma: no cover - safety
# 43             log.agent_action_outcome = "error"
# 44             err = ErrorLog(
# 45                 experiment_id="exp",
# 46                 round=0,
# 47                 error_type=type(exc).__name__,
# 48                 message=str(exc),
# 49                 file_path=self.target,
# 50             )
# 51             self.error_logs.append(err)
# 52             self.logger.exception("Formatting failed for %s", self.target)
# 53         finally:
# 54             log.stop = datetime.now(timezone.utc)


# extensions\context_providers\__init__.py

# 1 from .dummy_context_provider import DummyContextProvider
# 2 from .symbol_graph_provider import SymbolGraphProvider
# 3 from ...registries.context_providers import CONTEXT_PROVIDER_REGISTRY
# 4 
# 5 CONTEXT_PROVIDER_REGISTRY.register("dummy", DummyContextProvider)
# 6 CONTEXT_PROVIDER_REGISTRY.register("symbol_graph", SymbolGraphProvider)
# 7 
# 8 __all__ = ["DummyContextProvider", "SymbolGraphProvider"]


# extensions\context_providers\dummy_context_provider.py

# 1 from ...abstract_classes.context_provider_base import ContextProviderBase
# 2 
# 3 
# 4 class DummyContextProvider(ContextProviderBase):
# 5     def _get_context(self, *args, **kwargs):
# 6         self.logger.info("Dummy context provided")
# 7         return {}


# extensions\context_providers\symbol_graph_provider.py

# 1 from __future__ import annotations
# 2 
# 3 import ast
# 4 from pathlib import Path
# 5 from typing import Any, Dict, List, Set
# 6 
# 7 from ...abstract_classes.context_provider_base import ContextProviderBase
# 8 
# 9 
# 10 class SymbolGraphProvider(ContextProviderBase):
# 11     """Generate a simple symbol graph for a Python module."""
# 12 
# 13     def __init__(self, module_path: str) -> None:
# 14         super().__init__()
# 15         self.module_path = Path(module_path)
# 16 
# 17     def _get_context(self) -> Dict[str, Any]:
# 18         """Parse the module and return context information."""
# 19         if not self.module_path.exists():
# 20             self.logger.error("Module not found: %s", self.module_path)
# 21             return {}
# 22 
# 23         source = self.module_path.read_text(encoding="utf-8")
# 24         tree = ast.parse(source)
# 25 
# 26         functions: List[str] = []
# 27         classes: Dict[str, List[str]] = {}
# 28         call_map: Dict[str, Set[str]] = {}
# 29 
# 30         for node in tree.body:
# 31             if isinstance(node, ast.FunctionDef):
# 32                 sig = self._format_signature(node)
# 33                 functions.append(sig)
# 34                 call_map[node.name] = self._collect_calls(node)
# 35             elif isinstance(node, ast.ClassDef):
# 36                 method_sigs: List[str] = []
# 37                 for item in node.body:
# 38                     if isinstance(item, ast.FunctionDef):
# 39                         sig = self._format_signature(item)
# 40                         method_sigs.append(sig)
# 41                         key = f"{node.name}.{item.name}"
# 42                         call_map[key] = self._collect_calls(item)
# 43                 classes[node.name] = method_sigs
# 44 
# 45         context = {
# 46             "functions": functions,
# 47             "classes": classes,
# 48             "call_map": {k: sorted(v) for k, v in call_map.items()},
# 49         }
# 50         self.logger.info("Context generated for %s", self.module_path)
# 51         return context
# 52 
# 53     def _format_signature(self, func: ast.FunctionDef) -> str:
# 54         args = [arg.arg for arg in func.args.args]
# 55         if func.args.vararg:
# 56             args.append("*" + func.args.vararg.arg)
# 57         for kw in func.args.kwonlyargs:
# 58             args.append(kw.arg)
# 59         if func.args.kwarg:
# 60             args.append("**" + func.args.kwarg.arg)
# 61         return f"{func.name}({', '.join(args)})"
# 62 
# 63     def _collect_calls(self, func: ast.FunctionDef) -> Set[str]:
# 64         calls: Set[str] = set()
# 65         for node in ast.walk(func):
# 66             if isinstance(node, ast.Call):
# 67                 if isinstance(node.func, ast.Name):
# 68                     calls.add(node.func.id)
# 69                 elif isinstance(node.func, ast.Attribute):
# 70                     calls.add(node.func.attr)
# 71         return calls


# extensions\prompt_generators\__init__.py

# 1 from .dummy_prompt_generator import DummyPromptGenerator
# 2 from .basic_prompt_generator import BasicPromptGenerator
# 3 from ...registries.prompt_generators import PROMPT_GENERATOR_REGISTRY
# 4 
# 5 PROMPT_GENERATOR_REGISTRY.register("dummy", DummyPromptGenerator)
# 6 PROMPT_GENERATOR_REGISTRY.register("basic", BasicPromptGenerator)
# 7 
# 8 __all__ = ["DummyPromptGenerator", "BasicPromptGenerator"]


# extensions\prompt_generators\basic_prompt_generator.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Any, Dict
# 4 
# 5 from ...abstract_classes.prompt_generator_base import PromptGeneratorBase
# 6 from ...abstract_classes.context_provider_base import ContextProviderBase
# 7 
# 8 
# 9 class BasicPromptGenerator(PromptGeneratorBase):
# 10     """Combine system and agent templates with code context."""
# 11 
# 12     def __init__(self, context_provider: ContextProviderBase) -> None:
# 13         super().__init__()
# 14         self.context_provider = context_provider
# 15 
# 16     def _generate_prompt(
# 17         self, agent_config: Dict[str, Any], system_config: Dict[str, Any]
# 18     ) -> str:
# 19         context = self.context_provider.get_context()
# 20         self.logger.debug("Raw context: %s", context)
# 21 
# 22         system_template = system_config.get("template", "")
# 23         agent_template = agent_config.get("template", "")
# 24 
# 25         context_snippet = ""
# 26         if context:
# 27             funcs = ", ".join(context.get("functions", []))
# 28             classes = ", ".join(context.get("classes", {}).keys())
# 29             context_snippet = f"Functions: {funcs}\nClasses: {classes}"
# 30 
# 31         prompt = f"{system_template}\n\n{agent_template}\n\n{context_snippet}"
# 32         self.logger.info("Prompt generated")
# 33         self.logger.debug("Prompt content: %s", prompt)
# 34         return prompt


# extensions\prompt_generators\dummy_prompt_generator.py

# 1 from ...abstract_classes.prompt_generator_base import PromptGeneratorBase
# 2 
# 3 
# 4 class DummyPromptGenerator(PromptGeneratorBase):
# 5     def _generate_prompt(self, *args, **kwargs) -> str:
# 6         self.logger.info("Dummy prompt generated")
# 7         return "dummy prompt"


# extensions\scoring_models\__init__.py

# 1 from .dummy_scoring_provider import DummyScoringProvider
# 2 from .basic_scoring_provider import BasicScoringProvider
# 3 from ...registries.scoring_models import SCORING_MODEL_REGISTRY
# 4 
# 5 SCORING_MODEL_REGISTRY.register("dummy", DummyScoringProvider)
# 6 SCORING_MODEL_REGISTRY.register("basic", BasicScoringProvider)
# 7 
# 8 __all__ = ["DummyScoringProvider", "BasicScoringProvider"]


# extensions\scoring_models\basic_scoring_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Any, Dict, List
# 4 
# 5 from ...abstract_classes.scoring_provider_base import ScoringProviderBase
# 6 from ...utilities.metrics import compute_metrics
# 7 
# 8 
# 9 class BasicScoringProvider(ScoringProviderBase):
# 10     """Compute experiment metrics using basic heuristics."""
# 11 
# 12     def _score(self, logs: Dict[str, List[Any]]) -> Dict[str, float]:
# 13         self.logger.debug("Scoring with logs: %s", logs)
# 14         evaluation_logs = logs.get("evaluation", [])
# 15         code_quality_logs = logs.get("code_quality", [])
# 16         conversation_logs = logs.get("conversation", [])
# 17         prompt_logs = logs.get("prompt", [])
# 18         state_logs = logs.get("state", [])
# 19 
# 20         metrics = compute_metrics(
# 21             evaluation_logs,
# 22             code_quality_logs,
# 23             conversation_logs,
# 24             prompt_logs,
# 25             state_logs,
# 26         )
# 27         self.logger.info("Computed metrics: %s", metrics)
# 28         return metrics


# extensions\scoring_models\dummy_scoring_provider.py

# 1 from ...abstract_classes.scoring_provider_base import ScoringProviderBase
# 2 
# 3 
# 4 class DummyScoringProvider(ScoringProviderBase):
# 5     def _score(self, *args, **kwargs):
# 6         self.logger.info("Dummy scoring")
# 7         return 0


# extensions\state_managers\__init__.py

# 1 from .dummy_state_manager import DummyStateManager
# 2 from .state_manager import StateManager
# 3 from ...registries.state_managers import STATE_MANAGER_REGISTRY
# 4 
# 5 STATE_MANAGER_REGISTRY.register("dummy", DummyStateManager)
# 6 STATE_MANAGER_REGISTRY.register("state", StateManager)
# 7 
# 8 __all__ = ["DummyStateManager", "StateManager"]


# extensions\state_managers\dummy_state_manager.py

# 1 from ...abstract_classes.state_manager_base import StateManagerBase
# 2 
# 3 
# 4 class DummyStateManager(StateManagerBase):
# 5     def _run_state_logic(self, *args, **kwargs) -> None:
# 6         self.logger.info("Dummy state logic executed")


# extensions\state_managers\state_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ...abstract_classes.state_manager_base import StateManagerBase
# 4 
# 5 
# 6 class StateManager(StateManagerBase):
# 7     """Concrete manager executing actions within each FSM state."""
# 8 
# 9     def _run_state_logic(self, *args, **kwargs) -> None:
# 10         state = kwargs.get("state")
# 11         self.logger.info("Running state logic for %s", state)


# extensions\system_managers\__init__.py

# 1 from .dummy_system_manager import DummySystemManager
# 2 from .system_manager import SystemManager
# 3 from ...registries.system_managers import SYSTEM_MANAGER_REGISTRY
# 4 
# 5 SYSTEM_MANAGER_REGISTRY.register("dummy", DummySystemManager)
# 6 SYSTEM_MANAGER_REGISTRY.register("system", SystemManager)
# 7 
# 8 __all__ = ["DummySystemManager", "SystemManager"]


# extensions\system_managers\dummy_system_manager.py

# 1 from ...abstract_classes.system_manager_base import SystemManagerBase
# 2 
# 3 
# 4 class DummySystemManager(SystemManagerBase):
# 5     def _run_system_logic(self, *args, **kwargs) -> None:
# 6         self.logger.info("Dummy system logic executed")


# extensions\system_managers\system_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import List
# 4 
# 5 from ...factories.agent import AgentFactory
# 6 from ...factories.context_provider import ContextProviderFactory
# 7 from ...factories.prompt_manager import PromptGeneratorFactory
# 8 from ...utilities.db import init_db, insert_logs
# 9 from ...utilities.metadata.logging import (
# 10     CodeQualityLog,
# 11     PromptLog,
# 12     ScoringLog,
# 13     StateLog,
# 14     StateTransitionLog,
# 15 )
# 16 
# 17 from ...abstract_classes.system_manager_base import SystemManagerBase
# 18 from ...enums.system_enums import FSMState
# 19 from ..state_managers.state_manager import StateManager
# 20 
# 21 
# 22 class SystemManager(SystemManagerBase):
# 23     """Concrete system manager implementing FSM transitions."""
# 24 
# 25     def __init__(self) -> None:
# 26         super().__init__()
# 27         self.state_manager = StateManager()
# 28         self.current_state = FSMState.START
# 29         self.transition_logs: List[StateTransitionLog] = []
# 30         self.state_logs: List[StateLog] = []
# 31         self.prompt_logs: List[PromptLog] = []
# 32         self.code_quality_logs: List[CodeQualityLog] = []
# 33         self.scoring_logs: List[ScoringLog] = []
# 34 
# 35         self.context_provider = ContextProviderFactory.create(
# 36             "symbol_graph", module_path="sample_module.py"
# 37         )
# 38         self.prompt_generator = PromptGeneratorFactory.create(
# 39             "basic", context_provider=self.context_provider
# 40         )
# 41         self.generator = AgentFactory.create("generator", target="sample_module.py")
# 42         self.evaluator = AgentFactory.create("evaluator", target="sample_module.py")
# 43 
# 44     def _transition_to(self, next_state: FSMState, reason: str | None = None) -> None:
# 45         log = StateTransitionLog(
# 46             experiment_id="exp",
# 47             round=0,
# 48             from_state=self.current_state.value,
# 49             to_state=next_state.value,
# 50             reason=reason,
# 51         )
# 52         self.transition_logs.append(log)
# 53         self.logger.info("%s -> %s", self.current_state.value, next_state.value)
# 54         self.current_state = next_state
# 55 
# 56     def _run_system_logic(self, *args, **kwargs) -> None:
# 57         sequence = [
# 58             FSMState.GENERATE,
# 59             FSMState.DISCRIMINATE,
# 60             FSMState.MEDIATE,
# 61             FSMState.PATCHOR,
# 62             FSMState.RECOMMENDER,
# 63             FSMState.END,
# 64         ]
# 65         conn = init_db()
# 66         for state in sequence:
# 67             self._transition_to(state)
# 68             if state is not FSMState.END:
# 69                 self.state_manager.run(state=state.value)
# 70                 self.state_logs.append(
# 71                     StateLog(
# 72                         experiment_id="exp",
# 73                         system="system",
# 74                         round=0,
# 75                         state=state.value,
# 76                         action="run",
# 77                     )
# 78                 )
# 79                 if state is FSMState.GENERATE:
# 80                     self.generator.run()
# 81                     self.prompt_logs.extend(self.generator.prompt_logs)
# 82                 elif state is FSMState.DISCRIMINATE:
# 83                     self.evaluator.run()
# 84                     self.code_quality_logs.extend(self.evaluator.quality_logs)
# 85 
# 86         insert_logs(conn, "state_transition_log", self.transition_logs)
# 87         insert_logs(conn, "state_log", self.state_logs)
# 88         insert_logs(conn, "prompt_log", self.prompt_logs)
# 89         insert_logs(conn, "code_quality_log", self.code_quality_logs)
# 90         if self.scoring_logs:
# 91             insert_logs(conn, "scoring_log", self.scoring_logs)
# 92         conn.close()


# extensions\system_prompts\__init__.py

# 1 


# extensions\tool_providers\__init__.py

# 1 from .dummy_tool_provider import DummyToolProvider
# 2 from .black_runner import BlackToolProvider
# 3 from .mypy_runner import MypyToolProvider
# 4 from .radon_runner import RadonToolProvider
# 5 from .ruff_runner import RuffToolProvider
# 6 from ...registries.tool_providers import TOOL_PROVIDER_REGISTRY
# 7 
# 8 TOOL_PROVIDER_REGISTRY.register("dummy", DummyToolProvider)
# 9 TOOL_PROVIDER_REGISTRY.register("black", BlackToolProvider)
# 10 TOOL_PROVIDER_REGISTRY.register("mypy", MypyToolProvider)
# 11 TOOL_PROVIDER_REGISTRY.register("radon", RadonToolProvider)
# 12 TOOL_PROVIDER_REGISTRY.register("ruff", RuffToolProvider)
# 13 
# 14 __all__ = [
# 15     "DummyToolProvider",
# 16     "BlackToolProvider",
# 17     "MypyToolProvider",
# 18     "RadonToolProvider",
# 19     "RuffToolProvider",
# 20 ]


# extensions\tool_providers\black_runner.py

# 1 from __future__ import annotations
# 2 import subprocess
# 3 import sys
# 4 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 5 
# 6 
# 7 class BlackToolProvider(ToolProviderBase):
# 8     def _run(self, target: str, check: bool = False):
# 9         #  Use quiet mode to suppress emojis and extra output on Windows
# 10         cmd = [sys.executable, "-m", "black", "--quiet", target]
# 11         if check:
# 12             cmd.append("--check")
# 13         proc = subprocess.run(
# 14             cmd, capture_output=True, text=True, encoding="utf-8", errors="ignore"
# 15         )
# 16         if proc.stdout:
# 17             self.logger.debug(proc.stdout)
# 18         if proc.stderr:
# 19             self.logger.error(proc.stderr)
# 20         if proc.returncode != 0:
# 21             raise RuntimeError(f"black failed: {proc.stderr}")
# 22         return proc


# extensions\tool_providers\docformetter_runner.py

# 1 import subprocess
# 2 import sys
# 3 from app.abstract_classes.tool_provider_base import ToolProviderBase
# 4 
# 5 
# 6 class DocFormatterToolProvider(ToolProviderBase):
# 7     def _run(self, target: str, check: bool = False):
# 8         cmd = [sys.executable, "-m", "docformatter", target, "--in-place"]
# 9         if check:
# 10             cmd.append("--check")
# 11         proc = subprocess.run(cmd, capture_output=True, text=True)
# 12         if proc.stdout:
# 13             self.logger.debug(proc.stdout)
# 14         if proc.stderr:
# 15             self.logger.error(proc.stderr)
# 16         if proc.returncode != 0:
# 17             raise RuntimeError(f"docformatter failed: {proc.stderr}")
# 18         return proc


# extensions\tool_providers\dummy_tool_provider.py

# 1 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 2 
# 3 
# 4 class DummyToolProvider(ToolProviderBase):
# 5     def _run(self, *args, **kwargs):
# 6         self.logger.info("Dummy tool run")
# 7         return True


# extensions\tool_providers\mypy_runner.py

# 1 from __future__ import annotations
# 2 
# 3 import subprocess
# 4 import sys
# 5 
# 6 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 7 
# 8 
# 9 class MypyToolProvider(ToolProviderBase):
# 10     def _run(self, target: str):
# 11         cmd = [sys.executable, "-m", "mypy", target]
# 12         proc = subprocess.run(cmd, capture_output=True, text=True)
# 13         if proc.stdout:
# 14             self.logger.debug(proc.stdout)
# 15         if proc.stderr:
# 16             self.logger.error(proc.stderr)
# 17         if proc.returncode not in (0, 1):  #  mypy returns 1 if it finds issues
# 18             raise RuntimeError(f"mypy execution error: {proc.stderr}")
# 19         return proc


# extensions\tool_providers\radon_runner.py

# 1 from __future__ import annotations
# 2 import subprocess
# 3 import sys
# 4 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 5 
# 6 
# 7 class RadonToolProvider(ToolProviderBase):
# 8     def _run(self, target: str):
# 9         #  Call radon via -m so it returns a CompletedProcess
# 10         cmd = [sys.executable, "-m", "radon", "cc", target]
# 11         proc = subprocess.run(cmd, capture_output=True, text=True)
# 12 
# 13         if proc.stdout:
# 14             self.logger.debug(proc.stdout)
# 15         if proc.stderr:
# 16             self.logger.error(proc.stderr)
# 17 
# 18         if proc.returncode != 0:
# 19             raise RuntimeError(f"radon failed: {proc.stderr.strip()}")
# 20 
# 21         return proc


# extensions\tool_providers\ruff_runner.py

# 1 from __future__ import annotations
# 2 
# 3 import subprocess
# 4 import sys
# 5 
# 6 from ...abstract_classes.tool_provider_base import ToolProviderBase
# 7 
# 8 
# 9 class RuffToolProvider(ToolProviderBase):
# 10     def _run(self, target: str):
# 11         cmd = [sys.executable, "-m", "ruff", "check", target]
# 12         proc = subprocess.run(cmd, capture_output=True, text=True)
# 13         if proc.stdout:
# 14             self.logger.debug(proc.stdout)
# 15         if proc.stderr:
# 16             self.logger.error(proc.stderr)
# 17         if proc.returncode != 0:
# 18             raise RuntimeError(f"ruff failed: {proc.stderr}")
# 19         return proc


# extensions\tool_providers\sonarcloud_runner.py

# 1 from __future__ import annotations
# 2 
# 3 from app.abstract_classes.tool_provider_base import ToolProviderBase
# 4 
# 5 
# 6 class SonarCloudToolProvider(ToolProviderBase):
# 7     def _run(self, project_key: str, organization: str, token: str):
# 8         self.logger.info(
# 9             "SonarCloud stub executed for project '%s' in organization '%s'.",
# 10             project_key,
# 11             organization,
# 12         )
# 13         return {
# 14             "status": "success",
# 15             "project_key": project_key,
# 16             "organization": organization,
# 17             "analysis_url": "https://sonarcloud.io/dashboard?id=" + project_key,
# 18         }


# extensions\tools\__init__.py



# factories\__init__.py



# factories\agent.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.agents import AGENT_REGISTRY
# 4 
# 5 
# 6 class AgentFactory:
# 7     @staticmethod
# 8     def create(name: str, **kwargs):
# 9         cls = AGENT_REGISTRY.get(name)
# 10         if cls is None:
# 11             raise KeyError(f"Agent {name} not registered")
# 12         return cls(**kwargs)


# factories\context_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.context_providers import CONTEXT_PROVIDER_REGISTRY
# 4 
# 5 
# 6 class ContextProviderFactory:
# 7     @staticmethod
# 8     def create(name: str, **kwargs):
# 9         cls = CONTEXT_PROVIDER_REGISTRY.get(name)
# 10         if cls is None:
# 11             raise KeyError(f"Context provider {name} not registered")
# 12         return cls(**kwargs)


# factories\experiment_config_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Dict, Any
# 4 
# 5 _CONFIGS: Dict[int, Dict[str, Any]] = {}
# 6 
# 7 
# 8 class ExperimentConfigProvider:
# 9     """Simple provider storing experiment configurations in memory."""
# 10 
# 11     @staticmethod
# 12     def register(config_id: int, config: Dict[str, Any]) -> None:
# 13         _CONFIGS[config_id] = config
# 14 
# 15     @staticmethod
# 16     def load(config_id: int) -> Dict[str, Any]:
# 17         config = _CONFIGS.get(config_id)
# 18         if config is None:
# 19             raise KeyError(f"experiment config {config_id} not found")
# 20         return config


# factories\prompt_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.prompt_generators import PROMPT_GENERATOR_REGISTRY
# 4 
# 5 
# 6 class PromptGeneratorFactory:
# 7     @staticmethod
# 8     def create(name: str, **kwargs):
# 9         cls = PROMPT_GENERATOR_REGISTRY.get(name)
# 10         if cls is None:
# 11             raise KeyError(f"Prompt generator {name} not registered")
# 12         return cls(**kwargs)


# factories\scoring_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.scoring_models import SCORING_MODEL_REGISTRY
# 4 
# 5 
# 6 class ScoringProviderFactory:
# 7     @staticmethod
# 8     def create(name: str, **kwargs):
# 9         cls = SCORING_MODEL_REGISTRY.get(name)
# 10         if cls is None:
# 11             raise KeyError(f"Scoring provider {name} not registered")
# 12         return cls(**kwargs)


# factories\state_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.state_managers import STATE_MANAGER_REGISTRY
# 4 
# 5 
# 6 class StateManagerFactory:
# 7     @staticmethod
# 8     def create(name: str, **kwargs):
# 9         cls = STATE_MANAGER_REGISTRY.get(name)
# 10         if cls is None:
# 11             raise KeyError(f"State manager {name} not registered")
# 12         return cls(**kwargs)


# factories\system_config_provider.py

# 1 from __future__ import annotations
# 2 
# 3 
# 4 class SystemConfigProvider:
# 5     @staticmethod
# 6     def create(config: dict):
# 7         return config


# factories\system_manager.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.system_managers import SYSTEM_MANAGER_REGISTRY
# 4 
# 5 
# 6 class SystemManagerFactory:
# 7     @staticmethod
# 8     def create(name: str, **kwargs):
# 9         cls = SYSTEM_MANAGER_REGISTRY.get(name)
# 10         if cls is None:
# 11             raise KeyError(f"System manager {name} not registered")
# 12         return cls(**kwargs)


# factories\tool_provider.py

# 1 from __future__ import annotations
# 2 
# 3 from ..registries.tool_providers import TOOL_PROVIDER_REGISTRY
# 4 
# 5 
# 6 class ToolProviderFactory:
# 7     @staticmethod
# 8     def create(name: str, **kwargs):
# 9         cls = TOOL_PROVIDER_REGISTRY.get(name)
# 10         if cls is None:
# 11             raise KeyError(f"Tool provider {name} not registered")
# 12         return cls(**kwargs)


# registries\__init__.py

# 1 class Registry:
# 2     def __init__(self):
# 3         self._registry = {}
# 4 
# 5     def register(self, name: str, item):
# 6         existing = self._registry.get(name)
# 7         if existing is not None:
# 8             if existing is item:
# 9                 return
# 10             raise KeyError(f"{name} already registered")
# 11         self._registry[name] = item
# 12 
# 13     def get(self, name: str):
# 14         return self._registry.get(name)
# 15 
# 16     def all(self):
# 17         return dict(self._registry)


# registries\agent_engines\__init__.py

# 1 from .. import Registry
# 2 
# 3 AGENT_ENGINE_REGISTRY = Registry()


# registries\agent_prompts\__init__.py

# 1 from .. import Registry
# 2 
# 3 AGENT_PROMPT_REGISTRY = Registry()


# registries\agents\__init__.py

# 1 from .. import Registry
# 2 
# 3 AGENT_REGISTRY = Registry()


# registries\context_providers\__init__.py

# 1 from .. import Registry
# 2 
# 3 CONTEXT_PROVIDER_REGISTRY = Registry()


# registries\prompt_generators\__init__.py

# 1 from .. import Registry
# 2 
# 3 PROMPT_GENERATOR_REGISTRY = Registry()


# registries\scoring_models\__init__.py

# 1 from .. import Registry
# 2 
# 3 SCORING_MODEL_REGISTRY = Registry()


# registries\state_managers\__init__.py

# 1 from .. import Registry
# 2 
# 3 STATE_MANAGER_REGISTRY = Registry()


# registries\system_managers\__init__.py

# 1 from .. import Registry
# 2 
# 3 SYSTEM_MANAGER_REGISTRY = Registry()


# registries\system_prompts\__init__.py

# 1 from .. import Registry
# 2 
# 3 SYSTEM_PROMPT_REGISTRY = Registry()


# registries\tool_providers\__init__.py

# 1 from .. import Registry
# 2 
# 3 TOOL_PROVIDER_REGISTRY = Registry()


# registries\tools\__init__.py

# 1 from .. import Registry
# 2 
# 3 TOOL_REGISTRY = Registry()


# schemas\__init__.py

# 1 from .agent_engine_schema import AgentEngine
# 2 from .agent_prompt_schema import AgentPrompt
# 3 from .system_prompt_schema import SystemPrompt
# 4 from .context_provider_schema import ContextProvider
# 5 from .tooling_provider_schema import ToolingProvider
# 6 from .file_path_schema import FilePath
# 7 from .agent_config_schema import AgentConfig
# 8 from .prompt_generator_schema import PromptGenerator
# 9 from .scoring_provider_schema import ScoringProvider
# 10 from .state_manager_schema import StateManager
# 11 from .system_config_schema import SystemConfig
# 12 from .experiment_config_schema import ExperimentConfig
# 13 from .series_schema import Series
# 14 
# 15 __all__ = [
# 16     "AgentEngine",
# 17     "AgentPrompt",
# 18     "SystemPrompt",
# 19     "ContextProvider",
# 20     "ToolingProvider",
# 21     "FilePath",
# 22     "AgentConfig",
# 23     "PromptGenerator",
# 24     "ScoringProvider",
# 25     "StateManager",
# 26     "SystemConfig",
# 27     "ExperimentConfig",
# 28     "Series",
# 29 ]


# schemas\agent_config_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator, model_validator
# 8 
# 9 from .agent_engine_schema import AgentEngine
# 10 from .prompt_generator_schema import PromptGenerator
# 11 
# 12 
# 13 class AgentConfig(BaseModel):
# 14     """Schema for the agent_config table."""
# 15 
# 16     id: Optional[int] = None
# 17     name: str
# 18     description: Optional[str] = None
# 19     agent_role: Optional[str] = None
# 20     system_type: Optional[str] = None
# 21     agent_engine_id: Optional[int] = None
# 22     prompt_generator_id: Optional[int] = None
# 23     agent_engine: Optional[AgentEngine] = None
# 24     prompt_generator: Optional[PromptGenerator] = None
# 25     artifact_path: Optional[Path] = None
# 26 
# 27     table_name: str = "agent_config"
# 28 
# 29     @model_validator(mode="after")
# 30     def _sync_ids(self) -> "AgentConfig":
# 31         if self.agent_engine is not None and self.agent_engine_id is None:
# 32             self.agent_engine_id = getattr(self.agent_engine, "id", None)
# 33         if self.prompt_generator is not None and self.prompt_generator_id is None:
# 34             self.prompt_generator_id = getattr(self.prompt_generator, "id", None)
# 35         return self
# 36 
# 37     @field_validator("artifact_path")
# 38     @classmethod
# 39     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 40         if v is None:
# 41             return v
# 42         p = Path(v)
# 43         if not p.is_absolute() and ".." in p.parts:
# 44             raise ValueError("artifact_path must be absolute or project relative")
# 45         return p
# 46 
# 47     def model_dump(self) -> dict:  #  pragma: no cover - simple wrapper
# 48         data = getattr(super(), "model_dump", super().dict)()
# 49         for field in ("agent_engine", "prompt_generator"):
# 50             if data.get(field) is not None:
# 51                 data[field] = data[field].model_dump()
# 52         if data.get("artifact_path") is not None:
# 53             data["artifact_path"] = str(data["artifact_path"])
# 54         return data


# schemas\agent_engine_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class AgentEngine(BaseModel):
# 11     """Schema for the agent_engine table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     model: Optional[str] = None
# 17     engine_config: Optional[str] = None
# 18     artifact_path: Optional[Path] = None
# 19 
# 20     table_name: str = "agent_engine"
# 21 
# 22     @field_validator("artifact_path")
# 23     @classmethod
# 24     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 25         if v is None:
# 26             return v
# 27         p = Path(v)
# 28         if not p.is_absolute() and ".." in p.parts:
# 29             raise ValueError("artifact_path must be absolute or project relative")
# 30         return p
# 31 
# 32     def model_dump(self) -> dict:  #  pragma: no cover - simple wrapper
# 33         base_dump = getattr(super(), "model_dump", super().dict)
# 34         data = base_dump()
# 35         if data.get("artifact_path") is not None:
# 36             data["artifact_path"] = str(data["artifact_path"])
# 37         return data


# schemas\agent_prompt_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class AgentPrompt(BaseModel):
# 11     """Schema for the agent_prompt table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     agent_role: Optional[str] = None
# 17     system_type: Optional[str] = None
# 18     artifact_path: Optional[Path] = None
# 19 
# 20     table_name: str = "agent_prompt"
# 21 
# 22     @field_validator("artifact_path")
# 23     @classmethod
# 24     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 25         if v is None:
# 26             return v
# 27         p = Path(v)
# 28         if not p.is_absolute() and ".." in p.parts:
# 29             raise ValueError("artifact_path must be absolute or project relative")
# 30         return p
# 31 
# 32     def model_dump(self) -> dict:  #  pragma: no cover - simple wrapper
# 33         data = getattr(super(), "model_dump", super().dict)()
# 34         if data.get("artifact_path") is not None:
# 35             data["artifact_path"] = str(data["artifact_path"])
# 36         return data


# schemas\context_provider_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .tooling_provider_schema import ToolingProvider
# 9 
# 10 
# 11 class ContextProvider(BaseModel):
# 12     """Schema for the context_provider table."""
# 13 
# 14     id: Optional[int] = None
# 15     name: str
# 16     description: Optional[str] = None
# 17     system_type: Optional[str] = None
# 18     tooling_provider_id: Optional[int] = None
# 19     tooling_provider: Optional[ToolingProvider] = None
# 20 
# 21     table_name: str = "context_provider"
# 22 
# 23     @model_validator(mode="after")
# 24     def _sync_ids(self) -> "ContextProvider":
# 25         if self.tooling_provider is not None and self.tooling_provider_id is None:
# 26             self.tooling_provider_id = getattr(self.tooling_provider, "id", None)
# 27         return self
# 28 
# 29     def model_dump(self) -> dict:  #  pragma: no cover - simple wrapper
# 30         data = getattr(super(), "model_dump", super().dict)()
# 31         if data.get("tooling_provider") is not None:
# 32             data["tooling_provider"] = data["tooling_provider"].model_dump()
# 33         return data


# schemas\experiment_config_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .system_config_schema import SystemConfig
# 9 from .scoring_provider_schema import ScoringProvider
# 10 
# 11 
# 12 class ExperimentConfig(BaseModel):
# 13     """Schema for the experiment_config table."""
# 14 
# 15     id: Optional[int] = None
# 16     name: str
# 17     description: Optional[str] = None
# 18     system_manager_id: Optional[int] = None
# 19     scoring_model_id: Optional[int] = None
# 20 
# 21     system_manager: Optional[SystemConfig] = None
# 22     scoring_model: Optional[ScoringProvider] = None
# 23 
# 24     table_name: str = "experiment_config"
# 25 
# 26     @model_validator(mode="after")
# 27     def _sync_ids(self) -> "ExperimentConfig":
# 28         if self.system_manager is not None and self.system_manager_id is None:
# 29             self.system_manager_id = getattr(self.system_manager, "id", None)
# 30         if self.scoring_model is not None and self.scoring_model_id is None:
# 31             self.scoring_model_id = getattr(self.scoring_model, "id", None)
# 32         return self
# 33 
# 34     def model_dump(self) -> dict:  #  pragma: no cover - simple wrapper
# 35         data = getattr(super(), "model_dump", super().dict)()
# 36         for field in ("system_manager", "scoring_model"):
# 37             if data.get(field) is not None:
# 38                 data[field] = data[field].model_dump()
# 39         return data


# schemas\file_path_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import field_validator
# 7 
# 8 
# 9 class FilePath(BaseModel):
# 10     """Schema for file paths tracked in the database."""
# 11 
# 12     artifact_path: Path
# 13 
# 14     table_name: str = "file_path"
# 15 
# 16     @field_validator("artifact_path")
# 17     @classmethod
# 18     def _check_path(cls, v: Path) -> Path:
# 19         p = Path(v)
# 20         if not p.is_absolute() and ".." in p.parts:
# 21             raise ValueError("artifact_path must be absolute or project relative")
# 22         return p
# 23 
# 24     def model_dump(self) -> dict:  #  pragma: no cover - simple wrapper
# 25         return {"artifact_path": str(self.artifact_path)}


# schemas\prompt_generator_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator, model_validator
# 8 
# 9 from .agent_prompt_schema import AgentPrompt
# 10 from .system_prompt_schema import SystemPrompt
# 11 from .context_provider_schema import ContextProvider
# 12 
# 13 
# 14 class PromptGenerator(BaseModel):
# 15     """Schema for the prompt_generator table."""
# 16 
# 17     id: Optional[int] = None
# 18     name: str
# 19     description: Optional[str] = None
# 20     agent_prompt_id: Optional[int] = None
# 21     system_prompt_id: Optional[int] = None
# 22     content_provider_id: Optional[int] = None
# 23 
# 24     agent_prompt: Optional[AgentPrompt] = None
# 25     system_prompt: Optional[SystemPrompt] = None
# 26     content_provider: Optional[ContextProvider] = None
# 27     artifact_path: Optional[Path] = None
# 28 
# 29     table_name: str = "prompt_generator"
# 30 
# 31     @model_validator(mode="after")
# 32     def _sync_ids(self) -> "PromptGenerator":
# 33         if self.agent_prompt is not None and self.agent_prompt_id is None:
# 34             self.agent_prompt_id = getattr(self.agent_prompt, "id", None)
# 35         if self.system_prompt is not None and self.system_prompt_id is None:
# 36             self.system_prompt_id = getattr(self.system_prompt, "id", None)
# 37         if self.content_provider is not None and self.content_provider_id is None:
# 38             self.content_provider_id = getattr(self.content_provider, "id", None)
# 39         return self
# 40 
# 41     @field_validator("artifact_path")
# 42     @classmethod
# 43     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 44         if v is None:
# 45             return v
# 46         p = Path(v)
# 47         if not p.is_absolute() and ".." in p.parts:
# 48             raise ValueError("artifact_path must be absolute or project relative")
# 49         return p
# 50 
# 51     def model_dump(self) -> dict:  #  pragma: no cover - simple wrapper
# 52         data = getattr(super(), "model_dump", super().dict)()
# 53         for field in ("agent_prompt", "system_prompt", "content_provider"):
# 54             if data.get(field) is not None:
# 55                 data[field] = data[field].model_dump()
# 56         if data.get("artifact_path") is not None:
# 57             data["artifact_path"] = str(data["artifact_path"])
# 58         return data


# schemas\scoring_provider_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class ScoringProvider(BaseModel):
# 11     """Schema for the scoring_provider table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     artifact_path: Optional[Path] = None
# 17 
# 18     table_name: str = "scoring_provider"
# 19 
# 20     @field_validator("artifact_path")
# 21     @classmethod
# 22     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 23         if v is None:
# 24             return v
# 25         p = Path(v)
# 26         if not p.is_absolute() and ".." in p.parts:
# 27             raise ValueError("artifact_path must be absolute or project relative")
# 28         return p
# 29 
# 30     def model_dump(self) -> dict:  #  pragma: no cover - simple wrapper
# 31         data = getattr(super(), "model_dump", super().dict)()
# 32         if data.get("artifact_path") is not None:
# 33             data["artifact_path"] = str(data["artifact_path"])
# 34         return data


# schemas\series_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .experiment_config_schema import ExperimentConfig
# 9 
# 10 
# 11 class Series(BaseModel):
# 12     """Schema for the series table."""
# 13 
# 14     id: Optional[int] = None
# 15     experiment_config_id: Optional[int] = None
# 16     experiment_config: Optional[ExperimentConfig] = None
# 17 
# 18     table_name: str = "series"
# 19 
# 20     @model_validator(mode="after")
# 21     def _sync_ids(self) -> "Series":
# 22         if self.experiment_config is not None and self.experiment_config_id is None:
# 23             self.experiment_config_id = getattr(self.experiment_config, "id", None)
# 24         return self
# 25 
# 26     def model_dump(self) -> dict:  #  pragma: no cover - simple wrapper
# 27         data = getattr(super(), "model_dump", super().dict)()
# 28         if data.get("experiment_config") is not None:
# 29             data["experiment_config"] = data["experiment_config"].model_dump()
# 30         return data


# schemas\state_manager_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class StateManager(BaseModel):
# 11     """Schema for the state_manager table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     system_state: Optional[str] = None
# 17     system_type: Optional[str] = None
# 18     agent_id: Optional[int] = None
# 19     artifact_path: Optional[Path] = None
# 20 
# 21     table_name: str = "state_manager"
# 22 
# 23     @field_validator("artifact_path")
# 24     @classmethod
# 25     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 26         if v is None:
# 27             return v
# 28         p = Path(v)
# 29         if not p.is_absolute() and ".." in p.parts:
# 30             raise ValueError("artifact_path must be absolute or project relative")
# 31         return p
# 32 
# 33     def model_dump(self) -> dict:  #  pragma: no cover - simple wrapper
# 34         data = getattr(super(), "model_dump", super().dict)()
# 35         if data.get("artifact_path") is not None:
# 36             data["artifact_path"] = str(data["artifact_path"])
# 37         return data


# schemas\system_config_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import Optional
# 4 
# 5 from pydantic import BaseModel
# 6 from app.utilities.pydantic_compat import model_validator
# 7 
# 8 from .state_manager_schema import StateManager
# 9 from .scoring_provider_schema import ScoringProvider
# 10 
# 11 
# 12 class SystemConfig(BaseModel):
# 13     """Schema for the system_config table."""
# 14 
# 15     id: Optional[int] = None
# 16     name: str
# 17     description: Optional[str] = None
# 18     system_type: Optional[str] = None
# 19     state_manager_id: Optional[int] = None
# 20     scoring_model_id: Optional[int] = None
# 21 
# 22     state_manager: Optional[StateManager] = None
# 23     scoring_model: Optional[ScoringProvider] = None
# 24 
# 25     table_name: str = "system_config"
# 26 
# 27     @model_validator(mode="after")
# 28     def _sync_ids(self) -> "SystemConfig":
# 29         if self.state_manager is not None and self.state_manager_id is None:
# 30             self.state_manager_id = getattr(self.state_manager, "id", None)
# 31         if self.scoring_model is not None and self.scoring_model_id is None:
# 32             self.scoring_model_id = getattr(self.scoring_model, "id", None)
# 33         return self
# 34 
# 35     def model_dump(self) -> dict:  #  pragma: no cover - simple wrapper
# 36         data = getattr(super(), "model_dump", super().dict)()
# 37         for field in ("state_manager", "scoring_model"):
# 38             if data.get(field) is not None:
# 39                 data[field] = data[field].model_dump()
# 40         return data


# schemas\system_prompt_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class SystemPrompt(BaseModel):
# 11     """Schema for the system_prompt table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     system_type: Optional[str] = None
# 17     artifact_path: Optional[Path] = None
# 18 
# 19     table_name: str = "system_prompt"
# 20 
# 21     @field_validator("artifact_path")
# 22     @classmethod
# 23     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 24         if v is None:
# 25             return v
# 26         p = Path(v)
# 27         if not p.is_absolute() and ".." in p.parts:
# 28             raise ValueError("artifact_path must be absolute or project relative")
# 29         return p
# 30 
# 31     def model_dump(self) -> dict:  #  pragma: no cover - simple wrapper
# 32         data = getattr(super(), "model_dump", super().dict)()
# 33         if data.get("artifact_path") is not None:
# 34             data["artifact_path"] = str(data["artifact_path"])
# 35         return data


# schemas\tooling_provider_schema.py

# 1 from __future__ import annotations
# 2 
# 3 from pathlib import Path
# 4 from typing import Optional
# 5 
# 6 from pydantic import BaseModel
# 7 from app.utilities.pydantic_compat import field_validator
# 8 
# 9 
# 10 class ToolingProvider(BaseModel):
# 11     """Schema for the tooling_provider table."""
# 12 
# 13     id: Optional[int] = None
# 14     name: str
# 15     description: Optional[str] = None
# 16     artifact_path: Optional[Path] = None
# 17 
# 18     table_name: str = "tooling_provider"
# 19 
# 20     @field_validator("artifact_path")
# 21     @classmethod
# 22     def _check_path(cls, v: Optional[Path]) -> Optional[Path]:
# 23         if v is None:
# 24             return v
# 25         p = Path(v)
# 26         if not p.is_absolute() and ".." in p.parts:
# 27             raise ValueError("artifact_path must be absolute or project relative")
# 28         return p
# 29 
# 30     def model_dump(self) -> dict:  #  pragma: no cover - simple wrapper
# 31         data = getattr(super(), "model_dump", super().dict)()
# 32         if data.get("artifact_path") is not None:
# 33             data["artifact_path"] = str(data["artifact_path"])
# 34         return data


# utilities\__init__.py



# utilities\db.py

# 1 import sqlite3
# 2 from dataclasses import fields
# 3 from datetime import datetime
# 4 from pathlib import Path
# 5 from typing import Iterable, Any
# 6 
# 7 DB_PATH = Path("experiments") / "codecritic.sqlite3"
# 8 
# 9 
# 10 def _serialize(obj: Any) -> dict:
# 11     data = {}
# 12     for f in fields(obj):
# 13         val = getattr(obj, f.name)
# 14         if isinstance(val, datetime):
# 15             val = val.isoformat()
# 16         data[f.name] = val
# 17     return data
# 18 
# 19 
# 20 def get_connection() -> sqlite3.Connection:
# 21     DB_PATH.parent.mkdir(exist_ok=True)
# 22     conn = sqlite3.connect(DB_PATH)
# 23     return conn
# 24 
# 25 
# 26 def init_db() -> sqlite3.Connection:
# 27     conn = get_connection()
# 28     cur = conn.cursor()
# 29     cur.execute(
# 30         """CREATE TABLE IF NOT EXISTS state_log (
# 31             experiment_id TEXT,
# 32             system TEXT,
# 33             round INTEGER,
# 34             state TEXT,
# 35             action TEXT,
# 36             score REAL,
# 37             details TEXT,
# 38             timestamp TEXT
# 39         )"""
# 40     )
# 41     cur.execute(
# 42         """CREATE TABLE IF NOT EXISTS state_transition_log (
# 43             experiment_id TEXT,
# 44             round INTEGER,
# 45             from_state TEXT,
# 46             to_state TEXT,
# 47             reason TEXT,
# 48             timestamp TEXT
# 49         )"""
# 50     )
# 51     cur.execute(
# 52         """CREATE TABLE IF NOT EXISTS prompt_log (
# 53             experiment_id TEXT,
# 54             round INTEGER,
# 55             system TEXT,
# 56             agent_id TEXT,
# 57             agent_role TEXT,
# 58             agent_engine TEXT,
# 59             symbol TEXT,
# 60             prompt TEXT,
# 61             response TEXT,
# 62             attempt_number INTEGER,
# 63             agent_action_outcome TEXT,
# 64             start TEXT,
# 65             stop TEXT
# 66         )"""
# 67     )
# 68     cur.execute(
# 69         """CREATE TABLE IF NOT EXISTS code_quality_log (
# 70             experiment_id TEXT,
# 71             round INTEGER,
# 72             symbol TEXT,
# 73             lines_of_code INTEGER,
# 74             cyclomatic_complexity REAL,
# 75             maintainability_index REAL,
# 76             lint_errors INTEGER,
# 77             timestamp TEXT
# 78         )"""
# 79     )
# 80     cur.execute(
# 81         """CREATE TABLE IF NOT EXISTS scoring_log (
# 82             experiment_id TEXT,
# 83             round INTEGER,
# 84             metric TEXT,
# 85             value REAL,
# 86             timestamp TEXT
# 87         )"""
# 88     )
# 89     conn.commit()
# 90     return conn
# 91 
# 92 
# 93 def insert_logs(conn: sqlite3.Connection, table: str, logs: Iterable[Any]) -> None:
# 94     logs = list(logs)
# 95     if not logs:
# 96         return
# 97     row = _serialize(logs[0])
# 98     cols = ",".join(row.keys())
# 99     placeholders = ",".join(["?"] * len(row))
# 100     values = [tuple(_serialize(log).values()) for log in logs]
# 101     conn.executemany(
# 102         f"INSERT INTO {table} ({cols}) VALUES ({placeholders})",
# 103         values,
# 104     )
# 105     conn.commit()


# utilities\file_management\__init__.py



# utilities\metadata\__init__.py



# utilities\metadata\footer\__init__.py



# utilities\metadata\logging\__init__.py

# 1 """Logging schemas for structured experiment data."""
# 2 
# 3 from .log_schemas import (
# 4     StateLog,
# 5     StateTransitionLog,
# 6     PromptLog,
# 7     CodeQualityLog,
# 8     ErrorLog,
# 9     ScoringLog,
# 10 )
# 11 
# 12 __all__ = [
# 13     "StateLog",
# 14     "StateTransitionLog",
# 15     "PromptLog",
# 16     "CodeQualityLog",
# 17     "ErrorLog",
# 18     "ScoringLog",
# 19 ]


# utilities\metadata\logging\log_schemas.py

# 1 from __future__ import annotations
# 2 
# 3 from dataclasses import dataclass, field
# 4 from datetime import datetime, timezone
# 5 
# 6 
# 7 @dataclass
# 8 class StateLog:
# 9     experiment_id: str
# 10     system: str
# 11     round: int
# 12     state: str
# 13     action: str
# 14     score: float | None = None
# 15     details: str | None = None
# 16     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 17 
# 18 
# 19 @dataclass
# 20 class StateTransitionLog:
# 21     experiment_id: str
# 22     round: int
# 23     from_state: str
# 24     to_state: str
# 25     reason: str | None = None
# 26     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 27 
# 28 
# 29 @dataclass
# 30 class PromptLog:
# 31     """Log entry for prompts generated and executed by agents."""
# 32 
# 33     experiment_id: str
# 34     round: int
# 35     system: str
# 36     agent_id: str
# 37     agent_role: str
# 38     agent_engine: str | None = None
# 39     symbol: str | None = None
# 40     prompt: str | None = None
# 41     response: str | None = None
# 42     attempt_number: int = 0
# 43     agent_action_outcome: str | None = None
# 44     start: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 45     stop: datetime | None = None
# 46 
# 47 
# 48 @dataclass
# 49 class CodeQualityLog:
# 50     """Log entry for static analysis and linting results."""
# 51 
# 52     experiment_id: str
# 53     round: int
# 54     symbol: str
# 55     lines_of_code: int
# 56     cyclomatic_complexity: float
# 57     maintainability_index: float
# 58     lint_errors: int
# 59     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 60 
# 61 
# 62 @dataclass
# 63 class ErrorLog:
# 64     """Log entry for errors encountered during execution."""
# 65 
# 66     experiment_id: str
# 67     round: int
# 68     error_type: str
# 69     message: str
# 70     file_path: str | None = None
# 71     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
# 72 
# 73 
# 74 @dataclass
# 75 class ScoringLog:
# 76     """Log entry for computed evaluation metrics."""
# 77 
# 78     experiment_id: str
# 79     round: int
# 80     metric: str
# 81     value: float
# 82     timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))


# utilities\metadata\snapshots\__init__.py



# utilities\metrics.py

# 1 from __future__ import annotations
# 2 
# 3 import logging
# 4 from typing import Any, Dict, List
# 5 
# 6 logger = logging.getLogger(__name__)
# 7 
# 8 #  Unified list of metric names expected in experiment results
# 9 EVALUATION_METRICS = [
# 10     "bug_fix_success_rate",
# 11     "functional_correctness",
# 12     "avg_test_pass_rate",
# 13     "maintainability_index",
# 14     "cyclomatic_complexity",
# 15     "linting_compliance_rate",
# 16     "iterations_to_convergence",
# 17     "intervention_frequency",
# 18     "agent_role_success_rate",
# 19     "retry_success_rate",
# 20     "mediation_success_rate",
# 21 ]
# 22 
# 23 
# 24 def _get(obj: Any, key: str, default: Any = None) -> Any:
# 25     if isinstance(obj, dict):
# 26         return obj.get(key, default)
# 27     return getattr(obj, key, default)
# 28 
# 29 
# 30 def _avg(values: List[float]) -> float:
# 31     return sum(values) / len(values) if values else 0.0
# 32 
# 33 
# 34 def compute_metrics(
# 35     evaluation_logs: List[Any],
# 36     code_quality_logs: List[Any],
# 37     conversation_logs: List[Any],
# 38     prompt_logs: List[Any],
# 39     state_logs: List[Any],
# 40 ) -> Dict[str, float]:
# 41     """Compute experiment metrics from structured logs."""
# 42 
# 43     logger.debug(
# 44         "Computing metrics: evaluations=%d quality=%d conversation=%d prompts=%d states=%d",
# 45         len(evaluation_logs),
# 46         len(code_quality_logs),
# 47         len(conversation_logs),
# 48         len(prompt_logs),
# 49         len(state_logs),
# 50     )
# 51 
# 52     metrics: Dict[str, float] = {}
# 53 
# 54     bug_fixes = [_get(log, "bug_fixed", False) for log in evaluation_logs]
# 55     metrics["bug_fix_success_rate"] = _avg([1.0 for bug in bug_fixes if bug])
# 56 
# 57     func_correct = [_get(log, "all_tests_passed", False) for log in evaluation_logs]
# 58     metrics["functional_correctness"] = _avg([1.0 for passes in func_correct if passes])
# 59 
# 60     pass_rates = []
# 61     for log in evaluation_logs:
# 62         total = _get(log, "tests_total", 0)
# 63         passed = _get(log, "tests_passed", 0)
# 64         if total:
# 65             pass_rates.append(passed / total)
# 66     metrics["avg_test_pass_rate"] = _avg(pass_rates)
# 67 
# 68     mi_values = [_get(log, "maintainability_index", 0.0) for log in code_quality_logs]
# 69     metrics["maintainability_index"] = _avg(mi_values)
# 70 
# 71     cc_values = [_get(log, "cyclomatic_complexity", 0.0) for log in code_quality_logs]
# 72     metrics["cyclomatic_complexity"] = _avg(cc_values)
# 73 
# 74     lint_compliance = [
# 75         1.0 if _get(log, "lint_errors", 1) == 0 else 0.0 for log in code_quality_logs
# 76     ]
# 77     metrics["linting_compliance_rate"] = _avg(lint_compliance)
# 78 
# 79     rounds = [_get(log, "round", 0) for log in state_logs]
# 80     metrics["iterations_to_convergence"] = max(rounds) + 1 if rounds else 0.0
# 81 
# 82     interventions = [
# 83         1.0 for log in conversation_logs if _get(log, "intervention", False)
# 84     ]
# 85     metrics["intervention_frequency"] = _avg(interventions)
# 86 
# 87     outcomes = [_get(log, "agent_action_outcome", None) for log in prompt_logs]
# 88     successes = [1.0 for o in outcomes if o == "success"]
# 89     metrics["agent_role_success_rate"] = _avg(successes)
# 90 
# 91     retry_logs = [log for log in prompt_logs if _get(log, "attempt_number", 0) > 0]
# 92     retry_successes = [
# 93         1.0
# 94         for log in retry_logs
# 95         if _get(log, "agent_action_outcome", None) == "success"
# 96     ]
# 97     metrics["retry_success_rate"] = _avg(retry_successes)
# 98 
# 99     mediation_logs = [
# 100         log
# 101         for log in conversation_logs
# 102         if _get(log, "intervention_type", None) == "mediation"
# 103     ]
# 104     mediation_success = [
# 105         1.0 for log in mediation_logs if _get(log, "intervention", False)
# 106     ]
# 107     metrics["mediation_success_rate"] = _avg(mediation_success)
# 108 
# 109     logger.info("Metrics computed: %s", metrics)
# 110     return metrics


# utilities\pydantic_compat.py

# 1 from __future__ import annotations
# 2 
# 3 from typing import TYPE_CHECKING, Callable
# 4 
# 5 if TYPE_CHECKING:
# 6     from pydantic import field_validator, model_validator  #  type: ignore
# 7 else:  #  pragma: no cover - runtime fallback for pydantic v1
# 8     try:
# 9         from pydantic import field_validator, model_validator  #  type: ignore
# 10     except ImportError:  #  pragma: no cover - pydantic v1
# 11         from pydantic import validator
# 12 
# 13         def field_validator(*fields: str, **kwargs) -> Callable:
# 14             return validator(*fields, **kwargs)
# 15 
# 16         def model_validator(*fields, **kwargs) -> Callable:
# 17             return validator(*fields, **kwargs)


# utilities\schema\__init__.py

# 1 from .create_schema import initialize_database, create_tables, load_seed_data
# 2 
# 3 __all__ = ["initialize_database", "create_tables", "load_seed_data"]


# utilities\schema\create_schema.py

# 1 from __future__ import annotations
# 2 
# 3 import json
# 4 import sqlite3
# 5 from pathlib import Path
# 6 from typing import Type, Protocol, Any, get_origin, get_args, Union
# 7 
# 8 
# 9 from app.schemas import (
# 10     AgentEngine,
# 11     AgentPrompt,
# 12     SystemPrompt,
# 13     ContextProvider,
# 14     ToolingProvider,
# 15     FilePath,
# 16     AgentConfig,
# 17     PromptGenerator,
# 18     ScoringProvider,
# 19     StateManager,
# 20     SystemConfig,
# 21     ExperimentConfig,
# 22     Series,
# 23 )
# 24 from app.utilities import db
# 25 
# 26 
# 27 class SchemaProtocol(Protocol):
# 28     table_name: str
# 29     model_fields: dict
# 30 
# 31     def __init__(self, **data: object) -> None: ...
# 32 
# 33     def model_dump(self) -> dict: ...
# 34 
# 35 
# 36 SchemaModel = Type[Any]
# 37 
# 38 SCHEMAS: tuple[SchemaModel, ...] = (
# 39     AgentEngine,
# 40     AgentPrompt,
# 41     SystemPrompt,
# 42     ContextProvider,
# 43     ToolingProvider,
# 44     FilePath,
# 45     AgentConfig,
# 46     PromptGenerator,
# 47     ScoringProvider,
# 48     StateManager,
# 49     SystemConfig,
# 50     ExperimentConfig,
# 51     Series,
# 52 )
# 53 
# 54 TABLE_MAP = {model.table_name: model for model in SCHEMAS}
# 55 
# 56 
# 57 _TYPE_MAP = {
# 58     int: "INTEGER",
# 59     str: "TEXT",
# 60     float: "REAL",
# 61     Path: "TEXT",
# 62 }
# 63 
# 64 
# 65 def _sqlite_type(py_type: Type) -> str:
# 66     origin = get_origin(py_type)
# 67     if origin is Union:
# 68         py_type = get_args(py_type)[0]
# 69     return _TYPE_MAP.get(py_type, "TEXT")
# 70 
# 71 
# 72 def create_tables(conn: sqlite3.Connection) -> None:
# 73     cur = conn.cursor()
# 74     for model in SCHEMAS:
# 75         columns = []
# 76         for name, field in model.model_fields.items():
# 77             if name == "table_name":
# 78                 continue
# 79             column = f"{name}"
# 80             col_type = _sqlite_type(field.annotation)
# 81             if name == "id" and not field.is_required():
# 82                 columns.append(f"{column} INTEGER PRIMARY KEY")
# 83             else:
# 84                 columns.append(f"{column} {col_type}")
# 85         col_sql = ",".join(columns)
# 86         cur.execute(f"CREATE TABLE IF NOT EXISTS {model.table_name} ({col_sql})")
# 87     conn.commit()
# 88 
# 89 
# 90 def load_seed_data(
# 91     conn: sqlite3.Connection, seed_dir: Path | str = "experiments/config/seed"
# 92 ) -> None:
# 93     seed_path = Path(seed_dir)
# 94     if not seed_path.exists():
# 95         return
# 96     cur = conn.cursor()
# 97     for file in seed_path.glob("*.json"):
# 98         model = TABLE_MAP.get(file.stem)
# 99         if model is None:
# 100             continue
# 101         entries = json.loads(file.read_text())
# 102         if isinstance(entries, dict):
# 103             entries = [entries]
# 104         for entry in entries:
# 105             obj = model(**entry)
# 106             data = obj.model_dump()
# 107             cols = ",".join(data.keys())
# 108             placeholders = ",".join(["?"] * len(data))
# 109             cur.execute(
# 110                 f"INSERT INTO {model.table_name} ({cols}) VALUES ({placeholders})",
# 111                 list(data.values()),
# 112             )
# 113     conn.commit()
# 114 
# 115 
# 116 def initialize_database() -> sqlite3.Connection:
# 117     conn = db.get_connection()
# 118     create_tables(conn)
# 119     load_seed_data(conn)
# 120     return conn
# 121 
# 122 
# 123 if __name__ == "__main__":  #  pragma: no cover - manual invocation
# 124     initialize_database()


# utilities\tools\black_runner.py

# 1 #  Placeholder for black runner tool


# utilities\tools\doc_formatter.py

# 1 #  Placeholder for documentation formatter


# utilities\tools\mypy_runner.py

# 1 #  Placeholder for mypy runner


# utilities\tools\radon_runner.py

# 1 import sys
# 2 import json
# 3 from radon.complexity import cc_visit
# 4 from pathlib import Path
# 5 
# 6 
# 7 def analyze_file(filepath):
# 8     with open(filepath, "r", encoding="utf-8") as file:
# 9         code = file.read()
# 10     complexity = cc_visit(code)
# 11     result = [
# 12         {"name": c.name, "complexity": c.complexity, "lineno": c.lineno}
# 13         for c in complexity
# 14     ]
# 15     return result
# 16 
# 17 
# 18 if __name__ == "__main__":
# 19     try:
# 20         if len(sys.argv) != 2:
# 21             raise ValueError("Provide exactly one file path.")
# 22         file_path = Path(sys.argv[1])
# 23         if not file_path.exists():
# 24             raise FileNotFoundError("File does not exist.")
# 25         analysis_result = analyze_file(str(file_path))
# 26         print(json.dumps({"result": analysis_result}, ensure_ascii=False))
# 27     except Exception as e:
# 28         print(json.dumps({"error": str(e)}, ensure_ascii=False))
# 29         sys.exit(1)


# utilities\tools\ruff_runner.py

# 1 #  Placeholder for ruff runner


# utilities\tools\sonarcloud_runner.py

# 1 #  Placeholder for sonarcloud runner



# === NON-PYTHON FILES ===

